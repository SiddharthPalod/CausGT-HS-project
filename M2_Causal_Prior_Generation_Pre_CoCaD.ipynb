{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296b488b",
   "metadata": {},
   "source": [
    "# Initial Loading (From M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "load_path = \"extracted_output/A_co_occur.npz\"\n",
    "\n",
    "A_co_occur = load_npz(load_path)\n",
    "\n",
    "print(\"Loaded type:\", type(A_co_occur))\n",
    "print(\"Shape:\", A_co_occur.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved entities\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_extracted_entities(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - entities.json              → list of deduplicated entity/event nodes\n",
    "      - entities_per_chunk_debug.json → raw per-chunk extraction data\n",
    "    \"\"\"\n",
    "\n",
    "    entities_path = os.path.join(base_dir, \"entities.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"entities_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(entities_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {entities_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(entities_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        entities = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - entities.json\")\n",
    "    print(\" - entities_per_chunk_debug.json\")\n",
    "\n",
    "    return entities, per_chunk\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    nodes, nodes_per_chunk = load_extracted_entities()\n",
    "    print(f\"Total nodes loaded: {len(nodes)}\")\n",
    "    print(f\"Total chunks loaded: {len(nodes_per_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved relations\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_extracted_relations(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - relations.json\n",
    "      - relations_per_chunk_debug.json\n",
    "\n",
    "    Returns:\n",
    "      relations: list of all deduplicated relation edges\n",
    "      per_chunk_relations: list of per-chunk debug relation records\n",
    "    \"\"\"\n",
    "\n",
    "    relations_path = os.path.join(base_dir, \"relations.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"relations_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(relations_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {relations_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(relations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        relations = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk_relations = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - relations.json\")\n",
    "    print(\" - relations_per_chunk_debug.json\")\n",
    "\n",
    "    return relations, per_chunk_relations\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    relations, relations_per_chunk = load_extracted_relations()\n",
    "\n",
    "    print(f\"Total relations loaded: {len(relations)}\")\n",
    "    print(f\"Total chunks returned: {len(relations_per_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b141db",
   "metadata": {},
   "source": [
    "# 3.3 $C_{prior}$ generation\n",
    "\n",
    "The goal here is to create a sparse answer key $C_{prior}$ (Our causal prior).\n",
    "\n",
    "Our Cprior is kinda a high-quality ”training dataset” that tlls us the true causal links in the document.\n",
    "Since we have no human labels, we must generate the dataset ourselves. This is a self-supervised process.\n",
    "The core idea is to use a large, powerful ”Teacher” LLM (e.g., gemini-2.5-flash) to perform complex causal reasoning.\n",
    "The output of this phase is the Cprior (Causal Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c7f80",
   "metadata": {},
   "source": [
    "## Active Candidate-Set Expansion (ACE)\n",
    "\n",
    "We cant query LLM with all pairs of nodes: O($N^2$).. So prune them to a list of plausible candidate pairs $E_{prior}$. This small list is what we send to our LLM.. This is done in 2 stages:\n",
    "1. Structural Filter (GAE)\n",
    "2. Semantic Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb3c1d",
   "metadata": {},
   "source": [
    "### 1. Structural Filter (GAE):\n",
    "\n",
    "Our initial A_w are myopic (They only contain local 1-hop links within a paragraph). We need an unsupervised way to find pairs (i, j) that are strongly connected via multi-hop paths, as these are highly plausible for causal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf274a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import identity, csr_matrix\n",
    "\n",
    "# Assuming A_co_occur is the final CSR matrix generated previously.\n",
    "# If A_co_occur is not defined, this code assumes it has been loaded or created.\n",
    "\n",
    "def calculate_A_hat(A_co_occur: csr_matrix) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Calculates the normalized adjacency matrix with self-loops, A_hat.\n",
    "    A_hat = A_co_occur + I, where I is the Identity Matrix.\n",
    "    \n",
    "    Args:\n",
    "        A_co_occur: The unweighted, symmetric co-occurrence matrix (N x N).\n",
    "        \n",
    "    Returns:\n",
    "        The A_hat matrix in CSR format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get the dimension N\n",
    "    N = A_co_occur.shape[0]\n",
    "    \n",
    "    # 2. Create the Identity Matrix I (in sparse format)\n",
    "    # The identity matrix is N x N, with ones on the diagonal.\n",
    "    I = identity(N, dtype=A_co_occur.dtype, format='csr')\n",
    "    \n",
    "    # 3. Calculate A_hat = A_co-occur + I\n",
    "    # Sparse matrix addition handles the union of the two sets of non-zero entries.\n",
    "    A_hat = A_co_occur + I\n",
    "    \n",
    "    # Ensure the result is still binary (0 or 1), although A_co_occur should not have values > 1.\n",
    "    # The sum of 1 (from A_co-occur) and 1 (from I) on the diagonal will result in 2.\n",
    "    # We must ensure the result is clipped back to 1.\n",
    "    A_hat_binary = A_hat.sign()\n",
    "    \n",
    "    # Convert back to the desired CSR integer format\n",
    "    return A_hat_binary.astype(np.int8)\n",
    "\n",
    "# --- Example Usage (Assuming A_co_occur is available) ---\n",
    "A_hat = calculate_A_hat(A_co_occur)\n",
    "\n",
    "print(f\"✅ A_hat successfully created.\")\n",
    "print(f\"A_hat Shape: {A_hat.shape}\")\n",
    "print(f\"Non-Zero Entries (nnz): {A_hat.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from typing import List\n",
    "\n",
    "def calculate_D_hat(A_hat: csr_matrix) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Calculates the diagonal degree matrix D_hat from A_hat.\n",
    "    \n",
    "    Fix: Ensures the degree vector is explicitly flattened to 1D before diags().\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate Row Sums (The Degree Vector)\n",
    "    # A_hat.sum(axis=1) returns a dense numpy.matrix of shape (N, 1).\n",
    "    sum_matrix = A_hat.sum(axis=1)\n",
    "    \n",
    "    # 2. Flatten the result into a true 1D NumPy array (shape (N,))\n",
    "    # The .A1 attribute is the most robust way to flatten the result of a sparse matrix sum.\n",
    "    degree_vector = sum_matrix.A1 \n",
    "    \n",
    "    # 3. Construct the Diagonal Matrix D_hat\n",
    "    # diags now receives a proper 1D array for the diagonal.\n",
    "    D_hat = diags(degree_vector, format='csr')\n",
    "    \n",
    "    print(\"Degree Vector Shape (N):\", degree_vector.shape)\n",
    "    print(f\"D_hat diagonal values (first 5): {degree_vector[:5].tolist()}\")\n",
    "    \n",
    "    return D_hat\n",
    "\n",
    "# --- Example Usage ---\n",
    "D_hat = calculate_D_hat(A_hat)\n",
    "\n",
    "print(f\"✅ D_hat matrix created.\")\n",
    "print(f\"D_hat Shape: {D_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88684d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. Load nodes (entities) from entities.json\n",
    "# =========================================================\n",
    "\n",
    "def load_nodes(path: str = \"extracted_output/entities.json\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load the list of nodes produced by the entity extraction step.\n",
    "    Each node is expected to look like:\n",
    "\n",
    "    {\n",
    "        \"id\": \"N8\",\n",
    "        \"name\": \"Black Market Sales of Titan's strategic assets\",\n",
    "        \"type\": \"event\",\n",
    "        \"time\": \"around July 10th\",\n",
    "        \"location\": null,\n",
    "        \"description\": \"A significant increase ...\",\n",
    "        \"source_chunks\": [0]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        nodes = json.load(f)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Build node texts for embedding (using rich metadata)\n",
    "# =========================================================\n",
    "\n",
    "def make_node_text(node: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact text representation of a node using multiple fields,\n",
    "    not just the name. This is what we embed.\n",
    "\n",
    "    Fields used:\n",
    "      - name (always)\n",
    "      - type (if present)\n",
    "      - time (if present)\n",
    "      - location (if present)\n",
    "      - description (if present)\n",
    "    \"\"\"\n",
    "    parts = [node.get(\"name\", \"\").strip()]\n",
    "\n",
    "    node_type = node.get(\"type\")\n",
    "    if node_type:\n",
    "        parts.append(f\"type: {node_type}\")\n",
    "\n",
    "    time_str = node.get(\"time\")\n",
    "    if time_str:\n",
    "        parts.append(f\"time: {time_str}\")\n",
    "\n",
    "    loc = node.get(\"location\")\n",
    "    if loc:\n",
    "        parts.append(f\"location: {loc}\")\n",
    "\n",
    "    desc = node.get(\"description\")\n",
    "    if desc:\n",
    "        parts.append(desc.strip())\n",
    "\n",
    "    # Filter out any empty components and join with \" | \"\n",
    "    parts = [p for p in parts if p]\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. Build NodeEmbeddingMap and X matrix (with N0)\n",
    "# =========================================================\n",
    "\n",
    "def build_node_embeddings(\n",
    "    nodes: List[Dict[str, Any]],\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    ") -> (Dict[str, np.ndarray], np.ndarray, List[str]):\n",
    "    \"\"\"\n",
    "    Given the rich node list (with id/name/type/time/location/description),\n",
    "    build:\n",
    "\n",
    "      - NodeEmbeddingMap: { \"N0\": vec0, \"N1\": vec1, ... }\n",
    "      - X: 2D matrix where row index == numeric node ID (0..max_id).\n",
    "            * row 0 -> N0 (all zeros)\n",
    "            * row k -> Nk\n",
    "      - sorted_node_ids: list of IDs in numeric order: [\"N0\", \"N1\", \"N2\", ...]\n",
    "\n",
    "    We *do not* mutate the input nodes list; we just build a consistent mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Sort real nodes by numeric id (N1, N2, ..., N14)\n",
    "    def numeric_id(n: Dict[str, Any]) -> int:\n",
    "        return int(n[\"id\"][1:])  # \"N10\" -> 10\n",
    "\n",
    "    real_nodes_sorted = sorted(nodes, key=numeric_id)\n",
    "\n",
    "    # 2) Create the list of IDs including N0 as a virtual node\n",
    "    # N0 is not in 'nodes'; we add it logically here.\n",
    "    sorted_node_ids: List[str] = [\"N0\"] + [n[\"id\"] for n in real_nodes_sorted]\n",
    "\n",
    "    # 3) Build texts for embedding\n",
    "    # N0 gets a dummy label; we will zero out its embedding later anyway.\n",
    "    node_texts: List[str] = [\"GRAPH_PAD_NODE\"]  # for N0\n",
    "\n",
    "    for n in real_nodes_sorted:\n",
    "        node_texts.append(make_node_text(n))\n",
    "\n",
    "    # 4) Load embedder and compute embeddings\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"Loaded embedding model: {model_name}\")\n",
    "    print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    print(\"Starting node embedding...\")\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        node_texts,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "\n",
    "    # 5) Overwrite N0 vector with zeros\n",
    "    embeddings[0, :] = 0.0\n",
    "    print(\"\\n✅ N0 embedding set to zero.\")\n",
    "\n",
    "    # 6) Build NodeEmbeddingMap: { node_id: vector }\n",
    "    NodeEmbeddingMap: Dict[str, np.ndarray] = {\n",
    "        node_id: embeddings[i] for i, node_id in enumerate(sorted_node_ids)\n",
    "    }\n",
    "\n",
    "    # 7) Build X matrix where row index == numeric ID\n",
    "    #    Highest numeric id among real nodes:\n",
    "    max_id = max(int(n[\"id\"][1:]) for n in real_nodes_sorted)\n",
    "    d = embeddings.shape[1]\n",
    "\n",
    "    # We want rows 0..max_id, so shape = (max_id + 1, d)\n",
    "    X = np.zeros((max_id + 1, d), dtype=embeddings.dtype)\n",
    "\n",
    "    # Row 0: N0\n",
    "    X[0, :] = NodeEmbeddingMap[\"N0\"]\n",
    "\n",
    "    # Rows for N1..Nmax\n",
    "    for n in real_nodes_sorted:\n",
    "        idx = int(n[\"id\"][1:])\n",
    "        X[idx, :] = NodeEmbeddingMap[n[\"id\"]]\n",
    "\n",
    "    # Some prints for verification\n",
    "    print(\"\\n--- Node Embedding Results ---\")\n",
    "    print(f\"Total real nodes (no N0): {len(real_nodes_sorted)}\")\n",
    "    print(f\"Total logical nodes (with N0): {len(sorted_node_ids)}\")\n",
    "    print(f\"Embedding matrix X shape: {X.shape}\")\n",
    "    print(f\"Verification: sum(N0 row) = {np.sum(X[0]):.4f}\")\n",
    "\n",
    "    return NodeEmbeddingMap, X, sorted_node_ids\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. Optional: pretty-print a small snippet\n",
    "# =========================================================\n",
    "\n",
    "def debug_print_node_embeddings(\n",
    "    NodeEmbeddingMap: Dict[str, np.ndarray],\n",
    "    sorted_node_ids: List[str],\n",
    "    max_nodes_to_show: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print the first few node IDs and a snippet of their embedding vectors.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- NodeEmbeddingMap (snippet) ---\")\n",
    "    for node_id in sorted_node_ids[:max_nodes_to_show]:\n",
    "        vec_snip = NodeEmbeddingMap[node_id][:3]\n",
    "        print(f\"{node_id}: [{vec_snip[0]:.4f}, {vec_snip[1]:.4f}, {vec_snip[2]:.4f}, ...]\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. Build X and NodeEmbeddingMap end-to-end\n",
    "# =========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load nodes from the GraphRAG-style entities file\n",
    "    nodes = load_nodes(\"extracted_output/entities.json\")\n",
    "\n",
    "    # 2) Build embeddings\n",
    "    NodeEmbeddingMap, X, sorted_node_ids = build_node_embeddings(nodes)\n",
    "\n",
    "    # 3) Debug print\n",
    "    debug_print_node_embeddings(NodeEmbeddingMap, sorted_node_ids, max_nodes_to_show=6)\n",
    "\n",
    "    # 4)  Save X and NodeEmbeddingMap if you want to reuse later\n",
    "    os.makedirs(\"extracted_output\", exist_ok=True)\n",
    "\n",
    "    # Save X as numpy binary\n",
    "    np.save(\"extracted_output/node_features_X.npy\", X)\n",
    "\n",
    "    # Save NodeEmbeddingMap as a JSON + npy (store vectors in a separate .npy if you want)\n",
    "    # Here we just store IDs → index mapping; vectors are in X.\n",
    "    id_to_row_index = {node_id: int(node_id[1:]) if node_id != \"N0\" else 0\n",
    "                       for node_id in sorted_node_ids}\n",
    "\n",
    "    with open(\"extracted_output/node_id_to_row_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(id_to_row_index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  extracted_output/node_features_X.npy\")\n",
    "    print(\"  extracted_output/node_id_to_row_index.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7e370",
   "metadata": {},
   "source": [
    "#### Pre-computation and GAE Model Definition:\n",
    "\n",
    "This section defines the helper function to calculate the normalized adjacency matrix $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and the PyTorch module for the GAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Hyperparameters and Dimensions ---\n",
    "N_NODES = len(nodes)+1\n",
    "D_IN = 384\n",
    "D_HIDDEN = 128\n",
    "D_LATENT = 64\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 5e-4\n",
    "NUM_EPOCHS = 500   # 500 is enough to see behaviour\n",
    "\n",
    "\n",
    "# --- 1. Model Definitions ---\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\"Two-layer GCN encoder for GAE.\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True)\n",
    "\n",
    "        # Optional: explicit Glorot init (PyG already uses good defaults)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 2. Data Preparation (WITH FEATURE SCALING) ---\n",
    "\n",
    "def prepare_pyg_data(X_np: np.ndarray, A_cooccur_csr: csr_matrix):\n",
    "    \"\"\"\n",
    "    Converts NumPy/SciPy data into the PyTorch Geometric Data object,\n",
    "    applying necessary feature scaling (StandardScaler) for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale node features\n",
    "    scaler = StandardScaler()\n",
    "    X_np_scaled = scaler.fit_transform(X_np)\n",
    "\n",
    "    # ---- IMPORTANT: Make adjacency symmetric & remove self loops ----\n",
    "    A = A_cooccur_csr\n",
    "    # Symmetrize: A_undirected = A OR A^T\n",
    "    A_sym = ((A + A.T) > 0).astype(np.float32)\n",
    "\n",
    "    # Let GCNConv add self-loops itself; don't add I here\n",
    "    # Use PyG helper to create edge_index\n",
    "    edge_index, _ = from_scipy_sparse_matrix(A_sym)\n",
    "\n",
    "    x = torch.from_numpy(X_np_scaled).float()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# --- 3. Training Function ---\n",
    "\n",
    "def train_pyg_gae(data: Data, epochs: int):\n",
    "    \n",
    "    encoder = GCNEncoder(\n",
    "        in_channels=D_IN,\n",
    "        hidden_channels=D_HIDDEN,\n",
    "        out_channels=D_LATENT\n",
    "    )\n",
    "    model = GAE(encoder)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=LEARNING_RATE,\n",
    "                           weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    print(f\"Training GAE for {epochs} epochs (lr={LEARNING_RATE}, decay={WEIGHT_DECAY})...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "        # recon_loss expects a positive edge_index of the (unweighted) graph\n",
    "        loss = model.recon_loss(z, data.edge_index)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Return final embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# --- 4. Execution Block ---\n",
    "\n",
    "# Option A: your original random graph (will not give meaningful loss trend)\n",
    "X_np_sim = np.random.rand(N_NODES, D_IN).astype(np.float32)\n",
    "A_cooccur_sim = csr_matrix(\n",
    "    np.random.randint(0, 2, size=(N_NODES, N_NODES), dtype=np.int8)\n",
    ")\n",
    "\n",
    "# Make sure nonzero entries are 1\n",
    "A_cooccur_sim.data[:] = 1\n",
    "\n",
    "pyg_data = prepare_pyg_data(X_np_sim, A_cooccur_sim)\n",
    "\n",
    "Final_Embeddings_Z = train_pyg_gae(pyg_data, epochs=NUM_EPOCHS)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"✅ PyG GAE Training Successfully Executed.\")\n",
    "print(f\"Final Latent Embedding Matrix Z Shape: {Final_Embeddings_Z.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Final_Embeddings_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ddd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the above\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Make sure directory exists\n",
    "os.makedirs(\"extracted_output\", exist_ok=True)\n",
    "\n",
    "save_path = \"extracted_output/Z.pt\"\n",
    "\n",
    "# Z = Final_Embeddings_Z  # Ensure Z exists\n",
    "torch.save(Z, save_path)\n",
    "\n",
    "print(f\"Saved GAE embeddings Z to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading from the above\n",
    "\n",
    "import torch\n",
    "\n",
    "load_path = \"extracted_output/Z.pt\"\n",
    "\n",
    "# Load the saved tensor\n",
    "Z = torch.load(load_path)\n",
    "\n",
    "print(f\"Loaded GAE embeddings Z from: {load_path}\")\n",
    "print(f\"Z shape: {Z.shape}\")\n",
    "print(f\"Z dtype: {Z.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "Z_PATH = \"extracted_output/Z.pt\"          # Saved GAE embeddings\n",
    "NODES_PATH = \"extracted_output/entities.json\"  # GraphRAG-style nodes\n",
    "\n",
    "SCALING_RATIO = 0.30  # k' = floor(num_real_nodes * SCALING_RATIO)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD NODES AND BUILD INDEX MAPPINGS\n",
    "# ============================================================\n",
    "\n",
    "with open(NODES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    nodes: List[Dict[str, Any]] = json.load(f)\n",
    "\n",
    "# nodes[i][\"id\"] is like \"N1\", \"N2\", ... created by your entity extractor.\n",
    "# We will assume:\n",
    "#   - N0 is your placeholder node (row 0 in Z).\n",
    "#   - N1..Nn correspond to rows 1..n in Z, in natural numeric order.\n",
    "\n",
    "# Build id -> numeric index mapping: \"Nk\" -> k  (0 reserved for N0)\n",
    "node_id_to_index: Dict[str, int] = {}\n",
    "\n",
    "for node in nodes:\n",
    "    node_id_str = node[\"id\"]         # e.g. \"N10\"\n",
    "    numeric_part = int(node_id_str[1:])  # \"10\" -> 10\n",
    "    node_id_to_index[node_id_str] = numeric_part\n",
    "\n",
    "# Number of REAL nodes = len(nodes). Total rows in Z = real_nodes + 1 (for N0)\n",
    "NUM_REAL_NODES = len(nodes)\n",
    "print(f\"Loaded {NUM_REAL_NODES} real nodes from entities.json\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD GAE EMBEDDINGS Z\n",
    "# ============================================================\n",
    "\n",
    "if not os.path.exists(Z_PATH):\n",
    "    raise FileNotFoundError(f\"Z embeddings file not found at: {Z_PATH}\")\n",
    "\n",
    "Z: torch.Tensor = torch.load(Z_PATH)\n",
    "if not isinstance(Z, torch.Tensor):\n",
    "    raise TypeError(\"Loaded object from Z.pt is not a torch.Tensor\")\n",
    "\n",
    "print(f\"Loaded Z from {Z_PATH}\")\n",
    "print(f\"Z shape: {tuple(Z.shape)}  (rows x latent_dim)\")\n",
    "\n",
    "# Z should have row 0 = N0, rows 1.. = N1..Nn\n",
    "NUM_ROWS_Z, LATENT_DIM = Z.shape\n",
    "\n",
    "# Basic consistency check: NUM_ROWS_Z should be NUM_REAL_NODES + 1 (for N0)\n",
    "if NUM_ROWS_Z != NUM_REAL_NODES + 1:\n",
    "    print(\n",
    "        f\"[WARN] Z rows ({NUM_ROWS_Z}) != num_real_nodes + 1 \"\n",
    "        f\"({NUM_REAL_NODES + 1}). Check your graph construction.\"\n",
    "    )\n",
    "\n",
    "# Convert to float32 numpy for FAISS\n",
    "Z_np = Z.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. CHOOSE DYNAMIC K' FOR ANN NEIGHBORS\n",
    "# ============================================================\n",
    "\n",
    "# We want k' scaled by real nodes, not counting N0.\n",
    "K_PRIME_SCALED = int(NUM_REAL_NODES * SCALING_RATIO)\n",
    "K_PRIME = max(1, min(K_PRIME_SCALED, NUM_REAL_NODES))  # can't exceed num real nodes\n",
    "\n",
    "print(f\"Total rows in Z (including N0): {NUM_ROWS_Z}\")\n",
    "print(f\"Number of real nodes (excluding N0): {NUM_REAL_NODES}\")\n",
    "print(f\"SCALING_RATIO = {SCALING_RATIO}\")\n",
    "print(f\"k' (neighbors per node) = {K_PRIME}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. BUILD FAISS INDEX AND RUN ANN SEARCH\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nBuilding FAISS Index on Z...\")\n",
    "index = faiss.IndexFlatL2(LATENT_DIM)\n",
    "index.add(Z_np)\n",
    "\n",
    "# For each row i in Z, search its K_PRIME + 1 nearest neighbors\n",
    "# (including itself as the closest).\n",
    "print(\"Running ANN search...\")\n",
    "D, I = index.search(Z_np, K_PRIME + 1)\n",
    "# D, I have shape (NUM_ROWS_Z, K_PRIME + 1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. BUILD CANDIDATE SET C1 (IN INDEX SPACE)\n",
    "# ============================================================\n",
    "\n",
    "C1: Set[Tuple[int, int]] = set()\n",
    "raw_candidate_list: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"\\nPopulating Candidate Set C1, excluding N0 (index 0)...\")\n",
    "\n",
    "for i in range(NUM_ROWS_Z):\n",
    "    # i is the current node index (0..NUM_ROWS_Z-1)\n",
    "    neighbors_i = I[i]  # shape (K_PRIME + 1,)\n",
    "\n",
    "    # We skip neighbors_i[0] because it's i itself (distance = 0).\n",
    "    # We also skip any neighbor that is i, or either is 0 (N0).\n",
    "    for neighbor_idx in neighbors_i[1:]:\n",
    "        j = int(neighbor_idx)\n",
    "\n",
    "        # Skip any pair involving placeholder N0 (index 0)\n",
    "        if i == 0 or j == 0:\n",
    "            continue\n",
    "\n",
    "        # Skip self-loop if it ever appears\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        pair = (i, j)\n",
    "        if pair in C1:\n",
    "            continue\n",
    "\n",
    "        C1.add(pair)\n",
    "\n",
    "        # Find the position of j within neighbors_i to extract distance\n",
    "        pos_in_search = np.where(neighbors_i == j)[0][0]\n",
    "        dist_sq = float(D[i, pos_in_search])  # FAISS returns squared L2\n",
    "        dist = float(np.sqrt(dist_sq))\n",
    "\n",
    "        raw_candidate_list.append(\n",
    "            {\n",
    "                \"source_idx\": i,\n",
    "                \"target_idx\": j,\n",
    "                \"distance_squared\": dist_sq,\n",
    "                \"distance\": dist,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# 6. FINAL REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Candidate Set C1 Results ---\")\n",
    "print(f\"Total rows in Z (including N0): {NUM_ROWS_Z}\")\n",
    "print(f\"Number of real nodes (excluding N0): {NUM_REAL_NODES}\")\n",
    "print(f\"k' (neighbors searched per node): {K_PRIME}\")\n",
    "print(f\"Size of Candidate Set C1 (|C1|): {len(C1)} pairs\")\n",
    "\n",
    "# Optional: show a small snippet of candidates\n",
    "SNIPPET = 10\n",
    "print(f\"\\nSnippet of first {SNIPPET} candidate pairs:\")\n",
    "for candidate in raw_candidate_list[:SNIPPET]:\n",
    "    i = candidate[\"source_idx\"]\n",
    "    j = candidate[\"target_idx\"]\n",
    "    dist = candidate[\"distance\"]\n",
    "    print(f\"  {i} -> {j} | Dist: {dist:.4f}\")\n",
    "\n",
    "# If you want a mapping back to node_ids (\"N1\", \"N2\", ...) later,\n",
    "# you can create it like this (assuming your numeric indices match ids):\n",
    "index_to_node_id: Dict[int, str] = {}\n",
    "for node in nodes:\n",
    "    num = int(node[\"id\"][1:])\n",
    "    index_to_node_id[num] = node[\"id\"]\n",
    "index_to_node_id[0] = \"N0\"  # placeholder\n",
    "\n",
    "# Example of translating one pair (if any exist):\n",
    "if raw_candidate_list:\n",
    "    ex = raw_candidate_list[0]\n",
    "    i_idx, j_idx = ex[\"source_idx\"], ex[\"target_idx\"]\n",
    "    print(\n",
    "        f\"\\nExample mapping: {i_idx} -> {j_idx} \"\n",
    "        f\"== {index_to_node_id.get(i_idx, '?')} -> {index_to_node_id.get(j_idx, '?')}\"\n",
    "    )\n",
    "\n",
    "# C1 and raw_candidate_list are now ready to be plugged into\n",
    "# your downstream causal verification / RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(raw_candidate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "K1: List[Tuple[int, int]] = []\n",
    "\n",
    "for candidate in raw_candidate_list:\n",
    "    # Extract the source and target indices and store as a tuple (i, j)\n",
    "    source = candidate['source_idx']\n",
    "    target = candidate['target_idx']\n",
    "    \n",
    "    K1.append((source, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3304e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(K1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save K1 to the saved stuff directory\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"extracted_output\", exist_ok=True)\n",
    "\n",
    "# Convert np.int64 → int for JSON serialization\n",
    "def convert_k1_to_jsonable(K1):\n",
    "    return [(int(a), int(b)) for (a, b) in K1]\n",
    "\n",
    "jsonable_K1 = convert_k1_to_jsonable(K1)\n",
    "\n",
    "save_path = \"extracted_output/k1.json\"\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(jsonable_K1, f, indent=2)\n",
    "\n",
    "print(f\"Saved K1 to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the K1\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "load_path = \"extracted_output/k1.json\"\n",
    "\n",
    "with open(load_path, \"r\") as f:\n",
    "    loaded_list = json.load(f)\n",
    "\n",
    "# Convert back to (int, np.int64) tuple format\n",
    "K1_loaded = [(int(a), np.int64(b)) for (a, b) in loaded_list]\n",
    "\n",
    "print(f\"Loaded K1 from {load_path}\")\n",
    "print(K1_loaded[:10])  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the Z (GAE embeddings)\n",
    "\n",
    "import torch\n",
    "\n",
    "load_path = \"extracted_output/Z.pt\"\n",
    "\n",
    "Z_loaded = torch.load(load_path, map_location=\"cpu\")  # safe for CPU use\n",
    "\n",
    "print(f\"Loaded Z from: {load_path}\")\n",
    "print(\"Shape:\", Z_loaded.shape)\n",
    "print(\"Dtype:\", Z_loaded.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the A_co_occur\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "load_path = \"extracted_output/A_co_occur.npz\"\n",
    "\n",
    "A_co_occur_loaded = load_npz(load_path)\n",
    "\n",
    "print(f\"Loaded A_co_occur from: {load_path}\")\n",
    "print(A_co_occur_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_prior = K1_loaded\n",
    "A_co_occur = A_co_occur_loaded\n",
    "Z = Z_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac355841",
   "metadata": {},
   "source": [
    "### Semantic filter \n",
    "\n",
    "Here our basic goal is to filter our K1 \"structurally plausible set\" down to a small K2 \"semantically plausible set\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\") # This part is still left (our search space is small so lets see this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad255a39",
   "metadata": {},
   "source": [
    "### Brief Summary of Pre-CoCaD filteration part:\n",
    "\n",
    "1. Structural Filter (GAE/ANN): You begin with $N^2$ possible pairs. The GAE/ANN process filters this down to the structurally plausible set, $\\mathbf{C}_1$.\n",
    "2. Input to Semantic Filter: The $\\mathbf{C}_1$ set is the full input to the Semantic Filter.\n",
    "3. Semantic Filter (Verification/CPC): This stage rigorously verifies and classifies the plausibility of each link in $\\mathbf{C}_1$. It discards links that are semantically implausible, not supported by context, or lack mechanism/temporality.\n",
    "4. Final Output: The resulting, highly curated, high-quality set is $\\mathbf{K}_2$.$$\\mathbf{C}_1 \\xrightarrow[\\text{Verification, Classification}]{\\text{Semantic Filter}} \\mathbf{K}_2$$\n",
    "\n",
    "The final $\\mathbf{K}_2$ set represents your ultimate $\\mathbf{E}_{\\text{prior}}$, containing only the most promising candidates ready for the final causal algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82924c6c",
   "metadata": {},
   "source": [
    "### Note: The loaded $E_{prior}$ is supposed to be direction-agnostic, meaning if it contains node (i,j) it should contain node (j,i) also..\n",
    "\n",
    "Reason: Structural Completeness\n",
    "\n",
    "The primary goal of $E_{\\text{prior}}$ is to provide a complete list of plausible hypotheses for the final causal discovery algorithm.\n",
    "1. Symmetry of Evidence: The initial structural score ($Z_i Z_j^T$) is symmetric. If $i$ is structurally close to $j$, then $j$ is equally close to $i$.\n",
    "2. Causal Test Requirement: The final causal discovery phase must individually test the hypothesis $i \\to j$ against the hypothesis $j \\to i$. If you only included $(i, j)$, you would prematurely exclude the possibility that $j$ causes $i$.\n",
    "Therefore, to maintain structural completeness, both directed hypotheses must be present in $E_{\\text{prior}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ef36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_prior = K1_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ab113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Set\n",
    "\n",
    "\n",
    "def enforce_bidirectional_symmetry(initial_E_prior: List[Tuple[int, np.int64]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Takes a list of directed pairs (i, j) and ensures the inverse pair (j, i) \n",
    "    also exists, converting all elements to native Python integers (int) for cleanup.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert initial list to a Set of tuples (i, j) for fast lookup and deduplication.\n",
    "    # Convert np.int64 to standard Python int during this step.\n",
    "    full_set: Set[Tuple[int, int]] = set()\n",
    "    \n",
    "    # First pass: Populate the set with existing and inverse pairs\n",
    "    for i_np, j_np in initial_E_prior:\n",
    "        # Convert np.int64 to standard Python int\n",
    "        i = int(i_np)\n",
    "        j = int(j_np)\n",
    "        \n",
    "        # Add the original pair\n",
    "        full_set.add((i, j))\n",
    "        \n",
    "        # Add the inverse pair (j, i)\n",
    "        full_set.add((j, i))\n",
    "        \n",
    "    # 2. Convert the final Set back to a List\n",
    "    # Sorting is optional but makes debugging easier and guarantees a deterministic output order.\n",
    "    final_E_prior = sorted(list(full_set))\n",
    "    \n",
    "    return final_E_prior\n",
    "\n",
    "# Update the E_prior list in place\n",
    "E_prior = enforce_bidirectional_symmetry(E_prior)\n",
    "\n",
    "print(\"--- E_prior Symmetry Enforcement Complete ---\")\n",
    "print(f\"Initial size (approx): {29}\")\n",
    "print(f\"Final size: {len(E_prior)} (Should be ~2x the number of unique links)\")\n",
    "print(\"\\nSnippet of Final E_prior:\")\n",
    "print(E_prior[:5])\n",
    "print(\"...\")\n",
    "print(E_prior[-5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
