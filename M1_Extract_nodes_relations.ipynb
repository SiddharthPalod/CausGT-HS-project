{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827f1f32",
   "metadata": {},
   "source": [
    "# 3.2 Learning our correlational matrices $A_w$\n",
    "\n",
    "### Our correlational matrices $A_w$ = {$W_1$. $W_2$, .... $W_k$}, these are a set of K weighted adjacency matrices.. Each of $W_r \\in [0,1]^{N \\times N}$ above represents reationships between any of the N nodes and any of the other N nodes for the specific relation r. Hence $|E_r|$ is the number of non-zero entries in $W_r$. \n",
    "Note: Here we are just finding the 1-hop correlational links.\n",
    "\n",
    "### Hence total edges would be |E| = $\\Sigma |E_r|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f40e86",
   "metadata": {},
   "source": [
    "## 3.2.1 Document Parsing and Global Node identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b16040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is just for importing some important stuff throughout, just pretty\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28968dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "print(\"Available models for content generation:\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b9125",
   "metadata": {},
   "source": [
    "#### Chunk the text appropriately (NM just basic token chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load text from a file instead of using dedent\n",
    "    file_path = \"testing_text/sample_text.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        document_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# -------------------------\n",
    "# Chunking logic (same as before)\n",
    "# -------------------------\n",
    "def chunk_text(text: str, size: int = 1800) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Save chunks into JSON + per-chunk text files\n",
    "# -------------------------\n",
    "def save_document_chunks(document_text: str, out_dir: str = \"extracted_output/chunks\", chunk_size: int = 1800):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    chunks = chunk_text(document_text, size=chunk_size)\n",
    "\n",
    "    chunk_records = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        record = {\n",
    "            \"chunk_index\": idx,\n",
    "            \"chunk_text\": chunk\n",
    "        }\n",
    "        chunk_records.append(record)\n",
    "\n",
    "        # Save each chunk as a .txt file\n",
    "        with open(os.path.join(out_dir, f\"chunk_{idx}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Also save a JSON list of all chunks\n",
    "    with open(os.path.join(out_dir, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks into {out_dir}\")\n",
    "    return chunk_records\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load text from a file instead of using dedent\n",
    "    file_path = \"testing_text/sample_text.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        document_text = f.read()\n",
    "\n",
    "    save_document_chunks(document_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc525451",
   "metadata": {},
   "source": [
    "#### Now lets Extract our necessary entities (Do note the node format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from google.genai import Client, types\n",
    "from google.genai import errors as genai_errors\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "# NEW: fallback queue (added without removing original MODEL)\n",
    "MODEL_CANDIDATES = [\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "]\n",
    "\n",
    "CHUNK_SIZE = 1800        # characters per chunk (roughly page-ish)\n",
    "DELAY_SECONDS = 15        # small delay between calls (lite has better quota, but be nice)\n",
    "\n",
    "client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pydantic Schema for Entities\n",
    "# =========================\n",
    "\n",
    "class EntityNode(BaseModel):\n",
    "    \"\"\"\n",
    "    Single entity/event node with metadata.\n",
    "    We let the LLM fill these fields; we will assign global IDs later.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    type: Literal[\"event\", \"entity\", \"actor\", \"amount\", \"location\"]\n",
    "    time: Optional[str] = None        # e.g. \"July 15th\"\n",
    "    location: Optional[str] = None    # e.g. \"Valeron banking district\"\n",
    "    description: Optional[str] = None # short gloss / summary\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safe LLM Wrapper\n",
    "# =========================\n",
    "\n",
    "def safe_llm_structured(\n",
    "    prompt: str,\n",
    "    schema,\n",
    "    temperature: float = 0.2,\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls Gemini with a structured response schema (Pydantic / typing).\n",
    "    Retries on transient 429 / RESOURCE_EXHAUSTED.\n",
    "    \"\"\"\n",
    "\n",
    "    last_exc: Optional[Exception] = None\n",
    "\n",
    "    # NEW: Loop over fallback models while preserving your original behavior\n",
    "    for model_name in MODEL_CANDIDATES:\n",
    "\n",
    "        print(f\"\\n[LLM] Trying model: {model_name}\")\n",
    "\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "        )\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                print(f\"[LLM] Call attempt {attempt} with {model_name}\")\n",
    "                resp = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=prompt,\n",
    "                    config=config,\n",
    "                )\n",
    "                return resp\n",
    "\n",
    "            except genai_errors.ClientError as e:\n",
    "                msg = str(e)\n",
    "                last_exc = e\n",
    "\n",
    "                # NEW: Hard quota exhaustion → switch model\n",
    "                if \"quota\" in msg.lower() and (\"exceeded\" in msg.lower() or \"exhausted\" in msg.lower()):\n",
    "                    print(f\"[LLM] Daily quota exhausted for {model_name}. Switching to next model...\")\n",
    "                    break  # break retry loop, move to next model\n",
    "\n",
    "                # ORIGINAL: retry on transient errors\n",
    "                if \"RESOURCE_EXHAUSTED\" in msg or \"429\" in msg:\n",
    "                    print(\"[LLM] Rate limit / quota hit, backing off...\")\n",
    "                    time.sleep(DELAY_SECONDS)\n",
    "                    continue\n",
    "\n",
    "                # ORIGINAL: raise all other errors\n",
    "                raise\n",
    "\n",
    "        print(f\"[LLM] Model {model_name} failed after retries, moving to next fallback...\")\n",
    "\n",
    "    # If we exhaust all models, raise the last caught exception\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"safe_llm_structured failed unexpectedly.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Chunking\n",
    "# =========================\n",
    "\n",
    "def chunk_text(text: str, size: int = CHUNK_SIZE) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple character-based chunking.\n",
    "    (You can replace this later with a semantic chunker if you want.)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Entity Extraction per Chunk (GraphRAG-style “graph extraction”)\n",
    "# =========================\n",
    "\n",
    "ENTITY_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an information extraction system building a knowledge graph.\n",
    "\n",
    "From the following text chunk, extract all important EVENTS and ENTITIES as\n",
    "structured JSON.\n",
    "\n",
    "For each item, return an object with:\n",
    "- \"name\": concise description of the event/entity (no longer than 15 words)\n",
    "- \"type\": one of [\"event\", \"entity\", \"actor\", \"amount\", \"location\"]\n",
    "- \"time\":  date/time phrase exactly as in the text, if any\n",
    "- \"location\":  location phrase exactly as in the text, if any\n",
    "- \"description\":  1–2 sentence summary of the role of this node\n",
    "\n",
    "Return ONLY a JSON array, for example:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"name\": \"FCU announces formal investigation\",\n",
    "    \"type\": \"event\",\n",
    "    \"time\": \"July 15th\",\n",
    "    \"location\": \"Valeron\",\n",
    "    \"description\": \"The Financial Crimes Unit officially launches its probe into Titan.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "TEXT CHUNK:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Wrapper model because Gemini cannot return a top-level List[..]\n",
    "class EntityList(BaseModel):\n",
    "    items: List[EntityNode]\n",
    "\n",
    "\n",
    "def extract_entities_from_chunk(chunk: str) -> List[EntityNode]:\n",
    "    \"\"\"\n",
    "    Sends one chunk to Gemini and returns a list[EntityNode].\n",
    "    \"\"\"\n",
    "    prompt = ENTITY_PROMPT_TEMPLATE.format(chunk=chunk)\n",
    "\n",
    "    resp = safe_llm_structured(\n",
    "        prompt=prompt,\n",
    "        schema=EntityList,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    parsed = getattr(resp, \"parsed\", None)\n",
    "\n",
    "    if isinstance(parsed, EntityList):\n",
    "        return parsed.items\n",
    "\n",
    "    # Fallback: try raw JSON if schema somehow failed\n",
    "    try:\n",
    "        raw = json.loads(resp.text)\n",
    "        return [EntityNode(**obj) for obj in raw]\n",
    "    except:\n",
    "        print(\"[WARN] Empty/invalid entity response; returning [].\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =========================\n",
    "# High-level Pipeline\n",
    "# =========================\n",
    "\n",
    "def extract_entities_with_metadata(document_text: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline:\n",
    "\n",
    "    1. Chunk text.\n",
    "    2. For each chunk, call LLM to get EntityNodes.\n",
    "    3. Add chunk index as provenance.\n",
    "    4. Deduplicate globally.\n",
    "    5. Assign global IDs N1, N2, ...\n",
    "\n",
    "    Returns:\n",
    "        nodes: list of dicts with id, name, type, time, location, description, source_chunks\n",
    "        raw_per_chunk: list of dicts for debugging/inspection: {\"chunk_index\", \"chunk_text\", \"entities\"}\n",
    "    \"\"\"\n",
    "    chunks = chunk_text(document_text)\n",
    "    all_raw_entities: List[Dict[str, Any]] = []\n",
    "    provenance_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"\\n--- Processing chunk {idx + 1}/{len(chunks)} ---\")\n",
    "        entities = extract_entities_from_chunk(chunk)\n",
    "\n",
    "        # Save raw (per-chunk) info for debugging / analysis\n",
    "        provenance_records.append(\n",
    "            {\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk,\n",
    "                \"entities\": [e.dict() for e in entities],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Attach chunk index to each entity for later provenance\n",
    "        for e in entities:\n",
    "            d = e.dict()\n",
    "            d[\"_source_chunks\"] = {idx}  # use a set for merging later\n",
    "            all_raw_entities.append(d)\n",
    "\n",
    "        time.sleep(0.5)  # very light additional pacing\n",
    "\n",
    "    # ------------- Deduplication across chunks -------------\n",
    "    # Key: (name, type, time, location)\n",
    "    merged: Dict[Tuple[str, str, Optional[str], Optional[str]], Dict[str, Any]] = {}\n",
    "\n",
    "    for ent in all_raw_entities:\n",
    "        key = (ent[\"name\"], ent[\"type\"], ent.get(\"time\"), ent.get(\"location\"))\n",
    "\n",
    "        if key not in merged:\n",
    "            merged[key] = ent\n",
    "        else:\n",
    "            # Merge provenance + maybe pick longer description\n",
    "            merged[key][\"_source_chunks\"] |= ent[\"_source_chunks\"]\n",
    "            desc1 = merged[key].get(\"description\") or \"\"\n",
    "            desc2 = ent.get(\"description\") or \"\"\n",
    "            # Keep the longer, more informative description\n",
    "            if len(desc2) > len(desc1):\n",
    "                merged[key][\"description\"] = desc2\n",
    "\n",
    "    dedup_entities = list(merged.values())\n",
    "\n",
    "    # Assign global IDs N1, N2, ...\n",
    "    nodes: List[Dict[str, Any]] = []\n",
    "    for i, ent in enumerate(dedup_entities, start=1):\n",
    "        node = {\n",
    "            \"id\": f\"N{i}\",\n",
    "            \"name\": ent[\"name\"],\n",
    "            \"type\": ent[\"type\"],\n",
    "            \"time\": ent.get(\"time\"),\n",
    "            \"location\": ent.get(\"location\"),\n",
    "            \"description\": ent.get(\"description\"),\n",
    "            # convert set of chunk indices to sorted list\n",
    "            \"source_chunks\": sorted(list(ent[\"_source_chunks\"])),\n",
    "        }\n",
    "        nodes.append(node)\n",
    "\n",
    "    print(f\"\\nExtracted {len(nodes)} unique entity/event nodes.\")\n",
    "    return nodes, provenance_records\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example Usage\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Supply your document text here\n",
    "    #    e.g., from a file:\n",
    "    #\n",
    "    # with open(\"my_long_doc.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     document_text = f.read()\n",
    "    #\n",
    "    # For now, just set document_text = sample_text variable.\n",
    "    from textwrap import dedent\n",
    "\n",
    "    # 2) Run extraction\n",
    "    nodes, per_chunk = extract_entities_with_metadata(document_text)\n",
    "\n",
    "    # 3) Save results (GraphRAG-style “graph docs” but only nodes for now)\n",
    "    os.makedirs(\"extracted_output\", exist_ok=True)\n",
    "\n",
    "    with open(\"extracted_output/entities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(nodes, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"extracted_output/entities_per_chunk_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(per_chunk, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  extracted_output/entities.json\")\n",
    "    print(\"  extracted_output/entities_par_chunk_debug.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved entities\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_extracted_entities(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - entities.json              → list of deduplicated entity/event nodes\n",
    "      - entities_per_chunk_debug.json → raw per-chunk extraction data\n",
    "    \"\"\"\n",
    "\n",
    "    entities_path = os.path.join(base_dir, \"entities.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"entities_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(entities_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {entities_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(entities_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        entities = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - entities.json\")\n",
    "    print(\" - entities_per_chunk_debug.json\")\n",
    "\n",
    "    return entities, per_chunk\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    nodes, nodes_per_chunk = load_extracted_entities()\n",
    "    print(f\"Total nodes loaded: {len(nodes)}\")\n",
    "    print(f\"Total chunks loaded: {len(nodes_per_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308147e2",
   "metadata": {},
   "source": [
    "#### Now lets Extract our necessary Relations (Do note the format) for the above extracted node entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from google.genai import Client, types\n",
    "from google.genai import errors as genai_errors\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "# List of models to try, in order.\n",
    "MODEL_CANDIDATES = [\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "]\n",
    "\n",
    "MODEL = MODEL_CANDIDATES[0]\n",
    "CURRENT_MODEL_INDEX = 0\n",
    "EXHAUSTED_MODELS = set()\n",
    "\n",
    "DELAY_SECONDS = 15  # light rate limiting\n",
    "\n",
    "client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pydantic Schemas for Relations\n",
    "# =========================\n",
    "\n",
    "class RelationEdge(BaseModel):\n",
    "    \"\"\"\n",
    "    Single relation between two graph nodes (by node ID).\n",
    "    \"\"\"\n",
    "    source_id: str = Field(\n",
    "        description=\"ID of the source node (e.g., 'N3'). Must come from the provided node list.\"\n",
    "    )\n",
    "    target_id: str = Field(\n",
    "        description=\"ID of the target node (e.g., 'N7'). Must come from the provided node list.\"\n",
    "    )\n",
    "    relation: str = Field(\n",
    "        description=(\n",
    "            \"A short relation phrase (2-4 words max) describing the semantic relation \"\n",
    "            \"between source and target (e.g., 'leads to', 'investigates', 'causes', \"\n",
    "            \"'associated with'). Do NOT output a full sentence.\"\n",
    "        )\n",
    "    )\n",
    "    description: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"One short sentence explaining the relation in natural language.\"\n",
    "    )\n",
    "    evidence: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"A sentence or short snippet copied from the text that supports this relation.\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        description=\"A float between 0.0 and 1.0 indicating confidence in the correctness of this relation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RelationList(BaseModel):\n",
    "    \"\"\"\n",
    "    Wrapper for a list of relations (Gemini can't return top-level bare list).\n",
    "    \"\"\"\n",
    "    relations: List[RelationEdge] = Field(\n",
    "        description=\"List of directional relations between the provided nodes.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper for model switching\n",
    "# =========================\n",
    "\n",
    "def _switch_to_next_model():\n",
    "    \"\"\"\n",
    "    Mark current MODEL as exhausted and switch to the next available one\n",
    "    in MODEL_CANDIDATES. Raises if all are exhausted.\n",
    "    \"\"\"\n",
    "    global MODEL, CURRENT_MODEL_INDEX\n",
    "\n",
    "    EXHAUSTED_MODELS.add(MODEL)\n",
    "\n",
    "    for i in range(len(MODEL_CANDIDATES)):\n",
    "        idx = (CURRENT_MODEL_INDEX + 1 + i) % len(MODEL_CANDIDATES)\n",
    "        candidate = MODEL_CANDIDATES[idx]\n",
    "        if candidate not in EXHAUSTED_MODELS:\n",
    "            MODEL = candidate\n",
    "            CURRENT_MODEL_INDEX = idx\n",
    "            print(f\"[LLM] (relations) Switching to backup model: {MODEL}\")\n",
    "            return\n",
    "\n",
    "    raise RuntimeError(\"All configured models appear exhausted/quota-limited for today (relations).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safe LLM Wrapper (Structured)\n",
    "# =========================\n",
    "\n",
    "def safe_llm_structured(\n",
    "    prompt: str,\n",
    "    schema,\n",
    "    temperature: float = 0.1,\n",
    "    max_retries: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls Gemini with a structured response schema.\n",
    "    Retries on transient 429 / RESOURCE_EXHAUSTED.\n",
    "    Also rotates across multiple models if one hits quota.\n",
    "    \"\"\"\n",
    "    last_exc: Optional[Exception] = None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        from_model = MODEL\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            print(f\"[LLM] (relations) Call attempt {attempt} with {from_model}\")\n",
    "            resp = client.models.generate_content(\n",
    "                model=from_model,\n",
    "                contents=prompt,\n",
    "                config=config,\n",
    "            )\n",
    "            return resp\n",
    "        except genai_errors.ClientError as e:\n",
    "            msg = str(e)\n",
    "            if (\n",
    "                \"RESOURCE_EXHAUSTED\" in msg\n",
    "                or \"429\" in msg\n",
    "                or \"exceeded your current quota\" in msg.lower()\n",
    "            ):\n",
    "                print(f\"[LLM] (relations) Model {from_model} hit quota / rate limit: {msg}\")\n",
    "                last_exc = e\n",
    "                try:\n",
    "                    _switch_to_next_model()\n",
    "                except RuntimeError as switch_err:\n",
    "                    print(\"[LLM] (relations) No backup models left.\")\n",
    "                    raise switch_err\n",
    "                time.sleep(DELAY_SECONDS)\n",
    "                continue\n",
    "\n",
    "            raise\n",
    "\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"safe_llm_structured (relations) failed unexpectedly.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Relation Extraction Prompt\n",
    "# =========================\n",
    "\n",
    "RELATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are building a knowledge graph from a document.\n",
    "\n",
    "You are given:\n",
    "1) A TEXT CHUNK from the document.\n",
    "2) A list of NODES that appear in this chunk. Each node has:\n",
    "   - \"id\": global node ID like \"N3\"\n",
    "   - \"name\": short label\n",
    "   - \"type\": one of [\"event\", \"entity\", \"actor\", \"amount\", \"location\"]\n",
    "   - \"time\": optional date/time\n",
    "   - \"location\": optional location\n",
    "   - \"description\": optional summary\n",
    "\n",
    "Your task: Extract all meaningful, **directional** relations between these nodes\n",
    "based ONLY on the given TEXT CHUNK.\n",
    "\n",
    "Rules:\n",
    "- Only use node IDs that appear in the provided NODES list.\n",
    "- Each relation is directional: (source_id -> target_id).\n",
    "- The \"relation\" field must be a **short phrase (2–4 words)**, NOT a full sentence.\n",
    "  Examples: \"leads to\", \"causes\", \"results in\", \"investigates\", \"freezes assets of\",\n",
    "  \"occurs after\", \"associated with\".\n",
    "- \"description\": one short sentence in natural language explaining the relation.\n",
    "- \"evidence\": copy a sentence or short snippet from the TEXT CHUNK that justifies this relation.\n",
    "- If you are NOT confident about any relation between a pair, do not invent one.\n",
    "- It is allowed to return an empty list if no strong relations are present.\n",
    "- Include a 'confidence' score between 0.0 and 1.0. \n",
    "  Use ≥0.9 only when the relation is explicitly and unambiguously stated.\n",
    "\n",
    "\n",
    "Return ONLY a JSON object in this exact form:\n",
    "{{\n",
    "  \"relations\": [\n",
    "    {{\n",
    "      \"source_id\": \"N3\",\n",
    "      \"target_id\": \"N7\",\n",
    "      \"relation\": \"leads to\",\n",
    "      \"description\": \"FCU investigation follows the spike in black market sales.\",\n",
    "      \"evidence\": \"On July 15th, the Financial Crimes Unit (FCU) announced a formal investigation...\",\n",
    "      \"confidence\": 0.92\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "TEXT CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "NODES IN THIS CHUNK:\n",
    "{nodes_json}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities to Map Entities -> Node IDs\n",
    "# =========================\n",
    "\n",
    "def build_entity_key(ent: Dict[str, Any]) -> Tuple[str, str, Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Create a stable key from an entity dict:\n",
    "    (name, type, time, location)\n",
    "    This matches the dedup key used when building entities.json.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        ent[\"name\"],\n",
    "        ent[\"type\"],\n",
    "        ent.get(\"time\"),\n",
    "        ent.get(\"location\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_node_key_index(nodes: List[Dict[str, Any]]) -> Dict[Tuple[str, str, Optional[str], Optional[str]], str]:\n",
    "    \"\"\"\n",
    "    From the global node list (entities.json), build a mapping:\n",
    "        (name, type, time, location) -> node_id (\"N1\", \"N2\", ...)\n",
    "    \"\"\"\n",
    "    index: Dict[Tuple[str, str, Optional[str], Optional[str]], str] = {}\n",
    "    for node in nodes:\n",
    "        key = (\n",
    "            node[\"name\"],\n",
    "            node[\"type\"],\n",
    "            node.get(\"time\"),\n",
    "            node.get(\"location\"),\n",
    "        )\n",
    "        index[key] = node[\"id\"]\n",
    "    return index\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Per-chunk Relation Extraction\n",
    "# =========================\n",
    "\n",
    "def extract_relations_for_chunk(\n",
    "    chunk_record: Dict[str, Any],\n",
    "    node_key_index: Dict[Tuple[str, str, Optional[str], Optional[str]], str],\n",
    ") -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a single chunk:\n",
    "    - Build the list of chunk-local nodes with their global IDs.\n",
    "    - Ask Gemini to extract relations between them.\n",
    "    - Return:\n",
    "        relations_flat: List[dict] with source_id, target_id, relation, description, evidence, confidence, _chunk_index\n",
    "        debug_record:   Dict with chunk_index, chunk_text, nodes, relations (for debug JSON)\n",
    "    \"\"\"\n",
    "    chunk_index = chunk_record[\"chunk_index\"]\n",
    "    chunk_text = chunk_record[\"chunk_text\"]\n",
    "    entities_in_chunk = chunk_record[\"entities\"]\n",
    "\n",
    "    # Map chunk entities -> node IDs using key (name, type, time, location)\n",
    "    chunk_nodes: List[Dict[str, Any]] = []\n",
    "    for ent in entities_in_chunk:\n",
    "        key = build_entity_key(ent)\n",
    "        node_id = node_key_index.get(key)\n",
    "        if not node_id:\n",
    "            # Best-effort: skip entities that didn't make it into the final node set\n",
    "            continue\n",
    "        chunk_nodes.append(\n",
    "            {\n",
    "                \"id\": node_id,\n",
    "                \"name\": ent[\"name\"],\n",
    "                \"type\": ent[\"type\"],\n",
    "                \"time\": ent.get(\"time\"),\n",
    "                \"location\": ent.get(\"location\"),\n",
    "                \"description\": ent.get(\"description\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # If fewer than 2 nodes in this chunk, no relations to extract\n",
    "    if len(chunk_nodes) < 2:\n",
    "        debug_record = {\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"nodes\": chunk_nodes,\n",
    "            \"relations\": [],\n",
    "        }\n",
    "        return [], debug_record\n",
    "\n",
    "    prompt = RELATION_PROMPT_TEMPLATE.format(\n",
    "        chunk_text=chunk_text,\n",
    "        nodes_json=json.dumps(chunk_nodes, indent=2, ensure_ascii=False),\n",
    "    )\n",
    "\n",
    "    resp = safe_llm_structured(\n",
    "        prompt=prompt,\n",
    "        schema=RelationList,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    parsed = getattr(resp, \"parsed\", None)\n",
    "    relations: List[RelationEdge] = []\n",
    "\n",
    "    if isinstance(parsed, RelationList):\n",
    "        relations = parsed.relations\n",
    "    else:\n",
    "        # Fallback: try raw JSON\n",
    "        try:\n",
    "            raw = json.loads(resp.text)\n",
    "            raw_rels = raw.get(\"relations\", [])\n",
    "            relations = [RelationEdge(**r) for r in raw_rels]\n",
    "        except Exception:\n",
    "            print(f\"[WARN] Invalid relation response for chunk {chunk_index}; treating as empty.\")\n",
    "            relations = []\n",
    "\n",
    "    # Convert to plain dicts for serialization, tagging with chunk index\n",
    "    relations_flat: List[Dict[str, Any]] = []\n",
    "    for r in relations:\n",
    "        d = r.dict()\n",
    "        d[\"_chunk_index\"] = chunk_index\n",
    "        relations_flat.append(d)\n",
    "\n",
    "    debug_record = {\n",
    "        \"chunk_index\": chunk_index,\n",
    "        \"chunk_text\": chunk_text,\n",
    "        \"nodes\": chunk_nodes,\n",
    "        \"relations\": relations_flat,\n",
    "    }\n",
    "\n",
    "    return relations_flat, debug_record\n",
    "\n",
    "\n",
    "# =========================\n",
    "# High-level Pipeline\n",
    "# =========================\n",
    "\n",
    "def extract_relations_from_entities_and_chunks(\n",
    "    nodes: List[Dict[str, Any]],\n",
    "    per_chunk: List[Dict[str, Any]],\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    End-to-end relation extraction:\n",
    "\n",
    "    Inputs:\n",
    "      - nodes:       global entity/event nodes (id, name, type, time, location, ...)\n",
    "      - per_chunk:   list of {\"chunk_index\", \"chunk_text\", \"entities\": [...]}\n",
    "                     as produced by entities_per_chunk_debug.json\n",
    "\n",
    "    Returns:\n",
    "      - edges: list of deduplicated relations across the document, each:\n",
    "            {\n",
    "              \"source_id\": \"N3\",\n",
    "              \"target_id\": \"N7\",\n",
    "              \"relation\": \"leads to\",\n",
    "              \"description\": \"...\",\n",
    "              \"evidence\": \"...\",\n",
    "              \"confidence\": 0.91,\n",
    "              \"source_chunks\": [0, 2, ...]\n",
    "            }\n",
    "      - per_chunk_relations: list of debug records for each chunk:\n",
    "            {\n",
    "              \"chunk_index\": ...,\n",
    "              \"chunk_text\": ...,\n",
    "              \"nodes\": [...],\n",
    "              \"relations\": [...]\n",
    "            }\n",
    "    \"\"\"\n",
    "    node_key_index = build_node_key_index(nodes)\n",
    "\n",
    "    all_relations_flat: List[Dict[str, Any]] = []\n",
    "    per_chunk_relations: List[Dict[str, Any]] = []\n",
    "\n",
    "    print(f\"Total chunks to process for relations: {len(per_chunk)}\")\n",
    "\n",
    "    for idx, chunk_rec in enumerate(per_chunk):\n",
    "        print(f\"\\n--- Extracting relations for chunk {idx + 1}/{len(per_chunk)} ---\")\n",
    "\n",
    "        rels_flat, debug_rec = extract_relations_for_chunk(chunk_rec, node_key_index)\n",
    "        all_relations_flat.extend(rels_flat)\n",
    "        per_chunk_relations.append(debug_rec)\n",
    "\n",
    "        time.sleep(0.5)  # light pacing\n",
    "\n",
    "    # -------- Deduplicate relations across document ----------\n",
    "    merged_edges: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "\n",
    "    for rel in all_relations_flat:\n",
    "        key = (rel[\"source_id\"], rel[\"target_id\"], rel[\"relation\"])\n",
    "        chunk_idx = rel.get(\"_chunk_index\")\n",
    "        conf = float(rel.get(\"confidence\", 0.0))\n",
    "\n",
    "        if key not in merged_edges:\n",
    "            merged_edges[key] = {\n",
    "                \"source_id\": rel[\"source_id\"],\n",
    "                \"target_id\": rel[\"target_id\"],\n",
    "                \"relation\": rel[\"relation\"],\n",
    "                \"description\": rel.get(\"description\"),\n",
    "                \"evidence\": rel.get(\"evidence\"),\n",
    "                \"confidence_sum\": conf,\n",
    "                \"count\": 1,\n",
    "                \"source_chunks\": set([chunk_idx] if chunk_idx is not None else []),\n",
    "            }\n",
    "        else:\n",
    "            payload = merged_edges[key]\n",
    "            # aggregate confidence as average over occurrences\n",
    "            payload[\"confidence_sum\"] += conf\n",
    "            payload[\"count\"] += 1\n",
    "            if chunk_idx is not None:\n",
    "                payload[\"source_chunks\"].add(chunk_idx)\n",
    "            # keep the first non-empty description / evidence if missing\n",
    "            if not payload.get(\"description\") and rel.get(\"description\"):\n",
    "                payload[\"description\"] = rel[\"description\"]\n",
    "            if not payload.get(\"evidence\") and rel.get(\"evidence\"):\n",
    "                payload[\"evidence\"] = rel[\"evidence\"]\n",
    "\n",
    "    edges: List[Dict[str, Any]] = []\n",
    "    for (s, t, r), payload in merged_edges.items():\n",
    "        avg_conf = (\n",
    "            payload[\"confidence_sum\"] / payload[\"count\"]\n",
    "            if payload[\"count\"] > 0 else 0.0\n",
    "        )\n",
    "        edges.append(\n",
    "            {\n",
    "                \"source_id\": s,\n",
    "                \"target_id\": t,\n",
    "                \"relation\": r,\n",
    "                \"description\": payload.get(\"description\"),\n",
    "                \"evidence\": payload.get(\"evidence\"),\n",
    "                \"confidence\": avg_conf,\n",
    "                \"source_chunks\": sorted(list(payload[\"source_chunks\"])),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"\\nExtracted {len(edges)} unique relations across all chunks.\")\n",
    "    return edges, per_chunk_relations\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example Usage\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Assumes you already ran the entity-extraction script and have:\n",
    "      extracted_output/entities.json\n",
    "      extracted_output/entities_per_chunk_debug.json\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(\"extracted_output\", exist_ok=True)\n",
    "\n",
    "    # 1) Load entities + per-chunk debug from previous step\n",
    "    with open(\"extracted_output/entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        nodes = json.load(f)\n",
    "\n",
    "    with open(\"extracted_output/entities_per_chunk_debug.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk = json.load(f)\n",
    "\n",
    "    # 2) Run relation extraction\n",
    "    relations, relations_per_chunk = extract_relations_from_entities_and_chunks(\n",
    "        nodes=nodes,\n",
    "        per_chunk=per_chunk,\n",
    "    )\n",
    "\n",
    "    # 3) Save outputs\n",
    "    with open(\"extracted_output/relations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(relations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"extracted_output/relations_per_chunk_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(relations_per_chunk, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  extracted_output/relations.json\")\n",
    "    print(\"  extracted_output/relations_per_chunk_debug.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157248fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved relations\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_extracted_relations(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - relations.json\n",
    "      - relations_per_chunk_debug.json\n",
    "\n",
    "    Returns:\n",
    "      relations: list of all deduplicated relation edges\n",
    "      per_chunk_relations: list of per-chunk debug relation records\n",
    "    \"\"\"\n",
    "\n",
    "    relations_path = os.path.join(base_dir, \"relations.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"relations_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(relations_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {relations_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(relations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        relations = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk_relations = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - relations.json\")\n",
    "    print(\" - relations_per_chunk_debug.json\")\n",
    "\n",
    "    return relations, per_chunk_relations\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    relations, relations_per_chunk = load_extracted_relations()\n",
    "\n",
    "    print(f\"Total relations loaded: {len(relations)}\")\n",
    "    print(f\"Total chunks returned: {len(relations_per_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a01f2",
   "metadata": {},
   "source": [
    "#### Extract our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ee592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wtpsplit import SaT\n",
    "\n",
    "# --- 2. Load the SaT Model ---\n",
    "# Use 'sat-3l-sm' for a good balance of quality and fast inference.\n",
    "# The model will be downloaded automatically the first time this runs.\n",
    "print(\"Loading SaT model...\")\n",
    "try:\n",
    "    # Use the small/medium model for general sentence segmentation tasks\n",
    "    sat = SaT(\"sat-3l-sm\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading WtpSplit model: {e}\")\n",
    "    # Handle the error or exit gracefully\n",
    "\n",
    "# --- 3. Split the text ---\n",
    "# The .split() method processes the text and returns a list of segmented sentences.\n",
    "# By default, it handles newlines intelligently.\n",
    "sentence_list = sat.split(document_text)\n",
    "\n",
    "print(\"\\n--- Split Sentences (SaT) ---\")\n",
    "for i, sent in enumerate(sentence_list):\n",
    "    # Strip whitespace/newlines that the model might leave at the start/end of sentences\n",
    "    print(f\"{i+1}: {sent.strip()}\")\n",
    "\n",
    "print(\"\\nFinal Output Type:\", type(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bebbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [sent.strip() for sent in sentence_list if sent.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Split Sentences (SaT) ---\")\n",
    "for i, sent in enumerate(sentence_list):\n",
    "    # Strip whitespace/newlines that the model might leave at the start/end of sentences\n",
    "    print(f\"{i+1}: {sent.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Assuming 'sentence_list' is defined and populated earlier in your notebook.\n",
    "\n",
    "# --- CODE TO SAVE TO FILE ---\n",
    "\n",
    "# 1. Define the directory and filename\n",
    "output_directory = \"extracted_output\"\n",
    "json_filename = \"segmented_sentences.json\"\n",
    "\n",
    "# 2. Ensure the output directory exists\n",
    "# This handles the case where 'saved_stuff' hasn't been created yet.\n",
    "try:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating directory: {e}\")\n",
    "    # If the directory can't be created, the save operation will fail later, \n",
    "    # but we handle this gracefully.\n",
    "\n",
    "# 3. Construct the full file path\n",
    "file_path = os.path.join(output_directory, json_filename)\n",
    "\n",
    "# 4. Define the data structure you want to save\n",
    "# Assuming paragraph_list is already populated\n",
    "data_to_save = {\n",
    "    \"sentences\": sentence_list \n",
    "}\n",
    "\n",
    "# 5. Write the data to the file\n",
    "try:\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Use json.dump() to write the dictionary directly to the file with formatting\n",
    "        json.dump(data_to_save, f, indent=2)\n",
    "        \n",
    "    print(f\"\\n✅ Success: Sentences saved to {file_path} (JSON Format)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error saving JSON file: {e}\")\n",
    "\n",
    "# --- END SAVE CODE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea72285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_filename = \"segmented_sentences.json\"\n",
    "sentence_file_path = os.path.join(\"extracted_output\", sentence_filename)\n",
    "retrieved_sentences = None\n",
    "\n",
    "print(f\"\\nAttempting to load sentences from: {sentence_file_path}\")\n",
    "\n",
    "with open(sentence_file_path, 'r', encoding='utf-8') as f:\n",
    "    retrieved_sentences = json.load(f)\n",
    "    \n",
    "print(f\"✅ Success: Sentences loaded from {sentence_file_path}\")\n",
    "\n",
    "# Access the list of sentences\n",
    "sentences_list = retrieved_sentences.get(\"sentences\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Assemble phrases & combined weighted representation\n",
    "# -------------------------------------------------------\n",
    "relation_texts = []\n",
    "relation_keys = []     # to preserve 1–1 mapping\n",
    "\n",
    "for rel in relations:\n",
    "    relation_phrase = rel[\"relation\"]\n",
    "    desc = rel.get(\"description\", \"\")\n",
    "\n",
    "    # store the original phrase for lookup\n",
    "    relation_keys.append(relation_phrase)\n",
    "\n",
    "    # combined semantic signal\n",
    "    combined = f\"{relation_phrase} ; {desc}\"\n",
    "    relation_texts.append(combined)\n",
    "\n",
    "unique_phrases = sorted(list(set(relation_keys)))\n",
    "print(f\"Unique relation labels: {len(unique_phrases)}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Build combined embeddings\n",
    "# -------------------------------------------------------\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"\\nEncoding relation phrases and descriptions...\")\n",
    "relation_embeddings = []\n",
    "\n",
    "for rel in relations:\n",
    "    rel_vec = embedder.encode(rel[\"relation\"])\n",
    "    desc_vec = embedder.encode(rel.get(\"description\", \"\"))\n",
    "\n",
    "    combined_vec = rel_vec + 0.3 * desc_vec\n",
    "    relation_embeddings.append(combined_vec)\n",
    "\n",
    "relation_embeddings = np.vstack(relation_embeddings)\n",
    "print(\"Embedding shape:\", relation_embeddings.shape)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Choose K\n",
    "# -------------------------------------------------------\n",
    "num_unique = len(unique_phrases)\n",
    "K_MAX = 10\n",
    "K = min(K_MAX, max(1, num_unique))\n",
    "\n",
    "print(f\"\\nK selected = {K}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Cluster\n",
    "# -------------------------------------------------------\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "cluster_ids = kmeans.fit_predict(relation_embeddings)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Build ClusterMap (relation → cluster_id)\n",
    "# -------------------------------------------------------\n",
    "ClusterMap = {}\n",
    "for rel, cid in zip(relations, cluster_ids):\n",
    "    ClusterMap[rel[\"relation\"]] = int(cid) + 1     # convert 0-index → 1-index\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Display clusters grouped\n",
    "# -------------------------------------------------------\n",
    "clusters_grouped = {}\n",
    "for phrase, c_id in ClusterMap.items():\n",
    "    clusters_grouped.setdefault(c_id, []).append(phrase)\n",
    "\n",
    "print(\"\\n--- Final Cluster Map ---\")\n",
    "for cid, items in sorted(clusters_grouped.items()):\n",
    "    print(f\"\\nCluster {cid}: (n={len(items)})\")\n",
    "    for phrase in sorted(set(items)):\n",
    "        print(\"  -\", phrase)\n",
    "\n",
    "pprint(ClusterMap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ClusterMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b29a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cac89b",
   "metadata": {},
   "source": [
    "#### Getting our $W_k$ matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Build a mapping from node_id (\"N1\") → integer index\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "node_to_idx: Dict[str, int] = {}\n",
    "\n",
    "# nodes is your loaded entities.json list\n",
    "for node in nodes:\n",
    "    node_id_str = node[\"id\"]          # e.g. \"N10\"\n",
    "    numeric_id = int(node_id_str[1:]) # remove \"N\"\n",
    "    node_to_idx[node_id_str] = numeric_id\n",
    "\n",
    "NUM_NODES = len(nodes)\n",
    "MATRIX_DIM = NUM_NODES + 1            # keeping index 0 unused\n",
    "\n",
    "print(f\"Total nodes: {NUM_NODES}\")\n",
    "print(f\"Matrix dimension: {MATRIX_DIM} x {MATRIX_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(node_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Number of clusters (relation categories)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "K_CLUSTER = max(ClusterMap.values())\n",
    "print(f\"Total relation clusters (K): {K_CLUSTER}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Prepare COO builders for each W_k\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "W_data = [[] for _ in range(K_CLUSTER)]\n",
    "W_row  = [[] for _ in range(K_CLUSTER)]\n",
    "W_col  = [[] for _ in range(K_CLUSTER)]\n",
    "\n",
    "print(\"\\nPopulating sparse matrix builders…\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. Iterate over all relations\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "for rel in relations:\n",
    "\n",
    "    relation_phrase = rel[\"relation\"]       # \"causes\", \"targets\", etc.\n",
    "    confidence      = float(rel[\"confidence\"])\n",
    "\n",
    "    # find cluster ID for this relation phrase\n",
    "    cluster_id = ClusterMap.get(relation_phrase)\n",
    "    if cluster_id is None:\n",
    "        print(f\"[WARN] relation phrase '{relation_phrase}' not in ClusterMap. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    matrix_idx = cluster_id - 1  # convert to 0-indexed\n",
    "\n",
    "    # subject and object node integer indices\n",
    "    row_idx = node_to_idx[rel[\"source_id\"]]\n",
    "    col_idx = node_to_idx[rel[\"target_id\"]]\n",
    "\n",
    "    # append to the cluster builder\n",
    "    W_data[matrix_idx].append(confidence)\n",
    "    W_row[matrix_idx].append(row_idx)\n",
    "    W_col[matrix_idx].append(col_idx)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. Build the final sparse COO matrices W₁…Wₖ\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "W_k_matrices: List[coo_matrix] = []\n",
    "\n",
    "print(\"\\nConstructing W_k sparse matrices…\")\n",
    "\n",
    "for k in range(K_CLUSTER):\n",
    "    W_k = coo_matrix(\n",
    "        (W_data[k], (W_row[k], W_col[k])),\n",
    "        shape=(MATRIX_DIM, MATRIX_DIM)\n",
    "    )\n",
    "    W_k_matrices.append(W_k)\n",
    "\n",
    "    print(f\"W_{k+1}: shape={W_k.shape}, nnz={W_k.nnz}, \"\n",
    "          f\"sum(weights)={W_k.data.sum():.3f}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(W_k_matrices)} sparse cluster matrices.\")\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the above W_k sparse matrices\n",
    "\n",
    "import os\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "def save_sparse_matrices(W_k_matrices, directory=\"extracted_output/sparse_W\"):\n",
    "    \"\"\"\n",
    "    Saves each W_k sparse matrix as W_k.npz inside the specified directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    for idx, W_k in enumerate(W_k_matrices, start=1):\n",
    "        filename = os.path.join(directory, f\"W_{idx}.npz\")\n",
    "        save_npz(filename, W_k)\n",
    "        print(f\"[SAVE] Saved {filename} (shape={W_k.shape}, nnz={W_k.nnz})\")\n",
    "\n",
    "    print(f\"\\nSaved {len(W_k_matrices)} sparse matrices to '{directory}'.\")\n",
    "\n",
    "save_sparse_matrices(W_k_matrices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the above saved matrices\n",
    "\n",
    "import os\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "def load_sparse_matrices(directory=\"extracted_output/sparse_W\"):\n",
    "    \"\"\"\n",
    "    Loads all sparse matrices named W_*.npz in the directory.\n",
    "    Returns a list of COO matrices in sorted order.\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "\n",
    "    # Get all files named W_*.npz and sort by index\n",
    "    files = [f for f in os.listdir(directory) if f.startswith(\"W_\") and f.endswith(\".npz\")]\n",
    "    files = sorted(files, key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "\n",
    "    for filename in files:\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        W_k = load_npz(full_path).tocoo()  # ensure COO format\n",
    "        matrices.append(W_k)\n",
    "        print(f\"[LOAD] Loaded {full_path} (shape={W_k.shape}, nnz={W_k.nnz})\")\n",
    "\n",
    "    print(f\"\\nLoaded {len(matrices)} sparse matrices from '{directory}'.\")\n",
    "    return matrices\n",
    "\n",
    "\n",
    "W_k_loaded = load_sparse_matrices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building our final A_w which is a set of our W_k$\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, dok_matrix, csr_matrix\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0. Build node_to_idx and MATRIX_DIM from `nodes`\n",
    "#    nodes: list of dicts like:\n",
    "#      {\n",
    "#        \"id\": \"N1\",\n",
    "#        \"name\": \"...\",\n",
    "#        \"type\": \"...\",\n",
    "#        \"time\": ...,\n",
    "#        ...\n",
    "#      }\n",
    "# ---------------------------------------------------\n",
    "node_to_idx: Dict[str, int] = {}\n",
    "\n",
    "# Use the numeric part of \"N<number>\" as the index, keep 0 unused.\n",
    "max_index = 0\n",
    "for node in nodes:\n",
    "    node_id = node[\"id\"]          # e.g., \"N10\"\n",
    "    num_part = int(node_id[1:])   # \"10\" -> 10\n",
    "    node_to_idx[node_id] = num_part\n",
    "    max_index = max(max_index, num_part)\n",
    "\n",
    "NUM_NODES = len(nodes)            # number of unique nodes\n",
    "MATRIX_DIM = max_index + 1        # rows/cols = max index + 1 (0 unused)\n",
    "\n",
    "print(f\"Total number of unique nodes (len(nodes)): {NUM_NODES}\")\n",
    "print(f\"Max node index (from IDs): {max_index}\")\n",
    "print(f\"Matrix dimension (MATRIX_DIM): {MATRIX_DIM} (index 0 unused)\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Number of clusters from ClusterMap\n",
    "#    ClusterMap: dict mapping relation text -> cluster_id (1..K)\n",
    "# ---------------------------------------------------\n",
    "K_CLUSTER = max(ClusterMap.values()) if ClusterMap else 0\n",
    "print(f\"Total number of relation clusters (K): {K_CLUSTER}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Initialize K builders for COO-like accumulators\n",
    "#    One set of (data, row, col) lists per cluster\n",
    "# ---------------------------------------------------\n",
    "W_data: List[List[float]] = [[] for _ in range(K_CLUSTER)]\n",
    "W_row:  List[List[int]]   = [[] for _ in range(K_CLUSTER)]\n",
    "W_col:  List[List[int]]   = [[] for _ in range(K_CLUSTER)]\n",
    "\n",
    "print(\"\\nPopulating sparse matrix builders in a single pass over relations...\")\n",
    "\n",
    "for rel in relations:\n",
    "    # rel: {\n",
    "    #   \"source_id\": \"N1\",\n",
    "    #   \"target_id\": \"N2\",\n",
    "    #   \"relation\": \"targets\",\n",
    "    #   \"description\": \"...\",\n",
    "    #   \"evidence\": \"...\",\n",
    "    #   \"confidence\": 0.95,\n",
    "    #   \"source_chunks\": [...]\n",
    "    # }\n",
    "\n",
    "    relation_phrase = rel[\"relation\"]\n",
    "    confidence      = float(rel.get(\"confidence\", 0.0))\n",
    "\n",
    "    # Look up which cluster this relation phrase belongs to\n",
    "    cluster_id = ClusterMap.get(relation_phrase)\n",
    "    if cluster_id is None:\n",
    "        # If some relation string wasn’t clustered (should be rare),\n",
    "        # we can skip it or log a warning.\n",
    "        print(f\"Warning: relation '{relation_phrase}' not found in ClusterMap. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # cluster_id is 1-based; list index is 0-based\n",
    "    matrix_index = cluster_id - 1\n",
    "\n",
    "    # Map node IDs -> integer indices (as per node_to_idx)\n",
    "    src_id = rel[\"source_id\"]\n",
    "    tgt_id = rel[\"target_id\"]\n",
    "\n",
    "    if src_id not in node_to_idx or tgt_id not in node_to_idx:\n",
    "        print(f\"Warning: node id(s) {src_id}, {tgt_id} not in node_to_idx. Skipping edge.\")\n",
    "        continue\n",
    "\n",
    "    row_idx = node_to_idx[src_id]\n",
    "    col_idx = node_to_idx[tgt_id]\n",
    "\n",
    "    # Append this edge to that cluster’s accumulator lists\n",
    "    W_data[matrix_index].append(confidence)\n",
    "    W_row[matrix_index].append(row_idx)\n",
    "    W_col[matrix_index].append(col_idx)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Helper: max-aggregate multiple edges into final CSR\n",
    "# ---------------------------------------------------\n",
    "def finalize_relation_matrix(\n",
    "    data_list: List[float],\n",
    "    row_list: List[int],\n",
    "    col_list: List[int],\n",
    "    n_dim: int\n",
    ") -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Build an (n_dim x n_dim) sparse matrix where, for each (row, col),\n",
    "    we store the MAX of all confidence scores seen for that edge\n",
    "    in this cluster.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        # Return an empty matrix\n",
    "        return csr_matrix((n_dim, n_dim), dtype=np.float32)\n",
    "\n",
    "    W_dok = dok_matrix((n_dim, n_dim), dtype=np.float32)\n",
    "\n",
    "    for data, row, col in zip(data_list, row_list, col_list):\n",
    "        value = float(data)\n",
    "        current_max = W_dok[row, col]  # 0.0 if not yet set\n",
    "        if value > current_max:\n",
    "            W_dok[row, col] = value\n",
    "\n",
    "    return W_dok.tocsr()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Build final aggregated CSR matrices per cluster\n",
    "# ---------------------------------------------------\n",
    "Final_Aggregated_W: List[csr_matrix] = []\n",
    "\n",
    "print(\"\\n--- Starting Max Aggregation for All Clusters ---\")\n",
    "\n",
    "for k_index in range(K_CLUSTER):\n",
    "    raw_data_list = W_data[k_index]\n",
    "    row_list_k    = W_row[k_index]\n",
    "    col_list_k    = W_col[k_index]\n",
    "\n",
    "    if not raw_data_list:\n",
    "        # No edges in this cluster\n",
    "        empty_csr = csr_matrix((MATRIX_DIM, MATRIX_DIM), dtype=np.float32)\n",
    "        Final_Aggregated_W.append(empty_csr)\n",
    "        print(f\"✅ Final W_{k_index + 1}: EMPTY matrix, Shape={empty_csr.shape}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        data_array_k = np.asarray(raw_data_list, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR in W_{k_index + 1} data conversion: {e}\")\n",
    "        # Push empty matrix or skip; here we push empty to keep indexing consistent\n",
    "        empty_csr = csr_matrix((MATRIX_DIM, MATRIX_DIM), dtype=np.float32)\n",
    "        Final_Aggregated_W.append(empty_csr)\n",
    "        continue\n",
    "\n",
    "    Final_W_k_CSR = finalize_relation_matrix(\n",
    "        data_array_k.tolist(),  # list of floats\n",
    "        row_list_k,             # list of row indices\n",
    "        col_list_k,             # list of col indices\n",
    "        MATRIX_DIM\n",
    "    )\n",
    "\n",
    "    Final_Aggregated_W.append(Final_W_k_CSR)\n",
    "\n",
    "    print(\n",
    "        f\"✅ Final W_{k_index + 1}: \"\n",
    "        f\"Shape={Final_W_k_CSR.shape}, \"\n",
    "        f\"Unique Edges={Final_W_k_CSR.nnz}, \"\n",
    "        f\"Total Weight={Final_W_k_CSR.data.sum():.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(Final_Aggregated_W)} final, maximized CSR matrices.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Pretty-print non-zero entries for sanity check\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n--- Final Maximized Relational Matrices (W_k) ---\")\n",
    "print(f\"Total Matrices in List: {len(Final_Aggregated_W)}\\n\")\n",
    "\n",
    "for k_index, final_Wk in enumerate(Final_Aggregated_W):\n",
    "    cluster_id = k_index + 1\n",
    "    print(f\"\\n**Matrix W_{cluster_id} (Cluster {cluster_id})**\")\n",
    "    print(\n",
    "        f\"Shape: {final_Wk.shape}, \"\n",
    "        f\"Unique Edges (nnz): {final_Wk.nnz}, \"\n",
    "        f\"Total Weight: {final_Wk.data.sum():.4f}\"\n",
    "    )\n",
    "\n",
    "    if final_Wk.nnz == 0:\n",
    "        print(\"  [Matrix is empty (no relationships in this cluster)]\")\n",
    "        continue\n",
    "\n",
    "    coo_wk: coo_matrix = final_Wk.tocoo()\n",
    "    print(\"  Non-Zero Edges (Subject Index -> Object Index | Max Confidence Score):\")\n",
    "    for r, c, d in zip(coo_wk.row, coo_wk.col, coo_wk.data):\n",
    "        print(f\"  ({r} -> {c}) | Score: {d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_w = Final_Aggregated_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8365f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
