{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06abea73",
   "metadata": {},
   "source": [
    "Lets start the CausGT-HS core model implementation (A lot behind is left Lets see how it goes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4dd9f",
   "metadata": {},
   "source": [
    "# 3.4 CausGT-HS: A self-supervised, probabilistic energy-based causal graph token transformer\n",
    "\n",
    "The CausGT-HS (Causal Graph-Token Hierarchical Self-Supervised) model is the final, deep-learning encoder of our system. It is trained as an Energy-Based Model (EBM) to learn a single, globally coherent, multi-relational causal graph, $G$. Its design is motivated by the need for uncertainty quantification and robust mechanism learning. This is our main \"student\" GNN that we are gonna distil our $C_{prior}$ into along with the others.\n",
    "\n",
    "- An Overview: \n",
    "The CausGT-HS defines a probability distribution $P(G)$ over all possible causal graphs, where graphs consistent with the evidence have low energy.$$ \\\\ P(G) \\propto e^{-E\\_{\\psi}(G)}\n",
    "\n",
    "- Method: \n",
    "The model's global energy function, $E_{\\psi}(G)$, is trained to minimize the energy of \"correct\" graphs ($G_{\\text{positive}}$, derived from $C_{\\text{prior}}$) while maximizing the energy of \"incorrect\" graphs ($G_{\\text{negative}}$). This training is performed via a contrastive divergence loss (InfoNCE).\n",
    "\n",
    "- Inference (Uncertainty): Unlike deterministic models, CausGT-HS uses MCMC (Monte Carlo Markov Chain) sampling (Langevin Dynamics) to sample an ensemble of plausible graphs, $G_k$, from the low-energy regions of the distribution. This provides principled confidence intervals and structural uncertainty estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c6512",
   "metadata": {},
   "source": [
    "## 3.4.1 Inputs to the CausGT-HS model\n",
    "\n",
    "- Correlational Graphs: \n",
    "Our initial adjancency matrices\n",
    "$$\n",
    "A_w = \\{ W_1, W_2, \\ldots, W_k \\}\n",
    "$$\n",
    "\n",
    "- Rich Causal Prior ($P_{rich}$)\n",
    "\n",
    "It is the output produced by our CoCaD pipeline. It is a sparse list of tuples: \n",
    "\n",
    "(i, j, type, evidence,score, uncertainity_variance).\n",
    "\n",
    "Above:\n",
    "    \n",
    "    - type: A string indicating the causal relation, one of: 'DIRECT', 'MEDIATED' or 'CONFOUNDED'\n",
    "\n",
    "    - evidence: A list of node IDs (e.g. the mediator [k] or the confounders [$k_c$])\n",
    "\n",
    "    - score: The final $P_{direct}$ score (e.g., 0.95 for direct, 0 for mediated)\n",
    "\n",
    "    - uncertainity variance: The statistical variance of the score representing the teachers variance\n",
    "\n",
    "- Raw Text Data (S):\n",
    "\n",
    "This is the original doc segmented into sentence snippets. This is the ground truth textual evidence for our tokenphormer to read and reason from.\n",
    "\n",
    "- GAE Embeddings (Z):\n",
    "\n",
    "This is our $N \\times d_z$ structural node embeddings from our graph autoencoder. This is our initial structural embeddings $H^{(0)}$ which we will refine later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f471d",
   "metadata": {},
   "source": [
    "## 3.4.2 CausGT-HS model architectrure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d3324",
   "metadata": {},
   "source": [
    "The model is an **encoder-only**, hierarchical, *L*-layer stack (lets keep L=3 as of now).  \n",
    "This encoder, denoted as **$f_θ$**, takes all raw inputs and produces a final **causally-aware graph**:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "$$\n",
    "G = (H^{(L)},\\ A)\n",
    "$$\n",
    "\n",
    "</p>\n",
    "\n",
    "The encoder is composed of three major components:\n",
    "\n",
    "### • Embedding Layers\n",
    "Responsible for converting all raw signals (text, structural embeddings, priors, adjacency matrices) into a unified token or node representation.\n",
    "\n",
    "### • Reasoning Layers\n",
    "These layers iteratively refine the representations by performing multi-step reasoning over:\n",
    "- graph structure  \n",
    "- causal priors  \n",
    "- textual evidence  \n",
    "- learned embeddings  \n",
    "\n",
    "### • Proposer Network\n",
    "The final module that generates the updated causal graph edges and relations based on the refined representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cfe3f",
   "metadata": {},
   "source": [
    "### 3.4.2.1 Multi-faceted Representations:\n",
    "\n",
    "One embedding per node ain't interesting, nodes can have multiple meanings (polysemy), we must model this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf4de3",
   "metadata": {},
   "source": [
    "Dynamic facet selection at runtime:\n",
    "\n",
    "Each query (eg node i facet k) attends to the M facets of neighbor j.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc592b0",
   "metadata": {},
   "source": [
    "##### Multi-Embedding Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "DATA_DIR = \"./data\"  \n",
    "NODES_JSONL = os.path.join(DATA_DIR, \"entities.jsonl\")   \n",
    "TMAP_JSON = os.path.join(DATA_DIR, \"tmap.json\")        \n",
    "SENT_EMBS_NPY = os.path.join(DATA_DIR, \"sent_embs.npy\") \n",
    "AW_NPZ = os.path.join(DATA_DIR, \"AW.npz\")               \n",
    "\n",
    "M_FACETS = 5\n",
    "D_MODEL = 256\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 512\n",
    "LR = 3e-4\n",
    "LAMBDA_MAX = 1e-2\n",
    "WARMUP_EPOCHS = 10\n",
    "BATCH_NEIGH_K = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def load_jsonl(path: str) -> List[dict]:\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def load_tmap(path: str) -> Dict[str, List[int]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_sent_embeddings(path: str) -> np.ndarray:\n",
    "    return np.load(path)\n",
    "\n",
    "def load_adj_npz(path: str) -> sparse.spmatrix:\n",
    "    return sparse.load_npz(path)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset for nodes (samples node indices and neighbor lists)\n",
    "# -------------------------\n",
    "class NodeDataset(Dataset):\n",
    "    def __init__(self, node_ids: List[int], AW: Optional[sparse.spmatrix]=None, batch_neigh_k: int = 32):\n",
    "        self.node_ids = node_ids\n",
    "        self.AW = AW.tocsr() if AW is not None else None\n",
    "        self.batch_neigh_k = batch_neigh_k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        node_idx = self.node_ids[idx]\n",
    "        neigh = None\n",
    "        if self.AW is not None:\n",
    "            row = self.AW.getrow(node_idx).tocoo()\n",
    "            neigh = row.col\n",
    "            if len(neigh) > self.batch_neigh_k:\n",
    "                neigh = np.random.choice(neigh, size=self.batch_neigh_k, replace=False)\n",
    "        return int(node_idx), neigh\n",
    "\n",
    "def collate_batch(batch):\n",
    "    nodes = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    neighs = [b[1] for b in batch]\n",
    "    return nodes, neighs\n",
    "\n",
    "# -------------------------\n",
    "# MultiFacetModule\n",
    "# -------------------------\n",
    "class MultiFacetModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        M: int,\n",
    "        d_model: int,\n",
    "        device: torch.device,\n",
    "        init_facet_embeddings: Optional[np.ndarray] = None,\n",
    "        use_context_gate: bool = True,\n",
    "        ctx_dim: int = 384,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.N = num_nodes\n",
    "        self.M = M\n",
    "        self.d = d_model\n",
    "        self.device = device\n",
    "\n",
    "        # facets stored as embedding table (N*M, d)\n",
    "        self.facet_table = nn.Embedding(self.N * self.M, self.d)\n",
    "        nn.init.normal_(self.facet_table.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # static gating logits (learnable)\n",
    "        self.gate_logits = nn.Parameter(torch.zeros(self.N, self.M))\n",
    "\n",
    "        self.use_context_gate = use_context_gate\n",
    "        if use_context_gate:\n",
    "            self.ctx_net = nn.Sequential(\n",
    "                nn.Linear(ctx_dim, ctx_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(ctx_dim, self.M),\n",
    "            )\n",
    "\n",
    "        # small projections for facet attention (q/k/v)\n",
    "        self.proj_q = nn.Linear(self.d, self.d, bias=False)\n",
    "        self.proj_k = nn.Linear(self.d, self.d, bias=False)\n",
    "        self.proj_v = nn.Linear(self.d, self.d, bias=False)\n",
    "\n",
    "        if init_facet_embeddings is not None:\n",
    "            assert init_facet_embeddings.shape == (self.N, self.M, self.d)\n",
    "            flat = torch.tensor(init_facet_embeddings.reshape(-1, self.d), dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                self.facet_table.weight.data.copy_(flat)\n",
    "\n",
    "    def node_facet_indices(self, node_idxs: Tensor) -> Tensor:\n",
    "        # returns flattened indices into facet_table for node_idxs (B, M) -> (B*M,)\n",
    "        base = node_idxs.unsqueeze(1) * self.M\n",
    "        offsets = torch.arange(self.M, device=self.device).unsqueeze(0)\n",
    "        return (base + offsets).reshape(-1)\n",
    "\n",
    "    def fetch_facets_for_nodes(self, node_idxs: Tensor) -> Tensor:\n",
    "        idxs = self.node_facet_indices(node_idxs)\n",
    "        facets = self.facet_table(idxs).view(node_idxs.shape[0], self.M, self.d)\n",
    "        return facets\n",
    "\n",
    "    def forward(self, node_idxs: Tensor, context_vecs: Optional[Tensor] = None, neighbor_indices: Optional[List[np.ndarray]] = None, return_all=False):\n",
    "        B = node_idxs.shape[0]\n",
    "        facets = self.fetch_facets_for_nodes(node_idxs)            # (B, M, d)\n",
    "        static_logits = self.gate_logits[node_idxs]               # (B, M)\n",
    "        static_gate = torch.sigmoid(static_logits)                # (B, M)\n",
    "        if self.use_context_gate and (context_vecs is not None):\n",
    "            ctx_logits = self.ctx_net(context_vecs)               # (B, M)\n",
    "            gates = torch.sigmoid(static_logits + ctx_logits)     # (B, M)\n",
    "        else:\n",
    "            gates = static_gate\n",
    "\n",
    "        gates_unsq = gates.unsqueeze(-1)\n",
    "        H_active = (facets * gates_unsq).sum(dim=1)               # (B, d)\n",
    "\n",
    "        out = {\"H_active\": H_active, \"facets\": facets, \"gates\": gates}\n",
    "        # neighbor blending (vectorized across neighbors where possible)\n",
    "        if neighbor_indices is not None:\n",
    "            nb_blended = []\n",
    "            # process in-loop per-batch element (neighbors vary in length)\n",
    "            for i in range(B):\n",
    "                neigh = neighbor_indices[i]\n",
    "                if neigh is None or len(neigh) == 0:\n",
    "                    nb_blended.append(torch.zeros(self.d, device=self.device))\n",
    "                    continue\n",
    "                neigh_tensor = torch.tensor(np.asarray(neigh, dtype=np.int64), dtype=torch.long, device=self.device)\n",
    "                neigh_facets = self.fetch_facets_for_nodes(neigh_tensor)  # (K, M, d)\n",
    "                # project\n",
    "                q = self.proj_q(H_active[i].unsqueeze(0))  # (1, d)\n",
    "                k = self.proj_k(neigh_facets.view(-1, self.d))  # (K*M, d)\n",
    "                v = self.proj_v(neigh_facets.view(-1, self.d))  # (K*M, d)\n",
    "                att_logits = (q @ k.T) / math.sqrt(self.d)  # (1, K*M)\n",
    "                att = F.softmax(att_logits, dim=-1)\n",
    "                blended = att @ v\n",
    "                nb_blended.append(blended.squeeze(0))\n",
    "            out[\"neighbor_blended\"] = torch.stack(nb_blended, dim=0)  # (B, d)\n",
    "        if return_all:\n",
    "            out[\"static_gate\"] = static_gate\n",
    "        return out\n",
    "\n",
    "    def l1_gate_cost(self) -> Tensor:\n",
    "        gate_vals = torch.sigmoid(self.gate_logits)\n",
    "        return gate_vals.abs().sum() / (self.N * self.M)\n",
    "\n",
    "    def update_facets_via_clustering(self, nodes: List[dict], Tmap: Dict[str, List[int]], sent_embeddings: np.ndarray, per_node_min_context: int = 2):\n",
    "        init = np.random.normal(scale=0.02, size=(self.N, self.M, self.d)).astype(np.float32)\n",
    "        num_sent_dim = sent_embeddings.shape[1]\n",
    "        for node in nodes:\n",
    "            nid = node.get(\"id\")\n",
    "            if isinstance(nid, str) and nid.startswith(\"N\"):\n",
    "                try:\n",
    "                    nidx = int(nid[1:])\n",
    "                except:\n",
    "                    nidx = int(nid)\n",
    "            else:\n",
    "                nidx = int(nid)\n",
    "            s_indices = Tmap.get(str(nid), []) or Tmap.get(str(nidx), [])\n",
    "            if not s_indices or len(s_indices) < per_node_min_context:\n",
    "                continue\n",
    "            vecs = sent_embeddings[s_indices]  # (S, embed_dim)\n",
    "            k = min(self.M, max(1, vecs.shape[0]))\n",
    "            if k == 1:\n",
    "                centroids = np.vstack([vecs.mean(axis=0) for _ in range(self.M)])\n",
    "            else:\n",
    "                km = KMeans(n_clusters=k, random_state=0, n_init=4).fit(vecs)\n",
    "                centroids = km.cluster_centers_\n",
    "                if k < self.M:\n",
    "                    extras = np.tile(centroids[:1], (self.M - k, 1))\n",
    "                    centroids = np.vstack([centroids, extras])\n",
    "            if centroids.shape[1] != self.d:\n",
    "                rng = np.random.RandomState(0)\n",
    "                P = rng.normal(size=(centroids.shape[1], self.d)).astype(np.float32) * 0.02\n",
    "                centroids = centroids @ P\n",
    "            init[nidx] = centroids[:self.M]\n",
    "        with torch.no_grad():\n",
    "            flat = torch.tensor(init.reshape(-1, self.d), dtype=torch.float32, device=self.device)\n",
    "            self.facet_table.weight.data.copy_(flat)\n",
    "\n",
    "# -------------------------\n",
    "# Proxy loss: neighbor reconstruction \n",
    "# -------------------------\n",
    "def compute_proxy_causal_loss(module: MultiFacetModule, batch_node_idxs: Tensor, neighbor_lists: List[np.ndarray], AW: sparse.spmatrix, device: torch.device):\n",
    "    out = module(batch_node_idxs, return_all=False)\n",
    "    H_active = out[\"H_active\"]  # (B, d)\n",
    "    neighbor_blended = out.get(\"neighbor_blended\", None)  # (B, d) if present\n",
    "    losses = []\n",
    "    B = H_active.shape[0]\n",
    "    # If neighbor_blended available, compute dot(H_i, blended_Ni) as logits (I'll check this later on)\n",
    "    if neighbor_blended is not None:\n",
    "        logits = (H_active * neighbor_blended).sum(dim=-1)  # (B,)\n",
    "        targets = []\n",
    "        for i in range(B):\n",
    "            nid = int(batch_node_idxs[i].item())\n",
    "            row = AW.getrow(nid).tocoo()\n",
    "            # presence if any neighbor had weight>0 -> we treat positive target as 1 if row non-empty else 0\n",
    "            targets.append(1.0 if row.nnz > 0 else 0.0)\n",
    "        t = torch.tensor(targets, dtype=torch.float32, device=device)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, t)\n",
    "        return loss\n",
    "    # fallback: per-neighbor BCE as in original but batched\n",
    "    for i in range(B):\n",
    "        nid = int(batch_node_idxs[i].item())\n",
    "        neigh = neighbor_lists[i]\n",
    "        if neigh is None or len(neigh) == 0:\n",
    "            continue\n",
    "        neigh = np.asarray(neigh, dtype=np.int64)\n",
    "        neigh_tensor = torch.tensor(neigh, dtype=torch.long, device=device)\n",
    "        neigh_out = module(neigh_tensor, context_vecs=None, return_all=False)\n",
    "        Hn = neigh_out[\"H_active\"]\n",
    "        scores = (H_active[i].unsqueeze(0) * Hn).sum(dim=-1)\n",
    "        row = AW.getrow(nid).tocoo()\n",
    "        col_to_val = dict(zip(row.col.tolist(), row.data.tolist()))\n",
    "        targets = torch.tensor([col_to_val.get(int(n), 0.0) for n in neigh], dtype=torch.float32, device=device)\n",
    "        loss = F.binary_cross_entropy_with_logits(scores, targets)\n",
    "        losses.append(loss)\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "# -------------------------\n",
    "# Annealing schedule\n",
    "# -------------------------\n",
    "def lambda_fac_schedule(epoch: int, max_lambda: float, warmup_epochs: int = 10):\n",
    "    if epoch <= 0:\n",
    "        return 0.0\n",
    "    if epoch < warmup_epochs:\n",
    "        return max_lambda * (epoch / warmup_epochs)\n",
    "    return max_lambda\n",
    "\n",
    "# -------------------------\n",
    "# Training loop \n",
    "# -------------------------\n",
    "def train_loop(\n",
    "    module: MultiFacetModule,\n",
    "    nodes: List[dict],\n",
    "    tmap: Dict[str, List[int]],\n",
    "    sent_embs: np.ndarray,\n",
    "    AW: sparse.spmatrix,\n",
    "    device: torch.device,\n",
    "    epochs: int = EPOCHS,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    lr: float = LR,\n",
    "    lambda_max: float = LAMBDA_MAX,\n",
    "):\n",
    "    node_indices = list(range(module.N))\n",
    "    dataset = NodeDataset(node_indices, AW=AW, batch_neigh_k=BATCH_NEIGH_K)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_batch)\n",
    "    opt = torch.optim.AdamW(module.parameters(), lr=lr)\n",
    "    module.train()\n",
    "    for ep in range(epochs):\n",
    "        start = time.time()\n",
    "        lam = lambda_fac_schedule(ep, lambda_max, warmup_epochs=WARMUP_EPOCHS)\n",
    "        total_loss = 0.0\n",
    "        for batch_nodes, batch_neigh in loader:\n",
    "            batch_nodes = batch_nodes.to(device)\n",
    "            # build context vectors (average sentence embeddings for node)\n",
    "            ctx_list = []\n",
    "            sent_dim = sent_embs.shape[1]\n",
    "            for nid in batch_nodes.cpu().numpy().tolist():\n",
    "                sidx = tmap.get(str(nid), []) or tmap.get(str(nid), [])\n",
    "                if sidx and len(sidx) > 0:\n",
    "                    vec = sent_embs[sidx].mean(axis=0)\n",
    "                else:\n",
    "                    vec = np.zeros(sent_dim, dtype=np.float32)\n",
    "                ctx_list.append(vec)\n",
    "            context_vecs = torch.tensor(np.stack(ctx_list, axis=0), dtype=torch.float32, device=device)\n",
    "            out = module(batch_nodes, context_vecs=context_vecs, neighbor_indices=batch_neigh)\n",
    "            causal_loss = compute_proxy_causal_loss(module, batch_nodes, batch_neigh, AW, device)\n",
    "            l1 = module.l1_gate_cost()\n",
    "            loss = causal_loss + lam * l1\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += float(loss.detach().cpu().numpy())\n",
    "        avg_loss = total_loss / (len(loader) + 1e-12)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"[Epoch {ep+1}/{epochs}] avg_loss={avg_loss:.6f} lam={lam:.6e} time={elapsed:.1f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Main run - loads from DATA_DIR\n",
    "# -------------------------\n",
    "def run_all():\n",
    "    assert os.path.isdir(DATA_DIR), f\"Data dir '{DATA_DIR}' not found.\"\n",
    "    assert os.path.exists(NODES_JSONL), f\"{NODES_JSONL} missing.\"\n",
    "    assert os.path.exists(TMAP_JSON), f\"{TMAP_JSON} missing.\"\n",
    "    assert os.path.exists(SENT_EMBS_NPY), f\"{SENT_EMBS_NPY} missing.\"\n",
    "    assert os.path.exists(AW_NPZ), f\"{AW_NPZ} missing.\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nodes = load_jsonl(NODES_JSONL)\n",
    "    tmap = load_tmap(TMAP_JSON)\n",
    "    sent_embs = load_sent_embeddings(SENT_EMBS_NPY)\n",
    "    AW = load_adj_npz(AW_NPZ)\n",
    "\n",
    "    N = len(nodes)\n",
    "    module = MultiFacetModule(num_nodes=N, M=M_FACETS, d_model=D_MODEL, device=device, use_context_gate=True, ctx_dim=sent_embs.shape[1])\n",
    "    module.to(device)\n",
    "    module.update_facets_via_clustering(nodes=nodes, Tmap=tmap, sent_embeddings=sent_embs, per_node_min_context=1)\n",
    "\n",
    "    train_loop(\n",
    "        module=module,\n",
    "        nodes=nodes,\n",
    "        tmap=tmap,\n",
    "        sent_embs=sent_embs,\n",
    "        AW=AW,\n",
    "        device=device,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        lambda_max=LAMBDA_MAX\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb9a49",
   "metadata": {},
   "source": [
    "##### Multi-Relational Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a543205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, json\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "NODES_JSONL = os.path.join(DATA_DIR, \"entities.jsonl\")\n",
    "TMAP_JSON = os.path.join(DATA_DIR, \"tmap.json\")\n",
    "SENT_EMBS_NPY = os.path.join(DATA_DIR, \"sent_embs.npy\")\n",
    "AW_NPZ = os.path.join(DATA_DIR, \"AW.npz\")   # base co-occurrence adjacency (sparse)\n",
    "# Multi-relation config\n",
    "RELATION_NAMES = [\"causes\", \"inhibits\", \"related_to\", \"cooccurs\"]  \n",
    "DRANK = 128\n",
    "K_ANN = 50           # Top-K for Sr via ANN\n",
    "K_LEARN = 32         # per-node learnable extra neighbors (compact Slearned)\n",
    "D_MODEL = 256        # must match your MultiFacetModule d_model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# regularization & training params\n",
    "UR_VR_DROPOUT = 0.1\n",
    "UR_VR_WEIGHT_DECAY = 1e-5\n",
    "SLEARN_L1_LAMBDA = 1e-3\n",
    "UR_VR_L2_LAMBDA = 1e-4\n",
    "EDGE_NEG_SAMPLE_RATIO = 1    # #neg per positive in minibatch\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "LR = 3e-4\n",
    "\n",
    "# -------------------------\n",
    "# Utilities (loaders)\n",
    "# -------------------------\n",
    "def load_jsonl(path: str):\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            if l.strip():\n",
    "                out.append(json.loads(l))\n",
    "    return out\n",
    "\n",
    "def load_tmap(path: str):\n",
    "    import json\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_sent_embs(path: str):\n",
    "    return np.load(path)\n",
    "\n",
    "def load_adj_npz(path: str):\n",
    "    return sparse.load_npz(path)\n",
    "\n",
    "# -------------------------\n",
    "# Build Sr heuristic mask (sparse) from AW (cooccurrence) + ANN\n",
    "# returns Sr_as_set: set of (i,j) tuples considered plausible\n",
    "# -------------------------\n",
    "def build_Sr_from_AW_and_ann(AW_sparse: sparse.spmatrix, node_seed_embeddings: np.ndarray, k_ann: int = K_ANN):\n",
    "    N = AW_sparse.shape[0]\n",
    "    Sr_pairs = set()\n",
    "    # 1) text cooccurrence from AW (nonzero entries)\n",
    "    coo = AW_sparse.tocoo()\n",
    "    for i, j in zip(coo.row.tolist(), coo.col.tolist()):\n",
    "        Sr_pairs.add((int(i), int(j)))\n",
    "    # 2) ANN top-k on seed embeddings\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k_ann, node_seed_embeddings.shape[0]-1), algorithm=\"auto\", metric=\"cosine\").fit(node_seed_embeddings)\n",
    "    distances, indices = nbrs.kneighbors(node_seed_embeddings, return_distance=True)\n",
    "    for i in range(node_seed_embeddings.shape[0]):\n",
    "        for j in indices[i]:\n",
    "            if i == j: \n",
    "                continue\n",
    "            Sr_pairs.add((int(i), int(j)))\n",
    "    return Sr_pairs\n",
    "\n",
    "# -------------------------\n",
    "# RelationFactorizer class\n",
    "# -------------------------\n",
    "class RelationFactorizer(nn.Module):\n",
    "    def __init__(self, num_nodes: int, relation_names: List[str], drank: int = DRANK, device: torch.device = DEVICE, k_learn: int = K_LEARN, Sr_pairs: Optional[set] = None):\n",
    "        super().__init__()\n",
    "        self.N = num_nodes\n",
    "        self.R = len(relation_names)\n",
    "        self.rel_names = relation_names\n",
    "        self.drank = drank\n",
    "        self.device = device\n",
    "        self.k_learn = k_learn\n",
    "\n",
    "        # Ur and Vr per relation: implemented as Embedding (N x drank)\n",
    "        self.U_tables = nn.ModuleDict({r: nn.Embedding(self.N, self.drank) for r in relation_names})\n",
    "        self.V_tables = nn.ModuleDict({r: nn.Embedding(self.N, self.drank) for r in relation_names})\n",
    "        for r in relation_names:\n",
    "            nn.init.normal_(self.U_tables[r].weight, std=0.02)\n",
    "            nn.init.normal_(self.V_tables[r].weight, std=0.02)\n",
    "\n",
    "        # dropout on Ur/Vr during training\n",
    "        self.urvr_dropout = nn.Dropout(UR_VR_DROPOUT)\n",
    "\n",
    "        # initialize learned_candidate_idx as -1 for padding if insufficient candidates.\n",
    "        # learned_idx: (N, K_learn) ints on CPU (not Parameters) to be used to index neighbor nodes\n",
    "        self.learned_idx = torch.full((self.N, self.k_learn), -1, dtype=torch.long)  # filled later with ints (on cpu)\n",
    "        # logits for those candidate edges (trainable)\n",
    "        self.learned_logits = nn.Parameter(torch.zeros(self.N, self.k_learn))  # learnable logits\n",
    "        # mask for which learned slots contain valid neighbors (1 if valid index >=0)\n",
    "        self.register_buffer(\"_learned_valid_mask\", torch.zeros(self.N, self.k_learn, dtype=torch.bool))\n",
    "\n",
    "        # Sr (heuristic) stored as python set of tuples for quick membership checks\n",
    "        self.Sr_pairs = Sr_pairs if Sr_pairs is not None else set()\n",
    "\n",
    "    def to_device(self):\n",
    "        self.to(self.device)\n",
    "\n",
    "    def set_learned_candidates(self, learned_idx_np: np.ndarray):\n",
    "        \"\"\"Provide per-node learned candidate indices as numpy int array shape (N, K_learn),\n",
    "           invalid entries must be -1. This stores indices on CPU buffer and sets valid mask.\"\"\"\n",
    "        assert learned_idx_np.shape == (self.N, self.k_learn)\n",
    "        self.learned_idx = torch.from_numpy(learned_idx_np.astype(np.int64))\n",
    "        valid = (self.learned_idx >= 0)\n",
    "        self._learned_valid_mask = valid\n",
    "        # move learned_idx to cpu buffer and keep on CPU to index; during forward we'll move to device\n",
    "        # keep as attribute but not Parameter\n",
    "        # ensure learned_idx on cpu\n",
    "        if self.learned_idx.device != torch.device(\"cpu\"):\n",
    "            self.learned_idx = self.learned_idx.cpu()\n",
    "\n",
    "    def get_edge_score(self, rel_name: str, src_idx: torch.LongTensor, dst_idx: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized: src_idx, dst_idx are 1D LongTensors same len L -> returns logits (L,)\"\"\"\n",
    "        # fetch rows\n",
    "        U = self.U_tables[rel_name](src_idx.to(self.device))\n",
    "        V = self.V_tables[rel_name](dst_idx.to(self.device))\n",
    "        U = self.urvr_dropout(U)\n",
    "        V = self.urvr_dropout(V)\n",
    "        logits = (U * V).sum(dim=-1) / math.sqrt(self.drank)\n",
    "        return logits\n",
    "\n",
    "    def compute_prob_for_batch_pairs(self, rel_name: str, pair_list: List[Tuple[int,int]]) -> torch.Tensor:\n",
    "        \"\"\"pair_list: list of (i,j) on CPU ints -> returns probs tensor (len,)\"\"\"\n",
    "        if len(pair_list) == 0:\n",
    "            return torch.tensor([], device=self.device)\n",
    "        src = torch.tensor([p[0] for p in pair_list], dtype=torch.long, device=self.device)\n",
    "        dst = torch.tensor([p[1] for p in pair_list], dtype=torch.long, device=self.device)\n",
    "        logits = self.get_edge_score(rel_name, src, dst)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        # apply Shybrid mask (if pair not in Sr nor in learned -> zero)\n",
    "        mask = torch.tensor([1 if ((int(p[0]), int(p[1])) in self.Sr_pairs) else 0 for p in pair_list], dtype=torch.bool, device=self.device)\n",
    "        # to check learned candidates\n",
    "        if hasattr(self, \"learned_idx\"):\n",
    "            learned_idx_dev = self.learned_idx.to(self.device)\n",
    "            # for each src, check equality across k\n",
    "            src_expand = src.unsqueeze(1).expand(-1, self.k_learn)  # (L, K)\n",
    "            # gather the learned neighbors for those src\n",
    "            learned_neighbors_for_src = learned_idx_dev[src]  # (L, K)\n",
    "            # compare dst to each col\n",
    "            matches = (learned_neighbors_for_src == dst.unsqueeze(1))\n",
    "            learned_mask = matches.any(dim=1)  # (L,)\n",
    "            mask = mask | learned_mask\n",
    "        probs = probs * mask.float()\n",
    "        return probs\n",
    "\n",
    "    def shybrid_mask_for_pairs(self, pair_list: List[Tuple[int,int]]) -> torch.Tensor:\n",
    "        \"\"\"Return mask (0/1) tensor for given pairs according to Sr ∪ Slearned\"\"\"\n",
    "        L = len(pair_list)\n",
    "        mask = torch.zeros(L, dtype=torch.bool, device=self.device)\n",
    "        for idx, (i,j) in enumerate(pair_list):\n",
    "            if (i,j) in self.Sr_pairs:\n",
    "                mask[idx] = True\n",
    "        if hasattr(self, \"learned_idx\"):\n",
    "            learned_idx_dev = self.learned_idx.to(self.device)\n",
    "            src = torch.tensor([p[0] for p in pair_list], dtype=torch.long, device=self.device)\n",
    "            dst = torch.tensor([p[1] for p in pair_list], dtype=torch.long, device=self.device)\n",
    "            learned_neighbors_for_src = learned_idx_dev[src]  # (L, K)\n",
    "            matches = (learned_neighbors_for_src == dst.unsqueeze(1))\n",
    "            learned_mask = matches.any(dim=1)\n",
    "            mask = mask | learned_mask\n",
    "        return mask.float()\n",
    "\n",
    "    def forward_edge_logits(self, rel_name: str, pair_src: torch.LongTensor, pair_dst: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"Compute raw logits for given batches (tensor inputs). pair_src/dst on device.\"\"\"\n",
    "        U = self.U_tables[rel_name](pair_src)\n",
    "        V = self.V_tables[rel_name](pair_dst)\n",
    "        logits = (U * V).sum(dim=-1) / math.sqrt(self.drank)\n",
    "\n",
    "        mask = torch.tensor([1 if ((int(s.cpu().item()), int(d.cpu().item())) in self.Sr_pairs) else 0 for s,d in zip(pair_src, pair_dst)], dtype=torch.bool, device=self.device)\n",
    "        if hasattr(self, \"learned_idx\"):\n",
    "            learned_idx_dev = self.learned_idx.to(self.device)\n",
    "            learned_neighbors_for_src = learned_idx_dev[pair_src]\n",
    "            dst_expand = pair_dst.unsqueeze(1)\n",
    "            learned_matches = (learned_neighbors_for_src == dst_expand).any(dim=1)\n",
    "            mask = mask | learned_matches\n",
    "        logits = logits * mask.float() + (1.0 - mask.float()) * (-50.0)\n",
    "        return logits\n",
    "\n",
    "    def learned_sparsity_penalty(self):\n",
    "        \"\"\"L1 penalty on positive/activated learned logits to discourage adding many corrections.\"\"\"\n",
    "        gate = torch.sigmoid(self.learned_logits)\n",
    "        return gate.sum() / (self.N * max(1, self.k_learn))\n",
    "\n",
    "    def ur_vr_l2(self):\n",
    "        s = 0.0\n",
    "        for r in self.rel_names:\n",
    "            s = s + (self.U_tables[r].weight.norm(p=2) ** 2) + (self.V_tables[r].weight.norm(p=2) ** 2)\n",
    "        return s\n",
    "\n",
    "# -------------------------\n",
    "# Integration + joint training loop \n",
    "# -------------------------\n",
    "def joint_train_relation_and_facets(multi_facet_module, relation_factorizer, nodes, tmap, sent_embs, AW_sparse, device=DEVICE,\n",
    "                                   epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR):\n",
    "    N = len(nodes)\n",
    "    all_node_ids = np.arange(N, dtype=np.int64)\n",
    "    coo = AW_sparse.tocoo()\n",
    "    pos_pairs = list(zip(coo.row.tolist(), coo.col.tolist()))\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # optimizer for both modules\n",
    "    params = list(multi_facet_module.parameters()) + list(relation_factorizer.parameters())\n",
    "    opt = torch.optim.AdamW(params, lr=lr, weight_decay=UR_VR_WEIGHT_DECAY)\n",
    "\n",
    "    def get_H_active_for_nodes(node_list: List[int]):\n",
    "        out_list = []\n",
    "        chunk = 2048\n",
    "        for i in range(0, len(node_list), chunk):\n",
    "            sub = node_list[i:i+chunk]\n",
    "            sub_t = torch.tensor(sub, dtype=torch.long, device=device)\n",
    "            ctx_list = []\n",
    "            sent_dim = sent_embs.shape[1]\n",
    "            for nid in sub:\n",
    "                sidx = tmap.get(str(nid), []) or tmap.get(str(nid), [])\n",
    "                if sidx and len(sidx) > 0:\n",
    "                    vec = sent_embs[sidx].mean(axis=0)\n",
    "                else:\n",
    "                    vec = np.zeros(sent_dim, dtype=np.float32)\n",
    "                ctx_list.append(vec)\n",
    "            context_vecs = torch.tensor(np.stack(ctx_list, axis=0), dtype=torch.float32, device=device)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                out = multi_facet_module(sub_t, context_vecs=context_vecs, neighbor_indices=None)\n",
    "            out_list.append(out[\"H_active\"].detach())  \n",
    "        return torch.cat(out_list, dim=0)\n",
    "\n",
    "    # training iterations\n",
    "    multi_facet_module.train()\n",
    "    relation_factorizer.train()\n",
    "    P = len(pos_pairs)\n",
    "    for ep in range(epochs):\n",
    "        t0 = time.time()\n",
    "        perm = rng.permutation(P)\n",
    "        batch_losses = []\n",
    "        for start in range(0, P, batch_size):\n",
    "            end = min(start + batch_size, P)\n",
    "            batch_pos = [pos_pairs[perm[i]] for i in range(start, end)]\n",
    "            # negatives\n",
    "            batch_neg = []\n",
    "            for (a,b) in batch_pos:\n",
    "                for _ in range(EDGE_NEG_SAMPLE_RATIO):\n",
    "                    neg_j = int(rng.randint(0, N))\n",
    "                    batch_neg.append((a, neg_j))\n",
    "            rel_target = \"cooccurs\" if \"cooccurs\" in relation_factorizer.rel_names else relation_factorizer.rel_names[0]\n",
    "            pair_list = batch_pos + batch_neg\n",
    "            labels = torch.tensor([1.0]*len(batch_pos) + [0.0]*len(batch_neg), dtype=torch.float32, device=device)\n",
    "\n",
    "            src_tensor = torch.tensor([p[0] for p in pair_list], dtype=torch.long, device=device)\n",
    "            dst_tensor = torch.tensor([p[1] for p in pair_list], dtype=torch.long, device=device)\n",
    "\n",
    "            # compute logits from Ur/Vr\n",
    "            logits = relation_factorizer.forward_edge_logits(rel_target, src_tensor, dst_tensor)  # (L,)\n",
    "            prob = torch.sigmoid(logits)\n",
    "\n",
    "            unique_nodes = np.unique(np.concatenate([np.array([p[0] for p in pair_list]), np.array([p[1] for p in pair_list])]))\n",
    "            H_map = {}\n",
    "            chunk_sz = 2048\n",
    "            for i0 in range(0, len(unique_nodes), chunk_sz):\n",
    "                chunk_nodes = unique_nodes[i0:i0+chunk_sz].tolist()\n",
    "                sub_t = torch.tensor(chunk_nodes, dtype=torch.long, device=device)\n",
    "                ctx_list = []\n",
    "                sent_dim = sent_embs.shape[1]\n",
    "                for nid in chunk_nodes:\n",
    "                    sidx = tmap.get(str(nid), []) or tmap.get(str(nid), [])\n",
    "                    if sidx and len(sidx) > 0:\n",
    "                        vec = sent_embs[sidx].mean(axis=0)\n",
    "                    else:\n",
    "                        vec = np.zeros(sent_dim, dtype=np.float32)\n",
    "                    ctx_list.append(vec)\n",
    "                context_vecs = torch.tensor(np.stack(ctx_list, axis=0), dtype=torch.float32, device=device)\n",
    "                out = multi_facet_module(sub_t, context_vecs=context_vecs, neighbor_indices=None)\n",
    "                H_out = out[\"H_active\"]  # (chunk, d)\n",
    "                for idx_local, nid in enumerate(chunk_nodes):\n",
    "                    H_map[int(nid)] = H_out[idx_local]\n",
    "\n",
    "            if not hasattr(relation_factorizer, \"ur_to_h_linear\"):\n",
    "                relation_factorizer.ur_to_h_linear = nn.Linear(relation_factorizer.drank, D_MODEL).to(device)\n",
    "            # get Ur for unique src nodes\n",
    "            src_unique = torch.tensor([int(x) for x in unique_nodes], dtype=torch.long, device=device)\n",
    "            # map Ur -> d_model\n",
    "            Ur_batch = relation_factorizer.U_tables[rel_target](src_unique)  # (U, drank)\n",
    "            projected = relation_factorizer.ur_to_h_linear(Ur_batch)  # (U, d_model)\n",
    "            # build target H matrix\n",
    "            H_target = torch.stack([H_map[int(x)] for x in unique_nodes], dim=0)  # (U, d_model)\n",
    "            align_loss = F.mse_loss(projected, H_target)\n",
    "\n",
    "            # main BCE loss for relation\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "            # regularizers\n",
    "            l1_slearn = relation_factorizer.learned_sparsity_penalty() * SLEARN_L1_LAMBDA\n",
    "            l2_uv = relation_factorizer.ur_vr_l2() * UR_VR_L2_LAMBDA\n",
    "\n",
    "            loss = bce_loss + 0.1 * align_loss + l1_slearn + l2_uv\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            batch_losses.append(float(loss.detach().cpu().numpy()))\n",
    "\n",
    "        avg_loss = float(np.mean(batch_losses)) if len(batch_losses) > 0 else 0.0\n",
    "        t1 = time.time()\n",
    "        print(f\"[Epoch {ep+1}/{epochs}] avg_loss={avg_loss:.6f} time={(t1-t0):.1f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# run main \n",
    "# -------------------------\n",
    "def run_relation_module_with_facets():\n",
    "    assert os.path.isdir(DATA_DIR), f\"{DATA_DIR} missing\"\n",
    "    assert os.path.exists(NODES_JSONL)\n",
    "    assert os.path.exists(TMAP_JSON)\n",
    "    assert os.path.exists(SENT_EMBS_NPY)\n",
    "    assert os.path.exists(AW_NPZ)\n",
    "\n",
    "    nodes = load_jsonl(NODES_JSONL)\n",
    "    tmap = load_tmap(TMAP_JSON)\n",
    "    sent_embs = load_sent_embs(SENT_EMBS_NPY)\n",
    "    AW = load_adj_npz(AW_NPZ)\n",
    "    N = len(nodes)\n",
    "\n",
    "    multi_facet_module = MultiFacetModule(num_nodes=N, M=5, d_model=D_MODEL, device=DEVICE, use_context_gate=True, ctx_dim=sent_embs.shape[1])\n",
    "    multi_facet_module.to(DEVICE)\n",
    "    multi_facet_module.update_facets_via_clustering(nodes=nodes, Tmap=tmap, sent_embeddings=sent_embs, per_node_min_context=1)\n",
    "\n",
    "    node_seed_emb = np.zeros((N, sent_embs.shape[1]), dtype=np.float32)\n",
    "    for node in nodes:\n",
    "        nid = node.get(\"id\")\n",
    "        if isinstance(nid, str) and nid.startswith(\"N\"):\n",
    "            try:\n",
    "                idx = int(nid[1:])\n",
    "            except:\n",
    "                idx = int(nid)\n",
    "        else:\n",
    "            idx = int(nid)\n",
    "        sidx = tmap.get(str(nid), []) or tmap.get(str(idx), [])\n",
    "        if sidx and len(sidx) > 0:\n",
    "            node_seed_emb[idx] = sent_embs[sidx].mean(axis=0)\n",
    "        else:\n",
    "            node_seed_emb[idx] = np.zeros(sent_embs.shape[1], dtype=np.float32)\n",
    "\n",
    "    Sr_pairs = build_Sr_from_AW_and_ann(AW, node_seed_emb, K_ANN)\n",
    "\n",
    "    relation_factorizer = RelationFactorizer(num_nodes=N, relation_names=RELATION_NAMES, drank=DRANK, device=DEVICE, k_learn=K_LEARN, Sr_pairs=Sr_pairs)\n",
    "    relation_factorizer.to(DEVICE)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(2*K_LEARN+5, node_seed_emb.shape[0]-1), algorithm=\"auto\", metric=\"cosine\").fit(node_seed_emb)\n",
    "    _, knn_idx = nbrs.kneighbors(node_seed_emb, return_distance=True)\n",
    "    learned_idx = np.full((N, K_LEARN), -1, dtype=np.int64)\n",
    "    for i in range(N):\n",
    "        picks = []\n",
    "        for cand in knn_idx[i]:\n",
    "            if i == cand: \n",
    "                continue\n",
    "            if (i, int(cand)) in Sr_pairs:\n",
    "                continue\n",
    "            picks.append(int(cand))\n",
    "            if len(picks) >= K_LEARN:\n",
    "                break\n",
    "        for k in range(K_LEARN):\n",
    "            learned_idx[i, k] = picks[k] if k < len(picks) else -1\n",
    "    relation_factorizer.set_learned_candidates(learned_idx)\n",
    "\n",
    "    # run joint training\n",
    "    joint_train_relation_and_facets(multi_facet_module, relation_factorizer, nodes, tmap, sent_embs, AW, device=DEVICE, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_relation_module_with_facets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad5915",
   "metadata": {},
   "source": [
    "### 3.4.2.1 Hierarchical Architecture (Addressing the scalability issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513df50",
   "metadata": {},
   "source": [
    "#### Phase 1: Differentiable Graph Coarsening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG \n",
    "# -------------------------\n",
    "DATA_DIR = \"./data\"\n",
    "NODES_JSONL = os.path.join(DATA_DIR, \"entities.jsonl\")\n",
    "TMAP_JSON = os.path.join(DATA_DIR, \"tmap.json\")\n",
    "SENT_EMBS_NPY = os.path.join(DATA_DIR, \"sent_embs.npy\")\n",
    "AW_NPZ = os.path.join(DATA_DIR, \"AW.npz\")\n",
    "RELATIONS_DIR = os.path.join(DATA_DIR, \"relations\")  \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyperparams\n",
    "K_COMM = 256         \n",
    "BATCH_EPOCHS = 30\n",
    "LR = 1e-3\n",
    "RECON_MASK_TOPK = 1000000  # cap for masked reconstruction entries to compute loss on\n",
    "ENTROPY_COEFF = 1e-3   \n",
    "PRINT_EVERY = 1\n",
    "\n",
    "# -------------------------\n",
    "# I/O helpers\n",
    "# -------------------------\n",
    "def load_jsonl(path):\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def load_tmap(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_sent_embeddings(path):\n",
    "    return np.load(path)\n",
    "\n",
    "def load_sparse_npz(path):\n",
    "    return sp.load_npz(path).tocsr()\n",
    "\n",
    "def load_relations(rel_dir):\n",
    "    if not os.path.isdir(rel_dir):\n",
    "        return []\n",
    "    files = sorted([f for f in os.listdir(rel_dir) if f.endswith(\".npz\")])\n",
    "    mats = []\n",
    "    for fn in files:\n",
    "        mats.append(load_sparse_npz(os.path.join(rel_dir, fn)))\n",
    "    return mats\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: sparse -> torch\n",
    "# -------------------------\n",
    "def scipy_csr_to_edge_index_and_weight(A_csr):\n",
    "    A_coo = A_csr.tocoo()\n",
    "    row = torch.tensor(A_coo.row, dtype=torch.long)\n",
    "    col = torch.tensor(A_coo.col, dtype=torch.long)\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "    edge_weight = torch.tensor(A_coo.data, dtype=torch.float32)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# -------------------------\n",
    "# GNNCluster (2-layer GCN -> K logits)\n",
    "# -------------------------\n",
    "class GNNCluster(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, K):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim)\n",
    "        self.conv2 = GCNConv(hid_dim, hid_dim)\n",
    "        self.lin = nn.Linear(hid_dim, K)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        logits = self.lin(x)\n",
    "        return logits  # (N, K)\n",
    "\n",
    "# -------------------------\n",
    "# Differentiable Coarsening wrapper\n",
    "# -------------------------\n",
    "class DifferentiableCoarsening(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, K, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.gnncluster = GNNCluster(in_dim, hid_dim, K).to(device)\n",
    "\n",
    "    def forward(self, Z, A_edge_index, A_edge_weight):\n",
    "        logits = self.gnncluster(Z, A_edge_index, A_edge_weight)  # (N, K)\n",
    "        C = F.softmax(logits, dim=-1)                            # row-wise soft assignment\n",
    "        return C, logits\n",
    "\n",
    "    def compute_coarse_embeddings(self, C, Z):\n",
    "        H_coarse = C.t() @ Z   # (K, d)\n",
    "        return H_coarse\n",
    "\n",
    "    def compute_Acoarse_from_sparse(self, C, A_scipy):\n",
    "        C_torch = C  # (N, K)\n",
    "        AC = torch.tensor((A_scipy @ C_torch.detach().cpu().numpy()), dtype=torch.float32, device=self.device)  # (N, K)\n",
    "        Acoarse = C_torch.t() @ AC  # (K, K) on device\n",
    "        return Acoarse\n",
    "\n",
    "    def reconstruct_fine_from_coarse(self, C, Acoarse):\n",
    "        return C @ (Acoarse @ C.t())  # (N,N) dense -> careful for memory (we will only sample entries later)\n",
    "\n",
    "# -------------------------\n",
    "# Helper: sample mask indices for masked reconstruction loss\n",
    "# -------------------------\n",
    "def sample_recon_indices(A_scipy, max_samples):\n",
    "    # returns arrays (rows, cols, vals) sampled from A (positives) and random negatives\n",
    "    nz_rows, nz_cols = A_scipy.nonzero()\n",
    "    nnz = len(nz_rows)\n",
    "    if nnz == 0:\n",
    "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64), np.array([], dtype=np.float32)\n",
    "    idxs = np.arange(nnz)\n",
    "    if nnz > max_samples // 2:\n",
    "        chosen = np.random.choice(idxs, size=max_samples//2, replace=False)\n",
    "    else:\n",
    "        chosen = idxs\n",
    "    pos_r = nz_rows[chosen]; pos_c = nz_cols[chosen]; pos_v = A_scipy.data[chosen]\n",
    "    # sample equal number of negative pairs uniformly (ensure not in nz)\n",
    "    neg_count = len(pos_r)\n",
    "    N = A_scipy.shape[0]\n",
    "    neg_r = np.random.randint(0, N, size=neg_count)\n",
    "    neg_c = np.random.randint(0, N, size=neg_count)\n",
    "    # filter negatives that accidentally are positives\n",
    "    mask = A_scipy[neg_r, neg_c].A1 == 0\n",
    "    # if few pass, expand until we have enough (bounded)\n",
    "    attempts = 0\n",
    "    while mask.sum() < neg_count and attempts < 5:\n",
    "        more_r = np.random.randint(0, N, size=neg_count)\n",
    "        more_c = np.random.randint(0, N, size=neg_count)\n",
    "        combined_mask = A_scipy[more_r, more_c].A1 == 0\n",
    "        sel = combined_mask.nonzero()[0]\n",
    "        if len(sel) > 0:\n",
    "            take = min(neg_count - mask.sum(), len(sel))\n",
    "            insert_idx = np.where(mask == False)[0][:take]\n",
    "            neg_r[insert_idx] = more_r[sel[:take]]\n",
    "            neg_c[insert_idx] = more_c[sel[:take]]\n",
    "            mask = A_scipy[neg_r, neg_c].A1 == 0\n",
    "        attempts += 1\n",
    "    neg_v = np.zeros_like(neg_r, dtype=np.float32)\n",
    "    # concatenate pos and neg\n",
    "    rows = np.concatenate([pos_r, neg_r])\n",
    "    cols = np.concatenate([pos_c, neg_c])\n",
    "    vals = np.concatenate([pos_v.astype(np.float32), neg_v])\n",
    "    return rows, cols, vals\n",
    "\n",
    "# -------------------------\n",
    "# Main run for Phase 1\n",
    "# -------------------------\n",
    "def run_phase1():\n",
    "    assert os.path.isdir(DATA_DIR), f\"{DATA_DIR} missing\"\n",
    "    assert os.path.exists(NODES_JSONL), f\"{NODES_JSONL} missing\"\n",
    "    assert os.path.exists(TMAP_JSON), f\"{TMAP_JSON} missing\"\n",
    "    assert os.path.exists(SENT_EMBS_NPY), f\"{SENT_EMBS_NPY} missing\"\n",
    "    assert os.path.exists(AW_NPZ), f\"{AW_NPZ} missing\"\n",
    "\n",
    "    device = DEVICE\n",
    "    nodes = load_jsonl(NODES_JSONL)\n",
    "    tmap = load_tmap(TMAP_JSON)\n",
    "    sent_embs = load_sent_embeddings(SENT_EMBS_NPY)   # (num_sentences, dim)\n",
    "    AW = load_sparse_npz(AW_NPZ)                      # coarse co-occurrence (one relation fallback)\n",
    "    relations = load_relations(RELATIONS_DIR)\n",
    "    if len(relations) == 0:\n",
    "        relations = [AW]  # single relation fallback\n",
    "\n",
    "    N = len(nodes)\n",
    "    sent_dim = sent_embs.shape[1]\n",
    "\n",
    "    # Build node-level embeddings Z by averaging sentence embeddings per node as explained in spec\n",
    "    Z_np = np.zeros((N, sent_dim), dtype=np.float32)\n",
    "    for node in nodes:\n",
    "        nid = node.get(\"id\")\n",
    "        if isinstance(nid, str) and nid.startswith(\"N\"):\n",
    "            try:\n",
    "                nidx = int(nid[1:])\n",
    "            except:\n",
    "                nidx = int(nid)\n",
    "        else:\n",
    "            nidx = int(nid)\n",
    "        sidx = tmap.get(str(nid), []) or tmap.get(str(nidx), [])\n",
    "        if sidx and len(sidx) > 0:\n",
    "            Z_np[nidx] = sent_embs[sidx].mean(axis=0)\n",
    "        else:\n",
    "            Z_np[nidx] = np.random.normal(scale=0.01, size=(sent_dim,))\n",
    "    Z = torch.tensor(Z_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    # build edge_index / edge_weight for GCN from AW (use AW as structural connectivity)\n",
    "    edge_index, edge_weight = scipy_csr_to_edge_index_and_weight(AW)\n",
    "    edge_index = edge_index.to(device); edge_weight = edge_weight.to(device)\n",
    "\n",
    "    model = DifferentiableCoarsening(in_dim=sent_dim, hid_dim=GCN_HID, K=K_COMM, device=device).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "\n",
    "    Shybrid_path = os.path.join(DATA_DIR, \"Shybrid.npz\")\n",
    "    Shybrid = None\n",
    "    if os.path.exists(Shybrid_path):\n",
    "        Shybrid = load_sparse_npz(Shybrid_path)\n",
    "    for epoch in range(BATCH_EPOCHS):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        C, logits = model(Z, edge_index, edge_weight)\n",
    "        entropy = -(C * torch.log(C + 1e-12)).sum(dim=-1).mean()\n",
    "        ent_loss = ENTROPY_COEFF * entropy\n",
    "\n",
    "        recon_loss_total = 0.0\n",
    "        for r_idx, Ar in enumerate(relations):\n",
    "            # compute coarse adjacency\n",
    "            Acoarse = model.compute_Acoarse_from_sparse(C, Ar)  # (K,K)\n",
    "            rows, cols, vals = sample_recon_indices(Ar, max_samples=RECON_MASK_TOPK)\n",
    "            if rows.size == 0:\n",
    "                continue\n",
    "            V = (Acoarse @ C.t())  # (K, N)\n",
    "            Cr = C[rows]         # (S, K)\n",
    "            Vc = V[:, cols].t()  # (S, K)\n",
    "            A_hat_vals = (Cr * Vc).sum(dim=1)  # (S,)\n",
    "            target = torch.tensor(vals, dtype=torch.float32, device=device)\n",
    "            recon_loss = F.mse_loss(A_hat_vals, target)\n",
    "            recon_loss_total = recon_loss_total + recon_loss\n",
    "\n",
    "        loss = recon_loss_total + ent_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (epoch + 1) % PRINT_EVERY == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                C_eval, _ = model(Z, edge_index, edge_weight)\n",
    "                H_coarse = model.compute_coarse_embeddings(C_eval, Z)  # (K, d)\n",
    "                print(f\"[Epoch {epoch+1}/{BATCH_EPOCHS}] loss={loss.item():.6f} recon={recon_loss_total.item() if hasattr(recon_loss_total,'item') else recon_loss_total:.6f} entropy={entropy.item():.6f} time={time.time()-t0:.1f}s\")\n",
    "                # save assignments and coarse graph outputs\n",
    "                out_dir = os.path.join(DATA_DIR, \"coarsening_out\")\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                np.save(os.path.join(out_dir, \"C.npy\"), C_eval.detach().cpu().numpy())\n",
    "                np.save(os.path.join(out_dir, \"H_coarse.npy\"), H_coarse.detach().cpu().numpy())\n",
    "                # save Acoarse for each relation\n",
    "                for r_idx, Ar in enumerate(relations):\n",
    "                    Acoarse = model.compute_Acoarse_from_sparse(C_eval, Ar)\n",
    "                    np.save(os.path.join(out_dir, f\"Acoarse_r{r_idx}.npy\"), Acoarse.detach().cpu().numpy())\n",
    "\n",
    "    print(\"Phase-1 coarsening finished. Outputs saved under ./data/coarsening_out\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_phase1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c9423",
   "metadata": {},
   "source": [
    "#### Phase 2: The Coarse-Grained Model (Learning \"Highways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG \n",
    "# -------------------------\n",
    "DATA_DIR = \"./data\"\n",
    "COARSE_ASSIGN_NPY = os.path.join(DATA_DIR, \"C_assign.npy\")    # shape (N, C) soft assignments\n",
    "PCOARSE_JSONL = os.path.join(DATA_DIR, \"Pcoarse.jsonl\")\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"coarse_ebm_out\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "D_MODEL = 256\n",
    "EPOCHS = 200\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 1  # coarse model is small; training is effectively on whole coarse graph\n",
    "LAMBDA_DAG = 1.0     # weight for acyclicity penalty \n",
    "LAMBDA_MSE = 1.0     # weight for prior imitation loss\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# -------------------------\n",
    "# Utilities to load coarsened prior\n",
    "# -------------------------\n",
    "def load_coarse_assign(path: str) -> np.ndarray:\n",
    "    return np.load(path)  # shape (N, C)\n",
    "\n",
    "def load_pcoarse(path: str) -> List[dict]:\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def build_coarse_targets(pcoarse: List[dict], num_communities: int) -> Tuple[np.ndarray, Dict[str,int]]:\n",
    "    # Collect unique relations and build mapping\n",
    "    rel_set = []\n",
    "    for rec in pcoarse:\n",
    "        r = rec[\"relation\"]\n",
    "        if r not in rel_set:\n",
    "            rel_set.append(r)\n",
    "    rel_map = {r:i for i,r in enumerate(rel_set)}\n",
    "    R = len(rel_set)\n",
    "    target_A = np.zeros((R, num_communities, num_communities), dtype=np.float32)\n",
    "    # Also build mask to indicate which pairs exist in prior (for weighted loss)\n",
    "    mask = np.zeros_like(target_A, dtype=np.float32)\n",
    "    for rec in pcoarse:\n",
    "        ca = int(rec[\"ca\"])\n",
    "        cb = int(rec[\"cb\"])\n",
    "        r = rec[\"relation\"]\n",
    "        score = float(rec.get(\"score\", 1.0))\n",
    "        ridx = rel_map[r]\n",
    "        # aggregate by taking max (keeps strong signals); you could average weighted by counts\n",
    "        target_A[ridx, ca, cb] = max(target_A[ridx, ca, cb], score)\n",
    "        mask[ridx, ca, cb] = 1.0\n",
    "    return target_A, mask, rel_map\n",
    "\n",
    "# -------------------------\n",
    "# Coarse EBM Module\n",
    "# -------------------------\n",
    "class CoarseEBM(nn.Module):\n",
    "    def __init__(self, C: int, R: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.R = R\n",
    "        self.d = d_model\n",
    "        # Coarse community embeddings\n",
    "        self.H_coarse = nn.Parameter(torch.randn(C, d_model) * 0.02)\n",
    "        # Dense adjacency logits for each relation (R, C, C)\n",
    "        self.A_logits = nn.Parameter(torch.randn(R, C, C) * 0.02)\n",
    "        # small MLP readout to produce features for any downstream use\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        A_prob = torch.sigmoid(self.A_logits)  # (R, C, C)\n",
    "        H = self.readout(self.H_coarse)        # (C, d)\n",
    "        return A_prob, H\n",
    "\n",
    "    def acyclicity_penalty(self, Ar: Tensor) -> Tensor:\n",
    "        # h(Ar) = tr(exp(Ar ⊙ Ar)) - C\n",
    "        sq = Ar * Ar\n",
    "        # matrix exponential\n",
    "        expm = torch.matrix_exp(sq)\n",
    "        tr = torch.trace(expm)\n",
    "        return tr - float(self.C)\n",
    "\n",
    "# -------------------------\n",
    "# Training routine\n",
    "# -------------------------\n",
    "def train_coarse_ebm(\n",
    "    module: CoarseEBM,\n",
    "    target_A: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    causal_relation_names: List[str],\n",
    "    rel_map: Dict[str,int],\n",
    "    epochs: int = EPOCHS,\n",
    "    lr: float = LR,\n",
    "    lambda_dag: float = LAMBDA_DAG,\n",
    "    lambda_mse: float = LAMBDA_MSE,\n",
    "):\n",
    "    device = next(module.parameters()).device\n",
    "    target_t = torch.tensor(target_A, dtype=torch.float32, device=device)  # (R,C,C)\n",
    "    mask_t = torch.tensor(mask, dtype=torch.float32, device=device)\n",
    "    optimizer = torch.optim.AdamW(module.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    for ep in range(epochs):\n",
    "        t0 = time.time()\n",
    "        module.train()\n",
    "        optimizer.zero_grad()\n",
    "        A_prob, H = module()  # A_prob (R,C,C)\n",
    "        # imitation loss: MSE but only where mask==1 (we want to encourage matching known priors)\n",
    "        mse_loss = F.mse_loss(A_prob * mask_t, target_t * mask_t, reduction='sum')\n",
    "        # normalize by number of masked entries to keep scale stable\n",
    "        denom = mask_t.sum().clamp_min(1.0)\n",
    "        mse_loss = mse_loss / denom\n",
    "        dag_pen = torch.tensor(0.0, device=device)\n",
    "        for rname in causal_relation_names:\n",
    "            if rname not in rel_map:\n",
    "                continue\n",
    "            ridx = rel_map[rname]\n",
    "            Ar = A_prob[ridx]  # (C,C)\n",
    "            dag_pen = dag_pen + module.acyclicity_penalty(Ar)\n",
    "        loss = lambda_mse * mse_loss + lambda_dag * dag_pen\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_epoch = time.time() - t0\n",
    "        if (ep + 1) % 5 == 0 or ep == 0:\n",
    "            with torch.no_grad():\n",
    "                reg_A_mean = A_prob.mean().item()\n",
    "                print(f\"[Epoch {ep+1}/{epochs}] loss={loss.item():.6f} mse={mse_loss.item():.6f} dag={dag_pen.item():.6f} A_mean={reg_A_mean:.4f} time={t_epoch:.2f}s\")\n",
    "    # final save\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        A_prob_final, H_final = module()\n",
    "        torch.save(A_prob_final.cpu(), os.path.join(OUT_DIR, \"Acoarse_prob.pt\"))\n",
    "        torch.save(H_final.cpu(), os.path.join(OUT_DIR, \"Hcoarse.pt\"))\n",
    "        torch.save(module.state_dict(), os.path.join(OUT_DIR, \"coarse_ebm_state.pt\"))\n",
    "    return module\n",
    "\n",
    "# -------------------------\n",
    "# Main driver for Phase-2\n",
    "# -------------------------\n",
    "def run_phase2():\n",
    "    assert os.path.exists(COARSE_ASSIGN_NPY), f\"{COARSE_ASSIGN_NPY} missing.\"\n",
    "    assert os.path.exists(PCOARSE_JSONL), f\"{PCOARSE_JSONL} missing.\"\n",
    "\n",
    "    C_assign = load_coarse_assign(COARSE_ASSIGN_NPY)  # (N, C)\n",
    "    N, C = C_assign.shape\n",
    "\n",
    "    pcoarse = load_pcoarse(PCOARSE_JSONL)\n",
    "    target_A_np, mask_np, rel_map = build_coarse_targets(pcoarse, num_communities=C)\n",
    "\n",
    "    R = target_A_np.shape[0]\n",
    "    print(f\"Loaded coarse assign: N={N}, C={C}, relations={R}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CoarseEBM(C=C, R=R, d_model=D_MODEL).to(device)\n",
    "\n",
    "    # Determine which relation names correspond to causal relations requiring DAG constraint.\n",
    "    causal_relations = [r for r in rel_map.keys() if \"CAUSE\" in r.upper() or \"DIRECT\" in r.upper() or \"CAUSES\" in r.upper()]\n",
    "    print(\"Causal relations (acyclicity applied):\", causal_relations)\n",
    "\n",
    "    trained = train_coarse_ebm(\n",
    "        module=model,\n",
    "        target_A=target_A_np,\n",
    "        mask=mask_np,\n",
    "        causal_relation_names=causal_relations,\n",
    "        rel_map=rel_map,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LR,\n",
    "        lambda_dag=LAMBDA_DAG,\n",
    "        lambda_mse=LAMBDA_MSE,\n",
    "    )\n",
    "    print(\"Phase-2 finished. Outputs in:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_phase2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bad12",
   "metadata": {},
   "source": [
    "#### Phase 3: The fine-grained Model (with Adaptive context integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_DIR = \"./data\"   \n",
    "Z_NPY = os.path.join(DATA_DIR, \"Z.npy\")\n",
    "C_NPY = os.path.join(DATA_DIR, \"C.npy\")\n",
    "ACOARSE_NPY = os.path.join(DATA_DIR, \"Acoarse.npy\")\n",
    "HCOARSE_NPY = os.path.join(DATA_DIR, \"Hcoarse.npy\")\n",
    "PRICH_JSONL = os.path.join(DATA_DIR, \"prich.jsonl\")  \n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "D_MODEL = 256        # should match Hcoarse dim\n",
    "DZ = 64             \n",
    "K_COMM = None        # inferred\n",
    "R_RELS = None        # inferred\n",
    "\n",
    "BATCH_SIZE = 1024    # \n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# -------------------------\n",
    "# Basic loaders \n",
    "# -------------------------\n",
    "assert os.path.exists(Z_NPY), f\"{Z_NPY} not found\"\n",
    "assert os.path.exists(C_NPY), f\"{C_NPY} not found\"\n",
    "assert os.path.exists(ACOARSE_NPY), f\"{ACOARSE_NPY} not found\"\n",
    "assert os.path.exists(HCOARSE_NPY), f\"{HCOARSE_NPY} not found\"\n",
    "assert os.path.exists(PRICH_JSONL), f\"{PRICH_JSONL} not found\"\n",
    "\n",
    "Z = np.load(Z_NPY)                 # (N, dz)\n",
    "C_mat = np.load(C_NPY)             # (N, K)\n",
    "Acoarse = np.load(ACOARSE_NPY)     # (K, K, R)\n",
    "Hcoarse = np.load(HCOARSE_NPY)     # (K, d_model)\n",
    "\n",
    "N = Z.shape[0]\n",
    "DZ = Z.shape[1]\n",
    "K_COMM = C_mat.shape[1]\n",
    "R_RELS = Acoarse.shape[2]\n",
    "assert Hcoarse.shape[0] == K_COMM\n",
    "assert Hcoarse.shape[1] == D_MODEL\n",
    "assert Acoarse.shape[0] == K_COMM and Acoarse.shape[1] == K_COMM\n",
    "\n",
    "print(f\"Loaded: N={N}, dz={DZ}, K={K_COMM}, R={R_RELS}, d_model={D_MODEL}\")\n",
    "\n",
    "# -------------------------\n",
    "# Prich dataset (node-level rulebook)\n",
    "# -------------------------\n",
    "class PriChDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str):\n",
    "        self.entries = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ln in f:\n",
    "                j = json.loads(ln)\n",
    "                # Expect fields: i, j, r, score\n",
    "                self.entries.append((int(j[\"i\"]), int(j[\"j\"]), int(j[\"r\"]), float(j.get(\"score\", 1.0))))\n",
    "        if len(self.entries) == 0:\n",
    "            raise RuntimeError(\"prich dataset empty\")\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.entries[idx]\n",
    "\n",
    "# collate returns batched tensors\n",
    "def collate_prich(batch):\n",
    "    i_idx = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    j_idx = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    r_idx = torch.tensor([b[2] for b in batch], dtype=torch.long)\n",
    "    score = torch.tensor([b[3] for b in batch], dtype=torch.float32)\n",
    "    return i_idx, j_idx, r_idx, score\n",
    "\n",
    "# -------------------------\n",
    "# FineGrainedPhase3 Module\n",
    "# -------------------------\n",
    "class FineGrainedPhase3(nn.Module):\n",
    "    def __init__(self, dz: int, d_model: int, K: int, R: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.dz = dz\n",
    "        self.d_model = d_model\n",
    "        self.K = K\n",
    "        self.R = R\n",
    "        self.device = device\n",
    "\n",
    "        # attention projection matrices (WQ, WK, WV) operate in d_model space.\n",
    "        self.WQ = nn.Linear(dz, d_model, bias=False)        # Query from Init(Z_i) -> d_model\n",
    "        self.WK = nn.Linear(d_model, d_model, bias=False)   # Key from Hcoarse -> d_model\n",
    "        self.WV = nn.Linear(d_model, d_model, bias=False)   # Value from Hcoarse -> d_model\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # f_compat: small MLP that outputs [0,1] affinity from two primed embeddings\n",
    "        self.fcompat = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "\n",
    "        if dz != d_model:\n",
    "            self.Zproj = nn.Linear(dz, d_model, bias=False)\n",
    "        else:\n",
    "            self.Zproj = None\n",
    "\n",
    "    def primed_embeddings(self, Z_batch: torch.Tensor, Hcoarse: torch.Tensor) -> torch.Tensor:\n",
    "        # Z_batch: (B, dz) ; Hcoarse: (K, d_model)\n",
    "        Q = self.WQ(Z_batch)                          # (B, d_model)\n",
    "        Kc = self.WK(Hcoarse)                         # (K, d_model)\n",
    "        Vc = self.WV(Hcoarse)                         # (K, d_model)\n",
    "        # attention: Q (B, d) x Kc (K, d) -> scores (B, K)\n",
    "        scores = (Q @ Kc.T) / (self.d_model ** 0.5)   # (B, K)\n",
    "        att = F.softmax(scores, dim=-1)               # (B, K)\n",
    "        context = att @ Vc                            # (B, d_model)\n",
    "        # init projection\n",
    "        if self.Zproj is not None:\n",
    "            init_proj = self.Zproj(Z_batch)          # (B, d)\n",
    "        else:\n",
    "            # if dz==d_model, cast directly\n",
    "            init_proj = Z_batch                       # (B, d)\n",
    "        H0 = self.layer_norm(init_proj + context)     # (B, d)\n",
    "        return H0\n",
    "\n",
    "    def compute_coarse_prior_batch(self, C: torch.Tensor, Acoarse: torch.Tensor, i_idxs: torch.Tensor, j_idxs: torch.Tensor, r_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        Ci = C[i_idxs]         # (B, K)\n",
    "        Cj = C[j_idxs]         # (B, K)\n",
    "        # We'll process per distinct r value for efficiency\n",
    "        batch_size = Ci.shape[0]\n",
    "        out = torch.empty(batch_size, device=self.device, dtype=torch.float32)\n",
    "        # unique relations in batch\n",
    "        r_unique, inv = torch.unique(r_idxs, return_inverse=True)\n",
    "        for idx_r, r in enumerate(r_unique):\n",
    "            mask = (r_idxs == r)\n",
    "            pos = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "            Ci_sub = Ci[pos]   # (b, K)\n",
    "            Cj_sub = Cj[pos]   # (b, K)\n",
    "            A_r = Acoarse[:, :, int(r.item())]  \n",
    "            tmp = Ci_sub @ A_r    # (b, K)\n",
    "            vals = (tmp * Cj_sub).sum(dim=-1)    # (b,)\n",
    "            out[pos] = vals\n",
    "        return out  # (B,)\n",
    "\n",
    "    def forward_predict_batch(self, Z_batch: torch.Tensor, idx_i: torch.Tensor, idx_j: torch.Tensor, r_idxs: torch.Tensor,\n",
    "                              C_tensor: torch.Tensor, Acoarse_tensor: torch.Tensor, Hcoarse_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # Z_batch: batch of unique Z vectors corresponding to nodes used in this batch.\n",
    "        # Bwe fetch Z for i and j individually outside and pass relevant Z_i and Z_j.\n",
    "        raise NotImplementedError(\"Use predict_pair_batch convenience below\")\n",
    "\n",
    "    def predict_pair_batch(self,\n",
    "                           Z_i: torch.Tensor,    # (B, dz)\n",
    "                           Z_j: torch.Tensor,    # (B, dz)\n",
    "                           i_idxs: torch.Tensor, # (B,)\n",
    "                           j_idxs: torch.Tensor, # (B,)\n",
    "                           r_idxs: torch.Tensor, # (B,)\n",
    "                           C_tensor: torch.Tensor,   # (N, K)\n",
    "                           Acoarse_tensor: torch.Tensor, # (K, K, R)\n",
    "                           Hcoarse_tensor: torch.Tensor   # (K, d)\n",
    "                           ) -> torch.Tensor:\n",
    "        B = i_idxs.shape[0]\n",
    "        # primed embeddings\n",
    "        H0_i = self.primed_embeddings(Z_i, Hcoarse_tensor)  # (B, d)\n",
    "        H0_j = self.primed_embeddings(Z_j, Hcoarse_tensor)  # (B, d)\n",
    "        # fcompat\n",
    "        pair_cat = torch.cat([H0_i, H0_j], dim=-1)          # (B, 2d)\n",
    "        compat_logits = self.fcompat(pair_cat).squeeze(-1)  # (B,)\n",
    "        compat = torch.sigmoid(compat_logits)               # (B,)\n",
    "        # coarse prior per pair\n",
    "        coarse_vals = self.compute_coarse_prior_batch(C_tensor, Acoarse_tensor, i_idxs, j_idxs, r_idxs)  # (B,)\n",
    "        # final\n",
    "        out = coarse_vals * compat\n",
    "        return out, compat_logits  # out in [0,1], compat_logits raw\n",
    "\n",
    "# -------------------------\n",
    "# Helpers to batch fetch Z for indices\n",
    "# -------------------------\n",
    "def tensor_from_numpy(arr: np.ndarray, dtype=torch.float32, device=DEVICE):\n",
    "    return torch.tensor(arr, dtype=dtype, device=device)\n",
    "\n",
    "# -------------------------\n",
    "# Training for Phase-3\n",
    "# -------------------------\n",
    "def train_phase3():\n",
    "    ds = PriChDataset(PRICH_JSONL)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_prich, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = FineGrainedPhase3(dz=DZ, d_model=D_MODEL, K=K_COMM, R=R_RELS, device=DEVICE).to(DEVICE)\n",
    "\n",
    "    # prepare tensors\n",
    "    Z_t = tensor_from_numpy(Z, dtype=torch.float32, device=DEVICE)              # (N, dz)\n",
    "    C_t = tensor_from_numpy(C_mat.astype(np.float32), dtype=torch.float32, device=DEVICE)  # (N, K)\n",
    "    Acoarse_t = tensor_from_numpy(Acoarse.astype(np.float32), dtype=torch.float32, device=DEVICE)  # (K,K,R)\n",
    "    Hcoarse_t = tensor_from_numpy(Hcoarse.astype(np.float32), dtype=torch.float32, device=DEVICE)  # (K, d)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        model.train()\n",
    "        for i_idxs, j_idxs, r_idxs, scores in loader:\n",
    "            i_idxs = i_idxs.to(DEVICE)\n",
    "            j_idxs = j_idxs.to(DEVICE)\n",
    "            r_idxs = r_idxs.to(DEVICE)\n",
    "            scores = scores.to(DEVICE)  \n",
    "\n",
    "            Z_i = Z_t[i_idxs]            # (B, dz)\n",
    "            Z_j = Z_t[j_idxs]            # (B, dz)\n",
    "\n",
    "            # forward predict\n",
    "            pred_probs, compat_logits = model.predict_pair_batch(Z_i, Z_j, i_idxs, j_idxs, r_idxs, C_t, Acoarse_t, Hcoarse_t)\n",
    "            # pred_probs in [0,1], compat_logits raw; we want a training objective that backprops into compat and WQ/WK/WV\n",
    "            # Use MSE on final prob + BCE on compat (signal is whether local affinity needed)\n",
    "            # We'll use a composite loss:\n",
    "            #  - L_prob = MSE(pred_probs, scores)\n",
    "            #  - L_compat = BCEWithLogits(compat_logits, scores_binary) where scores_binary = scores>0.5\n",
    "            scores_bin = (scores > 0.5).float()\n",
    "            L_prob = F.mse_loss(pred_probs, scores)\n",
    "            L_compat = bce_loss(compat_logits, scores_bin)\n",
    "            loss = L_prob + 0.5 * L_compat\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += float(loss.item()) * i_idxs.shape[0]\n",
    "            total_samples += i_idxs.shape[0]\n",
    "\n",
    "        avg = total_loss / (total_samples + 1e-12)\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[Phase3] Epoch {ep+1}/{EPOCHS} avg_loss={avg:.6e} time={elapsed:.1f}s\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"meta\": {\"dz\": DZ, \"d_model\": D_MODEL, \"K\": K_COMM, \"R\": R_RELS}\n",
    "    }, os.path.join(DATA_DIR, \"phase3_finegrained_ckpt.pth\"))\n",
    "    print(\"Saved Phase3 checkpoint\")\n",
    "\n",
    "# -------------------------\n",
    "# Entry point\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_phase3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the above...\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "import torch\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = \"./data\"\n",
    "NODES_JSONL = os.path.join(DATA_DIR, \"entities.jsonl\")\n",
    "TMAP_JSON = os.path.join(DATA_DIR, \"tmap.json\")\n",
    "SENT_EMBS_NPY = os.path.join(DATA_DIR, \"sent_embs.npy\")\n",
    "AW_NPZ = os.path.join(DATA_DIR, \"AW.npz\")\n",
    "PRICH_JSONL = os.path.join(DATA_DIR, \"Prich.jsonl\")        # node-level prior \n",
    "PCOARSE_JSON = os.path.join(DATA_DIR, \"Pcoarse.json\")      # coarse prior \n",
    "C_ASSIGN_NPY = os.path.join(DATA_DIR, \"C_assign.npy\")      # soft assignment C (N x K) \n",
    "ACOARSE_PRED_NPY = os.path.join(DATA_DIR, \"Acoarse_pred.npy\") # predicted coarse adjacency (K,K,R) \n",
    "GFINAL_PRED_NPZ = os.path.join(DATA_DIR, \"Gfinal_preds.npz\")   # predicted edges list or dense structure \n",
    "S_HYBRID_NPZ = os.path.join(DATA_DIR, \"Shybrid.npz\")        # candidate set mask \n",
    "OUTPUT_REPORT = os.path.join(DATA_DIR, \"evaluation_report.json\")\n",
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def load_jsonl(path):\n",
    "    out=[]\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip(): out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def load_npz_adj(path):\n",
    "    m = sp.load_npz(path)\n",
    "    return m\n",
    "\n",
    "def safe_load(path, loader):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return loader(path)\n",
    "\n",
    "# ---------- PARSERS FOR PRIORS ----------\n",
    "def parse_prich(prich_path: str) -> List[Tuple[int,int,int,float]]:\n",
    "    data = load_jsonl(prich_path)\n",
    "    out=[]\n",
    "    for item in data:\n",
    "        # tolerate multiple formats\n",
    "        if \"pair\" in item:\n",
    "            a,b = item[\"pair\"]\n",
    "            r = item.get(\"relation\",0)\n",
    "            s = float(item.get(\"score\",1.0))\n",
    "            out.append((int(a if isinstance(a,int) else int(str(a).lstrip(\"N\"))),\n",
    "                        int(b if isinstance(b,int) else int(str(b).lstrip(\"N\"))),\n",
    "                        int(r), s))\n",
    "        elif all(k in item for k in (\"i\",\"j\")):\n",
    "            i = int(item[\"i\"]) if isinstance(item[\"i\"],int) else int(str(item[\"i\"]).lstrip(\"N\"))\n",
    "            j = int(item[\"j\"]) if isinstance(item[\"j\"],int) else int(str(item[\"j\"]).lstrip(\"N\"))\n",
    "            r = int(item.get(\"r\",0))\n",
    "            s = float(item.get(\"score\",1.0))\n",
    "            out.append((i,j,r,s))\n",
    "    return out\n",
    "\n",
    "def parse_pcoarse(pcoarse_path: str) -> List[Tuple[int,int,int,float]]:\n",
    "    try:\n",
    "        data = json.load(open(pcoarse_path,\"r\",encoding=\"utf-8\"))\n",
    "        out=[]\n",
    "        if isinstance(data, list):\n",
    "            for d in data:\n",
    "                ca = int(d.get(\"ca\",d.get(\"c_a\",0)))\n",
    "                cb = int(d.get(\"cb\",d.get(\"c_b\",0)))\n",
    "                r = int(d.get(\"r\",0))\n",
    "                s = float(d.get(\"score\", d.get(\"sc\",1.0)))\n",
    "                out.append((ca,cb,r,s))\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ---------- METRIC COMPUTATIONS ----------\n",
    "def rmse_over_S(pred_scores: List[float], target_scores: List[float]) -> float:\n",
    "    p = np.asarray(pred_scores, dtype=np.float64)\n",
    "    t = np.asarray(target_scores, dtype=np.float64)\n",
    "    return float(np.sqrt(np.mean((p - t)**2)))\n",
    "\n",
    "def precision_recall_at_k(pred_pairs_scores: List[Tuple[Tuple[int,int,int], float]],\n",
    "                          true_edge_set: Dict[Tuple[int,int,int], float],\n",
    "                          K: int) -> Tuple[float,float]:\n",
    "    # Sort by predicted score desc\n",
    "    sorted_preds = sorted(pred_pairs_scores, key=lambda x: -x[1])[:K]\n",
    "    tp = 0\n",
    "    for (i,j,r),_ in sorted_preds:\n",
    "        if (i,j,r) in true_edge_set and true_edge_set[(i,j,r)]>0:\n",
    "            tp += 1\n",
    "    precision = tp / max(K,1)\n",
    "    recall = tp / max(len([x for x in true_edge_set.values() if x>0]), 1)\n",
    "    return precision, recall\n",
    "\n",
    "def auc_and_ap(pred_list: List[float], labels: List[int]) -> Tuple[float,float]:\n",
    "    if len(set(labels))==1:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    try:\n",
    "        return float(roc_auc_score(labels, pred_list)), float(average_precision_score(labels, pred_list))\n",
    "    except Exception:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "def soft_assignment_entropy(C: np.ndarray) -> np.ndarray:\n",
    "    # C is N x K, rows sum to 1\n",
    "    eps = 1e-12\n",
    "    logc = np.log(C + eps)\n",
    "    ent = -np.sum(C * logc, axis=1)\n",
    "    return ent\n",
    "\n",
    "def acyclicity_trace_exponential(A: np.ndarray) -> float:\n",
    "    from scipy.linalg import expm\n",
    "    A2 = A * A\n",
    "    M = expm(A2)\n",
    "    return float(np.trace(M) - A.shape[0])\n",
    "\n",
    "# ---------- ASSEMBLY HELPERS ----------\n",
    "def assemble_Gfinal_from_components(C: np.ndarray,\n",
    "                                    Acoarse: np.ndarray,\n",
    "                                    fcompat_fn,\n",
    "                                    relation_count: int,\n",
    "                                    nodes_per_community: Optional[List[List[int]]] = None\n",
    "                                   ) -> Dict[Tuple[int,int,int], float]:\n",
    "    # Adaptive broadcasting as described: Aijr = Acoarse[k,m,r] * fcompat(Hi,Hj)\n",
    "    N, K = C.shape\n",
    "    preds = {}\n",
    "    topK = min(8, K)  # pragmatic default\n",
    "    top_indices = np.argsort(-C, axis=1)[:, :topK]  # (N, topK)\n",
    "    for i in range(N):\n",
    "        i_tops = top_indices[i]\n",
    "        for j in range(N):\n",
    "            j_tops = top_indices[j]\n",
    "            for r in range(relation_count):\n",
    "                coarse_score = 0.0\n",
    "                for k in i_tops:\n",
    "                    for l in j_tops:\n",
    "                        coarse_score += C[i,k] * C[j,l] * float(Acoarse[k,l,r])\n",
    "                # compute local affinity\n",
    "                local_aff = float(fcompat_fn(i,j))\n",
    "                score = coarse_score * local_aff\n",
    "                if score>0:\n",
    "                    preds[(i,j,r)] = score\n",
    "    return preds\n",
    "\n",
    "# ---------- MAIN EVALUATION WORKFLOW ----------\n",
    "def evaluate_all(data_dir=DATA_DIR, report_path=OUTPUT_REPORT, top_k_eval=100):\n",
    "    # load basic artifacts\n",
    "    assert os.path.isdir(data_dir), f\"Data dir {data_dir} missing\"\n",
    "    AW = load_npz_adj(AW_NPZ) if os.path.exists(AW_NPZ) else None\n",
    "    nodes = load_jsonl(NODES_JSONL) if os.path.exists(NODES_JSONL) else None\n",
    "    prich = parse_prich(PRICH_JSONL) if os.path.exists(PRICH_JSONL) else []\n",
    "    pcoarse = parse_pcoarse(PCOARSE_JSON) if os.path.exists(PCOARSE_JSON) else []\n",
    "    C = np.load(C_ASSIGN_NPY) if os.path.exists(C_ASSIGN_NPY) else None\n",
    "    Acoarse_pred = np.load(ACOARSE_PRED_NPY) if os.path.exists(ACOARSE_PRED_NPY) else None\n",
    "    gfinal_npz = None\n",
    "    if os.path.exists(GFINAL_PRED_NPZ):\n",
    "        gfinal_npz = np.load(GFINAL_PRED_NPZ, allow_pickle=True)\n",
    "    shybrid_mask = None\n",
    "    if os.path.exists(S_HYBRID_NPZ):\n",
    "        shybrid_mask = np.load(S_HYBRID_NPZ, allow_pickle=True)\n",
    "    true_edges = {}\n",
    "    if prich:\n",
    "        for i,j,r,s in prich:\n",
    "            true_edges[(i,j,r)] = float(s)\n",
    "    if not true_edges and AW is not None:\n",
    "        coo = AW.tocoo()\n",
    "        for a,b,v in zip(coo.row.tolist(), coo.col.tolist(), coo.data.tolist()):\n",
    "            true_edges[(int(a),int(b),0)] = float(v)\n",
    "    candidate_list = []\n",
    "    if shybrid_mask is not None:\n",
    "        # expect boolean mask shape (N,N) or (N,N,R)\n",
    "        mask = shybrid_mask\n",
    "        if mask.ndim==2:\n",
    "            rows, cols = np.where(mask)\n",
    "            for a,b in zip(rows.tolist(), cols.tolist()):\n",
    "                candidate_list.append((int(a),int(b),0))\n",
    "        elif mask.ndim==3:\n",
    "            R = mask.shape[2]\n",
    "            for r in range(R):\n",
    "                rows, cols = np.where(mask[:,:,r])\n",
    "                for a,b in zip(rows.tolist(), cols.tolist()):\n",
    "                    candidate_list.append((int(a),int(b),int(r)))\n",
    "    elif true_edges:\n",
    "        candidate_list = list(true_edges.keys())\n",
    "    elif AW is not None:\n",
    "        coo = AW.tocoo()\n",
    "        candidate_list = [(int(a),int(b),0) for a,b in zip(coo.row.tolist(), coo.col.tolist())]\n",
    "    else:\n",
    "        raise RuntimeError(\"No candidate set available (need Prich, Shybrid, or AW).\")\n",
    "    # Load/predict scores for candidate set\n",
    "    pred_scores = []\n",
    "    target_scores = []\n",
    "    labels = []\n",
    "    # If Gfinal predictions available as dict-like with keys (i,j,r) -> score\n",
    "    gfinal_dict = None\n",
    "    if gfinal_npz is not None:\n",
    "        # look for 'pred_dict' or arrays\n",
    "        if 'pred_dict' in gfinal_npz.files:\n",
    "            gfinal_dict = gfinal_npz['pred_dict'].item()\n",
    "        else:\n",
    "            # try arrays 'i','j','r','score'\n",
    "            if all(k in gfinal_npz.files for k in ('i','j','r','score')):\n",
    "                arr_i = gfinal_npz['i']; arr_j = gfinal_npz['j']; arr_r = gfinal_npz['r']; arr_s = gfinal_npz['score']\n",
    "                gfinal_dict = {(int(a),int(b),int(c)): float(s) for a,b,c,s in zip(arr_i,arr_j,arr_r,arr_s)}\n",
    "    assembled_preds = None\n",
    "    def default_fcompat(i,j):\n",
    "        return 1.0  # trivial fallback (no fine-grain model)\n",
    "    if C is not None and Acoarse_pred is not None:\n",
    "        # relation_count inferred from Acoarse_pred shape\n",
    "        if Acoarse_pred.ndim==3:\n",
    "            R = Acoarse_pred.shape[2]\n",
    "        else:\n",
    "            R = 1\n",
    "        fcompat_fn = default_fcompat\n",
    "        assembled_preds = {}\n",
    "        N,K = C.shape\n",
    "        for (i,j,r) in candidate_list:\n",
    "            if Acoarse_pred.ndim==3:\n",
    "                A_r = Acoarse_pred[:,:,r]\n",
    "            else:\n",
    "                A_r = Acoarse_pred\n",
    "            ci = C[i]  # K\n",
    "            cj = C[j]\n",
    "            coarse_val = float(ci @ (A_r @ cj))\n",
    "            local_val = float(fcompat_fn(i,j))\n",
    "            assembled_preds[(i,j,r)] = coarse_val * local_val\n",
    "        gfinal_dict = assembled_preds\n",
    "\n",
    "    # Now for metrics compute lists\n",
    "    for (i,j,r) in candidate_list:\n",
    "        pred = float(gfinal_dict.get((i,j,r), 0.0)) if gfinal_dict is not None else 0.0\n",
    "        target = float(true_edges.get((i,j,r), 0.0))\n",
    "        label = 1 if target>0 else 0\n",
    "        pred_scores.append(pred)\n",
    "        target_scores.append(target)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse = rmse_over_S(pred_scores, target_scores)\n",
    "\n",
    "    # Precision@K / Recall@K for K values\n",
    "    K_values = [10, 50, 100, top_k_eval]\n",
    "    pr_at_k = {}\n",
    "    preds_with_pairs = list(zip(candidate_list, pred_scores))\n",
    "    true_edge_map = {k:v for k,v in true_edges.items()}\n",
    "    for K in K_values:\n",
    "        p,r = precision_recall_at_k(preds_with_pairs, true_edge_map, K)\n",
    "        pr_at_k[K] = {\"precision\": p, \"recall\": r}\n",
    "\n",
    "    # AUC / AP\n",
    "    try:\n",
    "        auc, ap = auc_and_ap(pred_scores, labels)\n",
    "    except Exception as e:\n",
    "        auc, ap = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    spearman_corr = None\n",
    "    if pcoarse and Acoarse_pred is not None and C is not None:\n",
    "        node_to_comm = np.argmax(C, axis=1)\n",
    "        agg_true = {}\n",
    "        for i,j,r,s in prich:\n",
    "            ci = int(node_to_comm[i])\n",
    "            cj = int(node_to_comm[j])\n",
    "            agg_true.setdefault((ci,cj,r), []).append(s)\n",
    "        true_vals = []\n",
    "        pred_vals = []\n",
    "        for (ci,cj,r), vals in agg_true.items():\n",
    "            true_mean = float(np.mean(vals))\n",
    "            # predicted coarse\n",
    "            if Acoarse_pred.ndim==3:\n",
    "                pred_mean = float(Acoarse_pred[ci,cj,r])\n",
    "            else:\n",
    "                pred_mean = float(Acoarse_pred[ci,cj])\n",
    "            true_vals.append(true_mean)\n",
    "            pred_vals.append(pred_mean)\n",
    "        if len(true_vals)>=2:\n",
    "            spearman_corr = float(spearmanr(true_vals, pred_vals).correlation)\n",
    "        else:\n",
    "            spearman_corr = float(\"nan\")\n",
    "\n",
    "    purity_metrics = {}\n",
    "    communities_gt_path = os.path.join(data_dir, \"communities.npy\")\n",
    "    if C is not None and os.path.exists(communities_gt_path):\n",
    "        gt = np.load(communities_gt_path)\n",
    "        pred_labels = np.argmax(C, axis=1)\n",
    "        nmi = float(normalized_mutual_info_score(gt, pred_labels))\n",
    "        ari = float(adjusted_rand_score(gt, pred_labels))\n",
    "        ent = soft_assignment_entropy(C)\n",
    "        purity_metrics = {\"nmi\": nmi, \"ari\": ari, \"mean_entropy\": float(ent.mean()), \"median_entropy\": float(np.median(ent))}\n",
    "    elif C is not None:\n",
    "        ent = soft_assignment_entropy(C)\n",
    "        purity_metrics = {\"mean_entropy\": float(ent.mean()), \"median_entropy\": float(np.median(ent))}\n",
    "\n",
    "    acyc_h = None\n",
    "    if Acoarse_pred is not None:\n",
    "        if Acoarse_pred.ndim==3:\n",
    "            total = 0.0\n",
    "            for r in range(Acoarse_pred.shape[2]):\n",
    "                total += acyclicity_trace_exponential(np.asarray(Acoarse_pred[:,:,r], dtype=np.float64))\n",
    "            acyc_h = float(total)\n",
    "        else:\n",
    "            acyc_h = float(acyclicity_trace_exponential(np.asarray(Acoarse_pred, dtype=np.float64)))\n",
    "\n",
    "    # Build report\n",
    "    report = {\n",
    "        \"rmse_candidate_S\": rmse,\n",
    "        \"precision_recall_at_k\": pr_at_k,\n",
    "        \"auc\": auc,\n",
    "        \"average_precision\": ap,\n",
    "        \"spearman_coarse_fine\": spearman_corr,\n",
    "        \"clustering_metrics\": purity_metrics,\n",
    "        \"acyclicity_h_coarse\": acyc_h,\n",
    "        \"num_candidates_evaluated\": len(candidate_list),\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"=== Evaluation Summary ===\")\n",
    "    print(f\"Candidates evaluated: {len(candidate_list)}\")\n",
    "    print(f\"RMSE (candidates): {rmse:.6f}\")\n",
    "    print(f\"AUC: {auc:.6f}, AP: {ap:.6f}\")\n",
    "    print(\"Precision/Recall @ K:\")\n",
    "    for K,v in pr_at_k.items():\n",
    "        print(f\"  @ {K}: precision={v['precision']:.4f}, recall={v['recall']:.4f}\")\n",
    "    if spearman_corr is not None:\n",
    "        print(f\"Coarse<->Fine Spearman: {spearman_corr:.4f}\")\n",
    "    if purity_metrics:\n",
    "        print(\"Clustering: \", purity_metrics)\n",
    "    if acyc_h is not None:\n",
    "        print(f\"Acyclicity h(Acoarse) total: {acyc_h:.6f}\")\n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    return report\n",
    "\n",
    "# Run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    rep = evaluate_all(DATA_DIR, OUTPUT_REPORT, top_k_eval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a802c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import resample\n",
    "import networkx as nx\n",
    "\n",
    "DATA_DIR = \"./data\" \n",
    "PRICH_JSONL = os.path.join(DATA_DIR, \"prich.jsonl\")          # gold fine-grained prior (node i,j,r,score)\n",
    "CPRIOR_JSONL = os.path.join(DATA_DIR, \"cprior.jsonl\")        # teacher LLM prior \n",
    "CAND_PRED_NPZ = os.path.join(DATA_DIR, \"pred_candidates.npz\") # contains rows,cols,rels,scores_pred (arrays)\n",
    "AW_NPZ = os.path.join(DATA_DIR, \"AW.npz\")                    # co-occurrence adjacency (sparse)\n",
    "ACOARSE_NPY = os.path.join(DATA_DIR, \"Acoarse.npy\")          # coarse KxKxR learned scores \n",
    "ACOARSE_TARGET_NPY = os.path.join(DATA_DIR, \"Acoarse_target.npy\") # coarse target aggregated scores \n",
    "C_ASSIGN_NPY = os.path.join(DATA_DIR, \"C_assign.npy\")        # soft assignment C (N x K) from GNNcluster \n",
    "Z_EMB_NPY = os.path.join(DATA_DIR, \"Z.npy\")                  # structural embeddings (N x dz)\n",
    "H_FINAL_NPY = os.path.join(DATA_DIR, \"H_final.npy\")          # final node embeddings (N x dmodel)\n",
    "FACET_GATES_NPY = os.path.join(DATA_DIR, \"gates.npy\")        # gating values (N x M) optional\n",
    "COMMUNITY_LABELS_JSON = os.path.join(DATA_DIR, \"community_labels.json\") # optional ground-truth community map\n",
    "\n",
    "REPORT_OUT = os.path.join(DATA_DIR, \"evaluation_report.json\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def assert_exists(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required artifact missing: {path}\")\n",
    "\n",
    "def load_prich(path: str) -> List[Dict[str, Any]]:\n",
    "    assert_exists(path)\n",
    "    out=[]\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def load_candidates_npz(path: str):\n",
    "    assert_exists(path)\n",
    "    npz = np.load(path, allow_pickle=True)\n",
    "    # expects arrays: rows, cols, rels, scores_pred\n",
    "    for k in (\"rows\",\"cols\",\"rels\",\"scores_pred\"):\n",
    "        if k not in npz:\n",
    "            raise KeyError(f\"{path} missing array '{k}'\")\n",
    "    return npz[\"rows\"].astype(int), npz[\"cols\"].astype(int), npz[\"rels\"].astype(int), npz[\"scores_pred\"].astype(float)\n",
    "\n",
    "def load_sparse(path: str):\n",
    "    assert_exists(path)\n",
    "    return sp.load_npz(path)\n",
    "\n",
    "def load_npy_if_exists(path: str):\n",
    "    return np.load(path) if os.path.exists(path) else None\n",
    "\n",
    "def rmse(pred: np.ndarray, target: np.ndarray):\n",
    "    return float(np.sqrt(np.mean((pred - target) ** 2)))\n",
    "\n",
    "def precision_at_k(y_true_scores: np.ndarray, y_pred_scores: np.ndarray, k: int):\n",
    "    idx = np.argsort(-y_pred_scores)[:k]\n",
    "    return float(np.mean((y_true_scores[idx] > 0).astype(float))), float(np.sum((y_true_scores[idx] > 0))/max(1, np.sum(y_true_scores > 0)))\n",
    "\n",
    "# ---------- evaluation functions ----------\n",
    "def evaluate_edge_rmse(cand_rows, cand_cols, cand_rels, cand_pred_scores, prich_entries, AW=None):\n",
    "    # build mapping for gold targets \n",
    "    gold_map = {}  # (i,j,r) -> score\n",
    "    for e in prich_entries:\n",
    "        key=(int(e[\"i\"]), int(e[\"j\"]), int(e.get(\"r\",0)))\n",
    "        gold_map[key]=float(e.get(\"score\",1.0))\n",
    "    targets = []\n",
    "    preds = []\n",
    "    for i,j,r,s in zip(cand_rows, cand_cols, cand_rels, cand_pred_scores):\n",
    "        key=(int(i),int(j),int(r))\n",
    "        if key in gold_map:\n",
    "            t = gold_map[key]\n",
    "        else:\n",
    "            if AW is not None:\n",
    "                val = AW.getrow(int(i)).tocoo()\n",
    "                col_to_val = dict(zip(val.col.tolist(), val.data.tolist()))\n",
    "                t = float(col_to_val.get(int(j), 0.0))\n",
    "            else:\n",
    "                t = 0.0\n",
    "        preds.append(float(s))\n",
    "        targets.append(float(t))\n",
    "    preds = np.array(preds)\n",
    "    targets = np.array(targets)\n",
    "    return {\"rmse\": rmse(preds,targets), \"num_pairs\": len(preds)}\n",
    "\n",
    "def evaluate_topk_per_relation(cand_rows, cand_cols, cand_rels, cand_pred_scores, prich_entries, K_list=(10,50,100)):\n",
    "    per_rel = {}\n",
    "    gold_map = {}\n",
    "    for e in prich_entries:\n",
    "        key=(int(e[\"i\"]),int(e[\"j\"]),int(e.get(\"r\",0)))\n",
    "        gold_map[key]=float(e.get(\"score\",1.0))\n",
    "    for i,j,r,s in zip(cand_rows,cand_cols,cand_rels,cand_pred_scores):\n",
    "        per_rel.setdefault(int(r), {\"preds\":[], \"golds\":[]})\n",
    "        per_rel[int(r)][\"preds\"].append(float(s))\n",
    "        per_rel[int(r)][\"golds\"].append(float(gold_map.get((int(i),int(j),int(r)), 1.0 if False else 0.0)))\n",
    "    reports={}\n",
    "    for r,vals in per_rel.items():\n",
    "        preds = np.array(vals[\"preds\"]); golds = np.array(vals[\"golds\"])\n",
    "        reports[r]={}\n",
    "        for K in K_list:\n",
    "            p_at_k, recall_at_k = precision_at_k(golds, preds, K)\n",
    "            reports[r][f\"P@{K}\"]=p_at_k\n",
    "            reports[r][f\"Recall@{K}\"]=recall_at_k\n",
    "    return reports\n",
    "\n",
    "def evaluate_auc_pr(cand_rows, cand_cols, cand_rels, cand_pred_scores, prich_entries, pos_threshold=0.5):\n",
    "    gold_map = {}\n",
    "    for e in prich_entries:\n",
    "        key=(int(e[\"i\"]),int(e[\"j\"]),int(e.get(\"r\",0)))\n",
    "        gold_map[key]=float(e.get(\"score\",1.0))\n",
    "    per_rel = {}\n",
    "    for i,j,r,s in zip(cand_rows,cand_cols,cand_rels,cand_pred_scores):\n",
    "        r=int(r)\n",
    "        per_rel.setdefault(r, {\"preds\":[], \"labels\":[]})\n",
    "        per_rel[r][\"preds\"].append(float(s))\n",
    "        per_rel[r][\"labels\"].append(1 if gold_map.get((int(i),int(j),r),0.0) > pos_threshold else 0)\n",
    "    out={}\n",
    "    for r,vals in per_rel.items():\n",
    "        y_true=np.array(vals[\"labels\"]); y_score=np.array(vals[\"preds\"])\n",
    "        if y_true.sum() == 0 or y_true.sum() == len(y_true):\n",
    "            out[r]={\"roc_auc\": None, \"pr_auc\": None}\n",
    "            continue\n",
    "        try:\n",
    "            roc = roc_auc_score(y_true, y_score)\n",
    "            pr = average_precision_score(y_true, y_score)\n",
    "        except Exception:\n",
    "            roc=None; pr=None\n",
    "        out[r]={\"roc_auc\":roc, \"pr_auc\":pr}\n",
    "    return out\n",
    "\n",
    "def evaluate_coarsening_metrics(C_assign_path: str = C_ASSIGN_NPY, community_labels_path: str = COMMUNITY_LABELS_JSON):\n",
    "    C = load_npy_if_exists(C_assign_path)\n",
    "    if C is None:\n",
    "        return {\"available\": False}\n",
    "    # entropy per node\n",
    "    eps=1e-12\n",
    "    probs = np.clip(C, eps, 1.0)\n",
    "    ent = -np.sum(probs * np.log(probs), axis=1)\n",
    "    stats_ent = {\"mean_entropy\": float(ent.mean()), \"median_entropy\": float(np.median(ent))}\n",
    "    if os.path.exists(community_labels_path):\n",
    "        with open(community_labels_path,\"r\") as f:\n",
    "            labels = json.load(f)  \n",
    "        N = C.shape[0]\n",
    "        y_true = np.zeros(N, dtype=int)\n",
    "        for nid,lab in labels.items():\n",
    "            idx = int(nid) if isinstance(nid,str) and nid.isdigit() else int(nid)\n",
    "            y_true[idx]=int(lab)\n",
    "        y_pred = np.argmax(C, axis=1)\n",
    "        return {\"available\": True, \"entropy_stats\": stats_ent, \"nmi\": float(nmi_score(y_true,y_pred)), \"ari\": float(ari_score(y_true,y_pred))}\n",
    "    return {\"available\": True, \"entropy_stats\": stats_ent}\n",
    "\n",
    "def spearman_coarse_vs_fine(acoarse_path: str = ACOARSE_NPY, acoarse_target_path: str = ACOARSE_TARGET_NPY):\n",
    "    Acoarse = load_npy_if_exists(acoarse_path)\n",
    "    Atarget = load_npy_if_exists(acoarse_target_path)\n",
    "    if Acoarse is None or Atarget is None:\n",
    "        return {\"available\": False}\n",
    "    flat_pred = Acoarse.reshape(-1)\n",
    "    flat_target = Atarget.reshape(-1)\n",
    "    rho, p = stats.spearmanr(flat_pred, flat_target)\n",
    "    return {\"available\": True, \"spearman_rho\": float(rho), \"pvalue\": float(p)}\n",
    "\n",
    "def acyclicity_h_matrix(A_mat: np.ndarray):\n",
    "    import scipy.linalg as la\n",
    "    S = A_mat * A_mat\n",
    "    try:\n",
    "        expm = la.expm(S)\n",
    "        return float(np.trace(expm) - A_mat.shape[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def evaluate_acyclicity_for_relations(acoarse_path: str = ACOARSE_NPY):\n",
    "    Acoarse = load_npy_if_exists(acoarse_path)\n",
    "    if Acoarse is None:\n",
    "        return {\"available\": False}\n",
    "    K = Acoarse.shape[0]\n",
    "    R = Acoarse.shape[2]\n",
    "    res = {}\n",
    "    for r in range(R):\n",
    "        h = acyclicity_h_matrix(Acoarse[:,:,r])\n",
    "        res[r]= {\"h\": h}\n",
    "    return {\"available\": True, \"per_relation\": res}\n",
    "\n",
    "def evaluate_embedding_drift(Z_path: str = Z_EMB_NPY, H_final_path: str = H_FINAL_NPY):\n",
    "    Z = load_npy_if_exists(Z_path)\n",
    "    H = load_npy_if_exists(H_final_path)\n",
    "    if Z is None or H is None:\n",
    "        return {\"available\": False}\n",
    "    if Z.shape[0] != H.shape[0]:\n",
    "        raise ValueError(\"Z and H must have same number of nodes\")\n",
    "    # cosine similarities per node\n",
    "    def cos_sim(a,b):\n",
    "        an = a / (np.linalg.norm(a,axis=1,keepdims=True)+1e-12)\n",
    "        bn = b / (np.linalg.norm(b,axis=1,keepdims=True)+1e-12)\n",
    "        return np.sum(an*bn,axis=1)\n",
    "    sims = cos_sim(Z, H)\n",
    "    return {\"available\": True, \"mean_cosine\": float(np.mean(sims)), \"median_cosine\": float(np.median(sims))}\n",
    "\n",
    "def spectral_similarity(A_coarse_path: str = ACOARSE_NPY, A_fine_candidate_npz: str = CAND_PRED_NPZ, top_k=20):\n",
    "    Acoarse = load_npy_if_exists(A_coarse_path)\n",
    "    if Acoarse is None:\n",
    "        return {\"available\": False}\n",
    "    A_sym = (Acoarse.sum(axis=2) + Acoarse.sum(axis=2).T) / 2.0\n",
    "    L = np.diag(A_sym.sum(axis=1)) - A_sym\n",
    "    eigs_coarse = np.linalg.eigvalsh(L)\n",
    "    eigs_coarse_sorted = np.sort(eigs_coarse)[-top_k:]\n",
    "    C = load_npy_if_exists(C_ASSIGN_NPY)\n",
    "    if C is None:\n",
    "        return {\"available\": False, \"reason\":\"need C_assign to aggregate fine graph\"}\n",
    "    rows,cols,rels,scores = load_candidates_npz(A_fine_candidate_npz)\n",
    "    N = C.shape[0]\n",
    "    adj = sp.coo_matrix((scores, (rows,cols)), shape=(N,N)).toarray()\n",
    "    # aggregate to community level: A_agg = C^T * adj * C\n",
    "    A_agg = C.T @ adj @ C\n",
    "    Lf = np.diag(A_agg.sum(axis=1)) - A_agg\n",
    "    eigs_fine = np.linalg.eigvalsh(Lf)\n",
    "    eigs_fine_sorted = np.sort(eigs_fine)[-top_k:]\n",
    "    # compute relative L2 distance\n",
    "    dist = np.linalg.norm(eigs_coarse_sorted - eigs_fine_sorted) / (np.linalg.norm(eigs_fine_sorted) + 1e-12)\n",
    "    return {\"available\": True, \"spectral_rel_l2\": float(dist)}\n",
    "\n",
    "def evaluate_fcompat_separability(cand_npz=CAND_PRED_NPZ, prich_path=PRICH_JSONL, fcompat_scores_npz=None):\n",
    "    if fcompat_scores_npz is None or not os.path.exists(fcompat_scores_npz):\n",
    "        return {\"available\": False}\n",
    "    rows,cols,rels,preds = load_candidates_npz(cand_npz)\n",
    "    fcompat = np.load(fcompat_scores_npz)[\"scores\"]\n",
    "    prich = load_prich(prich_path)\n",
    "    gold_map = {(int(e[\"i\"]),int(e[\"j\"]),int(e.get(\"r\",0))): float(e.get(\"score\",1.0)) for e in prich}\n",
    "    labels = []\n",
    "    for i,j,r in zip(rows,cols,rels):\n",
    "        labels.append(1 if gold_map.get((int(i),int(j),int(r)),0.0) > 0.5 else 0)\n",
    "    labels = np.array(labels)\n",
    "    pos = fcompat[labels==1]\n",
    "    neg = fcompat[labels==0]\n",
    "    if len(pos)==0 or len(neg)==0:\n",
    "        return {\"available\": False, \"reason\": \"no pos or neg\"}\n",
    "    tstat, p = stats.ttest_ind(pos, neg, equal_var=False)\n",
    "    # Cohen's d\n",
    "    d = (pos.mean() - neg.mean()) / (np.sqrt((pos.std()**2 + neg.std()**2)/2) + 1e-12)\n",
    "    return {\"available\": True, \"tstat\":float(tstat), \"p\":float(p), \"cohens_d\":float(d), \"pos_mean\":float(pos.mean()), \"neg_mean\":float(neg.mean())}\n",
    "\n",
    "def compute_motif_counts_sample(adj_matrix: np.ndarray, sample_nodes: List[int], motif=\"triangles\"):\n",
    "    # motif counts in induced subgraph of sample_nodes\n",
    "    g = nx.from_numpy_array(adj_matrix, create_using=nx.DiGraph())\n",
    "    sub = g.subgraph(sample_nodes)\n",
    "    if motif==\"triangles\":\n",
    "        # convert to undirected for triangle counting\n",
    "        u = nx.Graph(sub.to_undirected())\n",
    "        tri = sum(nx.triangles(u).values()) // 3\n",
    "        return {\"triangles\": int(tri)}\n",
    "    elif motif==\"two_paths\":\n",
    "        # count length-2 paths: sum_over_nodes in-degree * out-degree\n",
    "        two = 0\n",
    "        for n in sub.nodes():\n",
    "            two += sub.in_degree(n) * sub.out_degree(n)\n",
    "        return {\"two_paths\": int(two)}\n",
    "    return {}\n",
    "\n",
    "# ---------- main evaluation runner ----------\n",
    "def run_evaluation():\n",
    "    start = time.time()\n",
    "    report = {\"created_at\": time.time(), \"notes\": \"CausGT-HS evaluation suite\"}\n",
    "    assert_exists(CAND_PRED_NPZ)\n",
    "    assert_exists(PRICH_JSONL)\n",
    "    cand_rows, cand_cols, cand_rels, cand_scores = load_candidates_npz(CAND_PRED_NPZ)\n",
    "    prich = load_prich(PRICH_JSONL)\n",
    "    AW_mat = load_sparse(AW_NPZ) if os.path.exists(AW_NPZ) else None\n",
    "\n",
    "    report[\"edge_rmse\"] = evaluate_edge_rmse(cand_rows, cand_cols, cand_rels, cand_scores, prich, AW=AW_mat)\n",
    "    report[\"topk_per_relation\"] = evaluate_topk_per_relation(cand_rows, cand_cols, cand_rels, cand_scores, prich, K_list=(10,50,100))\n",
    "    report[\"auc_pr_per_relation\"] = evaluate_auc_pr(cand_rows, cand_cols, cand_rels, cand_scores, prich, pos_threshold=0.5)\n",
    "    report[\"coarsening\"] = evaluate_coarsening_metrics()\n",
    "    report[\"spearman_coarse_vs_fine\"] = spearman_coarse_vs_fine()\n",
    "    report[\"acyclicity_coarse\"] = evaluate_acyclicity_for_relations()\n",
    "    report[\"embedding_drift\"] = evaluate_embedding_drift()\n",
    "    report[\"spectral_similarity\"] = spectral_similarity()\n",
    "    assembled_adj = None\n",
    "    assembled_path = os.path.join(DATA_DIR,\"assembled_adj.npz\")\n",
    "    if os.path.exists(assembled_path):\n",
    "        assembled_adj = sp.load_npz(assembled_path).toarray()\n",
    "    else:\n",
    "        N = load_npy_if_exists(Z_EMB_NPY)\n",
    "        if N is None:\n",
    "            assembled_adj = None\n",
    "        else:\n",
    "            Nn = N.shape[0]\n",
    "            A = np.zeros((Nn,Nn), dtype=float)\n",
    "            for i,j,_,s in zip(cand_rows,cand_cols,cand_rels,cand_scores):\n",
    "                A[int(i),int(j)] = max(A[int(i),int(j)], float(s))\n",
    "            assembled_adj = A\n",
    "    if assembled_adj is not None:\n",
    "        num_nodes = assembled_adj.shape[0]\n",
    "        samp = list(np.random.choice(num_nodes, min(2000, num_nodes), replace=False))\n",
    "        report[\"motifs_sample\"] = compute_motif_counts_sample(assembled_adj, samp, motif=\"triangles\")\n",
    "        report[\"motifs_sample_two_paths\"] = compute_motif_counts_sample(assembled_adj, samp, motif=\"two_paths\")\n",
    "    else:\n",
    "        report[\"motifs_sample\"] = {\"available\": False}\n",
    "\n",
    "    # Save report\n",
    "    with open(REPORT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Evaluation finished. Report saved to:\", REPORT_OUT, \" time(s):\", round(elapsed,2))\n",
    "    return report\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    r = run_evaluation()\n",
    "    print(json.dumps(r, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9693c58",
   "metadata": {},
   "source": [
    "### 3.4.2.3 The CausGT-HS Encoder ($f_{\\theta}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc172b",
   "metadata": {},
   "source": [
    "Here we fix the computational challenge of scaling it to large graphs...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b6d8a",
   "metadata": {},
   "source": [
    "#### Layer 1: LPA module (Learned Path Aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9baaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- config ----------\n",
    "RNG_SEED = 20251127\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LPA_DIR = Path(\"out/lpa_model\")\n",
    "ENCODER_CKPT = LPA_DIR / \"encoder_finetuned.pth\"\n",
    "AGGREGATOR_CKPT = LPA_DIR / \"aggregator_finetuned.pth\"\n",
    "AW_PATH = Path(\"out/graphs/AW_sparse.json\")\n",
    "NODE_EMB_PATH = LPA_DIR / \"node_embeddings.pt\"\n",
    "H0_PATH = LPA_DIR / \"H0_init.pt\"\n",
    "SAVE_OUT = Path(\"out/lpa_layer1_output.pt\")\n",
    "\n",
    "MAX_DEGREE_CLAMP = 128\n",
    "MAX_MPOOL = 256\n",
    "M_FACETS = 4\n",
    "DEFAULT_NODE_EMB_DIM = 64\n",
    "D_MODEL = 256\n",
    "D_PATH = 256\n",
    "\n",
    "# ---------- sanity ----------\n",
    "if not ENCODER_CKPT.exists():\n",
    "    raise FileNotFoundError(ENCODER_CKPT)\n",
    "if not AGGREGATOR_CKPT.exists():\n",
    "    raise FileNotFoundError(AGGREGATOR_CKPT)\n",
    "if not AW_PATH.exists():\n",
    "    raise FileNotFoundError(AW_PATH)\n",
    "\n",
    "try:\n",
    "    from model.lpa_components import PathTransformerEncoder, AttentionAggregator\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Ensure model/lpa_components.py defines PathTransformerEncoder and AttentionAggregator\") from e\n",
    "\n",
    "encoder = PathTransformerEncoder(\n",
    "    node_emb_dim=None, d_model=D_MODEL, nhead=8, num_layers=4, dim_feedforward=512, d_path=D_PATH, max_len=32\n",
    ")\n",
    "aggregator = AttentionAggregator(d_path=D_PATH, path_feat_dim=8, hidden_dim=256)\n",
    "\n",
    "enc_state = torch.load(ENCODER_CKPT, map_location=\"cpu\")\n",
    "agg_state = torch.load(AGGREGATOR_CKPT, map_location=\"cpu\")\n",
    "\n",
    "if \"encoder_state\" in enc_state:\n",
    "    encoder.load_state_dict(enc_state[\"encoder_state\"], strict=True)\n",
    "else:\n",
    "    encoder.load_state_dict(enc_state, strict=True)\n",
    "\n",
    "if \"aggregator_state\" in agg_state:\n",
    "    aggregator.load_state_dict(agg_state[\"aggregator_state\"], strict=True)\n",
    "else:\n",
    "    aggregator.load_state_dict(agg_state, strict=True)\n",
    "\n",
    "encoder.to(DEVICE).eval()\n",
    "aggregator.to(DEVICE).eval()\n",
    "s_value_transform = aggregator.head.to(DEVICE).eval()\n",
    "\n",
    "# ---------- utils ----------\n",
    "def load_sparse_adj(path: Path) -> torch.sparse_coo_tensor:\n",
    "    obj = json.loads(path.read_text())\n",
    "    rows = torch.tensor(obj[\"rows\"], dtype=torch.long)\n",
    "    cols = torch.tensor(obj[\"cols\"], dtype=torch.long)\n",
    "    vals = torch.tensor(obj.get(\"vals\", [1.0] * len(rows)), dtype=torch.float32)\n",
    "    N = int(obj.get(\"N\", int(max(rows.max().item(), cols.max().item()) + 1)))\n",
    "    idx = torch.stack([rows, cols], dim=0)\n",
    "    return torch.sparse_coo_tensor(indices=idx, values=vals, size=(N, N)).coalesce()\n",
    "\n",
    "def adjacency_to_index_tensor(A_sparse: torch.sparse_coo_tensor, max_deg: int = MAX_DEGREE_CLAMP) -> torch.Tensor:\n",
    "    N = A_sparse.size(0)\n",
    "    rows = A_sparse.indices()[0].cpu().tolist()\n",
    "    cols = A_sparse.indices()[1].cpu().tolist()\n",
    "    neighbors: List[List[int]] = [[] for _ in range(N)]\n",
    "    for r, c in zip(rows, cols):\n",
    "        neighbors[r].append(c)\n",
    "    out = torch.full((N, max_deg), -1, dtype=torch.long)\n",
    "    for i in range(N):\n",
    "        nb = neighbors[i][:max_deg]\n",
    "        if nb:\n",
    "            out[i, :len(nb)] = torch.tensor(nb, dtype=torch.long)\n",
    "    return out.to(DEVICE)\n",
    "\n",
    "def enumerate_paths_vectorized(neigh_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    N, H = neigh_idx.shape\n",
    "    P1 = neigh_idx.clone()\n",
    "    safe = neigh_idx.clone()\n",
    "    safe[safe == -1] = 0\n",
    "    second_hop = neigh_idx[safe]            # [N, H, H]\n",
    "    mask_invalid = (neigh_idx == -1).unsqueeze(-1)\n",
    "    second_hop = torch.where(mask_invalid, torch.full_like(second_hop, -1), second_hop)\n",
    "    return P1, second_hop\n",
    "\n",
    "def build_path_embedding_tensors(P1: torch.Tensor, P2: torch.Tensor, node_embeddings: torch.Tensor,\n",
    "                                 max_mpool: int = MAX_MPOOL) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    N, H = P1.shape\n",
    "    L = 3\n",
    "    D = node_embeddings.size(1)\n",
    "    device = node_embeddings.device\n",
    "    paths_idx = torch.full((N, max_mpool, L), -1, dtype=torch.long, device=device)\n",
    "    for i in range(N):\n",
    "        cnt = 0\n",
    "        for a_j in range(H):\n",
    "            a = P1[i, a_j].item()\n",
    "            if a == -1:\n",
    "                continue\n",
    "            for b_j in range(H):\n",
    "                b = P2[i, a_j, b_j].item()\n",
    "                if b == -1:\n",
    "                    continue\n",
    "                if cnt >= max_mpool:\n",
    "                    break\n",
    "                paths_idx[i, cnt, 0] = i\n",
    "                paths_idx[i, cnt, 1] = a\n",
    "                paths_idx[i, cnt, 2] = b\n",
    "                cnt += 1\n",
    "            if cnt >= max_mpool:\n",
    "                break\n",
    "    pad_emb = torch.zeros((1, D), device=device)\n",
    "    emb_shift = torch.cat([pad_emb, node_embeddings.to(device)], dim=0)\n",
    "    safe_paths_idx = paths_idx.clone()\n",
    "    mask_neg = (safe_paths_idx == -1)\n",
    "    safe_paths_idx = safe_paths_idx + 1\n",
    "    safe_paths_idx[mask_neg] = 0\n",
    "    paths_emb = emb_shift[safe_paths_idx]   # [N, mpool, L, D]\n",
    "    return paths_emb.to(device), paths_emb.to(device)\n",
    "\n",
    "# ---------- path encoder / aggregators ----------\n",
    "class LPALayer1_PathEncoder:\n",
    "    def __init__(self, transformer_path_model, device):\n",
    "        self.tp = transformer_path_model.to(device)\n",
    "        self.tp.eval()\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, paths_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        N, M, L, D = paths_tensor.shape\n",
    "        flat = paths_tensor.reshape(N * M, L, D).to(self.device)\n",
    "        emb = self.tp(flat)\n",
    "        return emb.reshape(N, M, -1)\n",
    "\n",
    "class FacetAwareAggregator(nn.Module):\n",
    "    def __init__(self, d_path, d_model, M, hidden_dim=256, s_value_transform_module=None):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.W_h = nn.Linear(d_path, hidden_dim, bias=False)\n",
    "        self.U = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.w = nn.Parameter(torch.randn(hidden_dim))\n",
    "        if s_value_transform_module is None:\n",
    "            raise ValueError(\"Provide s_value_transform_module\")\n",
    "        self.MLP_s = s_value_transform_module\n",
    "        self.d_path = d_path\n",
    "\n",
    "    def forward(self, H0, path_embs):\n",
    "        N, M, dm = H0.shape\n",
    "        mpool = path_embs.shape[1]\n",
    "        H0_exp = H0.unsqueeze(2).expand(N, M, mpool, dm)\n",
    "        path_exp = path_embs.unsqueeze(1).expand(N, M, mpool, self.d_path)\n",
    "        Wh_hp = self.W_h(path_exp)\n",
    "        U_sk = self.U(H0_exp)\n",
    "        z = torch.tanh(Wh_hp + U_sk + self.b)\n",
    "        att_logits = torch.einsum(\"nmkd,d->nmk\", z, self.w)\n",
    "        att = F.softmax(att_logits, dim=2)\n",
    "        svals = self.MLP_s(path_exp.reshape(N * M * mpool, self.d_path))\n",
    "        svals = svals.reshape(N, M, mpool, -1)\n",
    "        ctx = torch.sum(att.unsqueeze(-1) * svals, dim=2)\n",
    "        return ctx\n",
    "\n",
    "class LPALayer1_DualStreamAggregator(nn.Module):\n",
    "    def __init__(self, d_path, d_model, M, s_value_transform_module):\n",
    "        super().__init__()\n",
    "        self.corr = FacetAwareAggregator(d_path, d_model, M, s_value_transform_module=s_value_transform_module)\n",
    "        self.causal = FacetAwareAggregator(d_path, d_model, M, s_value_transform_module=s_value_transform_module)\n",
    "\n",
    "    def forward(self, H0, path_corr, path_causal):\n",
    "        return self.corr(H0, path_corr), self.causal(H0, path_causal)\n",
    "\n",
    "class LPALayer1_Primer(nn.Module):\n",
    "    def __init__(self, d_s, d_model):\n",
    "        super().__init__()\n",
    "        self.W_proj = nn.Linear(2 * d_s, d_model)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, H0, c_corr, c_causal):\n",
    "        master = torch.cat([c_corr, c_causal], dim=-1)\n",
    "        proj = self.W_proj(master)\n",
    "        return self.ln(H0 + proj)\n",
    "\n",
    "class CausGT_LPA_Layer1(nn.Module):\n",
    "    def __init__(self, transformer_path, s_value_transform_module, d_path, d_model, M, device):\n",
    "        super().__init__()\n",
    "        self.path_encoder = LPALayer1_PathEncoder(transformer_path, device)\n",
    "        self.agg = LPALayer1_DualStreamAggregator(d_path, d_model, M, s_value_transform_module)\n",
    "        self.primer = LPALayer1_Primer(d_s=d_path, d_model=d_model)\n",
    "\n",
    "    def forward(self, H0, paths_corr, paths_causal):\n",
    "        pe_corr = self.path_encoder.encode(paths_corr)\n",
    "        pe_causal = self.path_encoder.encode(paths_causal)\n",
    "        c_corr, c_causal = self.agg(H0, pe_corr, pe_causal)\n",
    "        return self.primer(H0, c_corr, c_causal)\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    AW = load_sparse_adj(AW_PATH).to(DEVICE)\n",
    "    AW = AW.coalesce()\n",
    "    N = AW.size(0)\n",
    "\n",
    "    neigh_idx = adjacency_to_index_tensor(AW, max_deg=MAX_DEGREE_CLAMP)\n",
    "    P1, P2 = enumerate_paths_vectorized(neigh_idx)\n",
    "\n",
    "    expected_node_dim = DEFAULT_NODE_EMB_DIM\n",
    "    for attr in (\"node_emb_dim\", \"input_dim\", \"d_token\", \"token_dim\"):\n",
    "        if hasattr(encoder, attr) and getattr(encoder, attr) is not None:\n",
    "            expected_node_dim = int(getattr(encoder, attr))\n",
    "            break\n",
    "    if expected_node_dim == DEFAULT_NODE_EMB_DIM:\n",
    "        for name, p in encoder.named_parameters():\n",
    "            if p.dim() >= 2:\n",
    "                expected_node_dim = p.size(1)\n",
    "                break\n",
    "\n",
    "    if NODE_EMB_PATH.exists():\n",
    "        try:\n",
    "            node_emb_obj = torch.load(NODE_EMB_PATH, map_location=DEVICE)\n",
    "            if isinstance(node_emb_obj, dict) and \"node_embeddings\" in node_emb_obj:\n",
    "                node_embeddings = node_emb_obj[\"node_embeddings\"].to(DEVICE)\n",
    "            elif isinstance(node_emb_obj, torch.Tensor):\n",
    "                node_embeddings = node_emb_obj.to(DEVICE)\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown node embeddings format\")\n",
    "            if node_embeddings.size(0) != N:\n",
    "                node_embeddings = torch.randn(N, expected_node_dim, device=DEVICE) * 0.01\n",
    "        except Exception:\n",
    "            node_embeddings = torch.randn(N, expected_node_dim, device=DEVICE) * 0.01\n",
    "    else:\n",
    "        node_embeddings = torch.randn(N, expected_node_dim, device=DEVICE) * 0.01\n",
    "\n",
    "    paths_corr_tensor, paths_causal_tensor = build_path_embedding_tensors(P1, P2, node_embeddings, max_mpool=MAX_MPOOL)\n",
    "\n",
    "    M = M_FACETS\n",
    "    if H0_PATH.exists():\n",
    "        try:\n",
    "            h0_obj = torch.load(H0_PATH, map_location=DEVICE)\n",
    "            if isinstance(h0_obj, dict) and \"H0\" in h0_obj:\n",
    "                H0 = h0_obj[\"H0\"].to(DEVICE)\n",
    "            elif isinstance(h0_obj, torch.Tensor):\n",
    "                H0 = h0_obj.to(DEVICE)\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown H0 format\")\n",
    "            if H0.size(0) != N or H0.size(1) != M or H0.size(2) != D_MODEL:\n",
    "                H0 = torch.randn(N, M, D_MODEL, device=DEVICE) * 0.01\n",
    "        except Exception:\n",
    "            H0 = torch.randn(N, M, D_MODEL, device=DEVICE) * 0.01\n",
    "    else:\n",
    "        H0 = torch.randn(N, M, D_MODEL, device=DEVICE) * 0.01\n",
    "\n",
    "    layer1 = CausGT_LPA_Layer1(\n",
    "        transformer_path=encoder,\n",
    "        s_value_transform_module=s_value_transform,\n",
    "        d_path=D_PATH,\n",
    "        d_model=D_MODEL,\n",
    "        M=M,\n",
    "        device=DEVICE\n",
    "    ).to(DEVICE)\n",
    "    layer1.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        H1 = layer1(H0=H0, paths_corr=paths_corr_tensor, paths_causal=paths_causal_tensor)\n",
    "        SAVE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save({\"H1\": H1.detach().cpu()}, SAVE_OUT)\n",
    "        print(f\"Saved H1 to {SAVE_OUT} (shape {H1.shape})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c8bf8",
   "metadata": {},
   "source": [
    "#### Layer 2...L: The Sparse Dual-Stream Causal-MoE-Tokenphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "RNG_SEED = 20251127\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LPA_DIR = Path(\"out/lpa_model\")\n",
    "ENCODER_CKPT = LPA_DIR / \"encoder_finetuned.pth\"\n",
    "AGGREGATOR_CKPT = LPA_DIR / \"aggregator_finetuned.pth\"\n",
    "AW_PATH = Path(\"out/graphs/AW_sparse.json\")\n",
    "NODE_EMB_PATH = LPA_DIR / \"node_embeddings.pt\"\n",
    "H0_PATH = LPA_DIR / \"H0_init.pt\"\n",
    "SAVE_OUT = Path(\"out/lpa_layer1_output.pt\")   # contains {\"H1\": tensor}\n",
    "SAVE_LAYER2 = Path(\"out/lpa_layer2_checkpoint.pth\")\n",
    "\n",
    "MAX_DEGREE_CLAMP = 128\n",
    "MAX_MPOOL = 256\n",
    "M_FACETS = 4\n",
    "DEFAULT_NODE_EMB_DIM = 64\n",
    "D_MODEL = 256\n",
    "D_PATH = 256\n",
    "\n",
    "# ---------- sanity ----------\n",
    "if not SAVE_OUT.exists():\n",
    "    raise FileNotFoundError(f\"Layer1 output not found: {SAVE_OUT}\")\n",
    "if not AW_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Adjacency file not found: {AW_PATH}\")\n",
    "\n",
    "# ---------- utilities from Layer1 ----------\n",
    "def load_sparse_adj(path: Path) -> torch.sparse_coo_tensor:\n",
    "    obj = json.loads(path.read_text())\n",
    "    rows = torch.tensor(obj[\"rows\"], dtype=torch.long)\n",
    "    cols = torch.tensor(obj[\"cols\"], dtype=torch.long)\n",
    "    vals = torch.tensor(obj.get(\"vals\", [1.0] * len(rows)), dtype=torch.float32)\n",
    "    N = int(obj.get(\"N\", int(max(rows.max().item(), cols.max().item()) + 1)))\n",
    "    idx = torch.stack([rows, cols], dim=0)\n",
    "    return torch.sparse_coo_tensor(indices=idx, values=vals, size=(N, N)).coalesce()\n",
    "\n",
    "def adjacency_to_index_tensor(A_sparse: torch.sparse_coo_tensor, max_deg: int = MAX_DEGREE_CLAMP) -> torch.Tensor:\n",
    "    N = A_sparse.size(0)\n",
    "    rows = A_sparse.indices()[0].cpu().tolist()\n",
    "    cols = A_sparse.indices()[1].cpu().tolist()\n",
    "    neighbors = [[] for _ in range(N)]\n",
    "    for r, c in zip(rows, cols):\n",
    "        neighbors[r].append(c)\n",
    "    out = torch.full((N, max_deg), -1, dtype=torch.long)\n",
    "    for i in range(N):\n",
    "        nb = neighbors[i][:max_deg]\n",
    "        if nb:\n",
    "            out[i, :len(nb)] = torch.tensor(nb, dtype=torch.long)\n",
    "    return out.to(DEVICE)\n",
    "\n",
    "# ---------- Model components ----------\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MHSAWrapper(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "    def forward(self, seq, attn_mask:Optional[torch.Tensor]=None):\n",
    "        attn_out, _ = self.mha(seq, seq, seq, attn_mask=attn_mask, need_weights=False)\n",
    "        return self.ln(seq + attn_out)\n",
    "\n",
    "class ExpertModule(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, aux_hidden:int=128, aux_out:int=1, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAWrapper(d_model, nhead, dropout=dropout)\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(d_model, aux_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(aux_hidden, aux_out)\n",
    "        )\n",
    "    def forward(self, seq):\n",
    "        seq_out = self.mhsa(seq)\n",
    "        cls = seq_out[:, 0, :]\n",
    "        aux = self.aux_head(cls)\n",
    "        return seq_out, aux\n",
    "\n",
    "def topk_sparse_weights(logits: torch.Tensor, k: int, temp: float):\n",
    "    probs = F.softmax(logits / temp, dim=-1)\n",
    "    topk_vals, topk_idx = torch.topk(probs, k=k, dim=-1)\n",
    "    B, E = probs.shape\n",
    "    device = logits.device\n",
    "    weights = torch.zeros_like(probs)\n",
    "    arange_b = torch.arange(B, device=device).unsqueeze(1).expand(-1, k)\n",
    "    weights[arange_b.reshape(-1), topk_idx.reshape(-1)] = topk_vals.reshape(-1)\n",
    "    return weights, topk_idx, probs\n",
    "\n",
    "class SparseCausalMoE(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, n_experts:int=8, top_k:int=2,\n",
    "                 router_hidden:int=128, temp:float=1.0, dropout:float=0.1, aux_out:int=1):\n",
    "        super().__init__()\n",
    "        self.E = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.temp = temp\n",
    "        self.router = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, router_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(router_hidden, n_experts),\n",
    "        )\n",
    "        self.experts = nn.ModuleList([ExpertModule(d_model, nhead, aux_hidden=128, aux_out=aux_out, dropout=dropout)\n",
    "                                      for _ in range(n_experts)])\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "    def forward(self, seq, self_token_vec):\n",
    "        pooled = seq.mean(dim=1)\n",
    "        router_in = torch.cat([self_token_vec, pooled], dim=-1)\n",
    "        logits = self.router(router_in)\n",
    "        weights, topk_idx, probs = topk_sparse_weights(logits, k=self.top_k, temp=self.temp)\n",
    "        expert_outputs = []\n",
    "        aux_logits = []\n",
    "        # run all experts\n",
    "        for e in range(self.E):\n",
    "            out_e, aux_e = self.experts[e](seq)\n",
    "            expert_outputs.append(out_e)\n",
    "            aux_logits.append(aux_e)\n",
    "        expert_stack = torch.stack(expert_outputs, dim=0).permute(1,0,2,3)  # [B, E, S, d]\n",
    "        w = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "        aggregated = (w * expert_stack).sum(dim=1)\n",
    "        aggregated = self.layernorm(aggregated)\n",
    "        aux_stack = torch.stack(aux_logits, dim=0).permute(1,0,2)  # [B, E, aux_out]\n",
    "        return aggregated, aux_stack, logits, probs\n",
    "\n",
    "class CorrelationalStream(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAWrapper(d_model, nhead, dropout=dropout)\n",
    "    def forward(self, seq, attn_mask:Optional[torch.Tensor]=None):\n",
    "        return self.mhsa(seq, attn_mask=attn_mask)\n",
    "\n",
    "class TokenphormerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model:int=256,\n",
    "                 nhead:int=8,\n",
    "                 n_experts:int=8,\n",
    "                 top_k_experts:int=2,\n",
    "                 k_neighbors:int=16,\n",
    "                 ff_hidden:int=1024,\n",
    "                 dropout:float=0.1,\n",
    "                 router_temp:float=1.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.correl_stream = CorrelationalStream(d_model, nhead, dropout=dropout)\n",
    "        self.causal_moe = SparseCausalMoE(d_model, nhead, n_experts, top_k_experts, router_hidden=256,\n",
    "                                          temp=router_temp, dropout=dropout, aux_out=1)\n",
    "        self.gate_proj = nn.Linear(d_model, 1)\n",
    "        self.ffn = SimpleFFN(d_model, ff_hidden, dropout=dropout)\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "\n",
    "    def build_hybrid_sequence(self,\n",
    "                              H_prev_flat: torch.Tensor,\n",
    "                              H_all: torch.Tensor,\n",
    "                              neighbor_idx: torch.LongTensor,\n",
    "                              H_coarse: Optional[torch.Tensor] = None,\n",
    "                              text_tokens: Optional[torch.Tensor] = None):\n",
    "        device = H_prev_flat.device\n",
    "        B = H_prev_flat.size(0)\n",
    "        d = self.d_model\n",
    "        self_token = H_prev_flat.unsqueeze(1)\n",
    "        N_nodes, M_facets, _ = H_all.shape\n",
    "        M = M_facets\n",
    "        assert B == N_nodes * M, \"shape mismatch\"\n",
    "        node_ids = torch.arange(B, device=device) // M\n",
    "        if H_coarse is not None:\n",
    "            hcontext = H_coarse.mean(dim=0, keepdim=True).expand(B, -1).unsqueeze(1)\n",
    "        else:\n",
    "            hcontext = torch.zeros(B, 1, d, device=device)\n",
    "        k_total = neighbor_idx.size(1)\n",
    "        cand_nodes = neighbor_idx[node_ids]  # [B, k_total]\n",
    "        cand_nodes_exp = cand_nodes.unsqueeze(-1).expand(-1, -1, M)\n",
    "        H_all_expand = H_all[cand_nodes_exp]  # [B, k_total, M, d]\n",
    "        neighbor_facets = H_all_expand.reshape(B, k_total * M, d)\n",
    "        attn_scores = (self_token @ neighbor_facets.transpose(1,2)).squeeze(1)\n",
    "        Ksel = min(self.k_neighbors, attn_scores.size(1))\n",
    "        topk_vals, topk_idx = torch.topk(attn_scores, k=Ksel, dim=-1)\n",
    "        arange_b = torch.arange(B, device=device).unsqueeze(1).expand(-1, Ksel)\n",
    "        selected_neighbors = neighbor_facets[arange_b.reshape(-1), topk_idx.reshape(-1)].reshape(B, Ksel, d)\n",
    "        if text_tokens is not None:\n",
    "            text_tokens_exp = text_tokens[node_ids]\n",
    "        else:\n",
    "            text_tokens_exp = torch.zeros(B, 0, d, device=device)\n",
    "        seq = torch.cat([self_token, hcontext, selected_neighbors, text_tokens_exp], dim=1)\n",
    "        return seq\n",
    "\n",
    "    def forward(self,\n",
    "                H_prev: torch.Tensor,\n",
    "                neighbor_idx: torch.LongTensor,\n",
    "                H_coarse: Optional[torch.Tensor] = None,\n",
    "                text_tokens: Optional[torch.Tensor] = None):\n",
    "        N, M, d = H_prev.shape\n",
    "        device = H_prev.device\n",
    "        B = N * M\n",
    "        H_prev_flat = H_prev.reshape(B, d)\n",
    "        seq = self.build_hybrid_sequence(H_prev_flat, H_prev, neighbor_idx, H_coarse, text_tokens)\n",
    "        corr_out = self.correl_stream(seq)\n",
    "        self_token_vec = seq[:, 0, :]\n",
    "        causal_out, aux_logits, router_logits, router_probs = self.causal_moe(seq, self_token_vec)\n",
    "        h_corr = corr_out[:, 0, :]\n",
    "        h_causal = causal_out[:, 0, :]\n",
    "        lam = torch.sigmoid(self.gate_proj(H_prev_flat)).squeeze(-1)\n",
    "        hfused = lam.unsqueeze(-1) * h_corr + (1.0 - lam).unsqueeze(-1) * h_causal\n",
    "        hffn = self.ffn(hfused)\n",
    "        hnext_flat = self.ln_final(hfused + hffn)\n",
    "        H_next = hnext_flat.reshape(N, M, d)\n",
    "        return H_next, router_logits, router_probs, aux_logits\n",
    "\n",
    "# ---------- training utilities & losses ----------\n",
    "def load_layer1_H1(path: Path) -> torch.Tensor:\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict) and \"H1\" in obj:\n",
    "        H1 = obj[\"H1\"]\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        H1 = obj\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown H1 format in SAVE_OUT\")\n",
    "    return H1\n",
    "\n",
    "def kl_to_uniform_mean(probs: torch.Tensor):\n",
    "    eps = 1e-9\n",
    "    mean_p = probs.mean(dim=0)  # [E]\n",
    "    E = mean_p.size(0)\n",
    "    kl = (mean_p * (torch.log(mean_p + eps) - math.log(1.0 / E))).sum()\n",
    "    return kl\n",
    "\n",
    "def train_loop(layer: TokenphormerLayer,\n",
    "               H_init: torch.Tensor,\n",
    "               neighbor_idx: torch.LongTensor,\n",
    "               epochs: int = 20,\n",
    "               lr: float = 3e-4,\n",
    "               weight_decay: float = 1e-6,\n",
    "               save_path: Path = SAVE_LAYER2):\n",
    "    layer.train()\n",
    "    layer.to(DEVICE)\n",
    "    H = H_init.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(layer.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "    N, M, d = H.shape\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            H_next, router_logits, router_probs, aux_logits = layer(H, neighbor_idx)\n",
    "            # Self-supervised reconstruction loss: encouraged stability (H_next close to H)\n",
    "            recon_loss = F.mse_loss(H_next, H)\n",
    "            # router load-balance loss: KL(mean_probs || uniform)\n",
    "            lb_loss = kl_to_uniform_mean(router_probs)\n",
    "            # aux regularization (small L2 on aux logits)\n",
    "            aux_reg = aux_logits.pow(2).mean()\n",
    "            total_loss = recon_loss + 0.1 * lb_loss + 1e-3 * aux_reg\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(layer.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        H = H_next.detach()  \n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | total_loss {total_loss.item():.6f} recon {recon_loss.item():.6f} lb {lb_loss.item():.6f} aux {aux_reg.item():.6e}\")\n",
    "        # save checkpoint each epoch\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": layer.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"H_curr\": H.cpu(),\n",
    "        }\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(ckpt, save_path)\n",
    "    return layer, H\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    AW = load_sparse_adj(AW_PATH).to(DEVICE).coalesce()\n",
    "    neigh_idx = adjacency_to_index_tensor(AW, max_deg=MAX_DEGREE_CLAMP)  # [N, max_deg]\n",
    "\n",
    "    H1 = load_layer1_H1(SAVE_OUT)  # assumed [N, M, D_MODEL]\n",
    "    if H1.dim() != 3:\n",
    "        raise RuntimeError(\"Expected H1 of shape [N, M, D_MODEL]\")\n",
    "    N, M, D = H1.shape\n",
    "    if D != D_MODEL:\n",
    "        proj = nn.Linear(D, D_MODEL).to(DEVICE)\n",
    "        H1 = proj(H1.to(DEVICE)).detach().cpu()\n",
    "\n",
    "    desired_k_neighbors = min(32, neigh_idx.size(1))\n",
    "    neigh_idx = neigh_idx[:, :desired_k_neighbors].contiguous()\n",
    "\n",
    "    layer2 = TokenphormerLayer(d_model=D_MODEL,\n",
    "                               nhead=8,\n",
    "                               n_experts=8,\n",
    "                               top_k_experts=2,\n",
    "                               k_neighbors=min(16, desired_k_neighbors),\n",
    "                               ff_hidden=1024,\n",
    "                               dropout=0.1,\n",
    "                               router_temp=1.0)\n",
    "\n",
    "    neighbor_idx = neigh_idx.to(DEVICE)\n",
    "\n",
    "    # train\n",
    "    epochs = 20\n",
    "    trained_layer, H_final = train_loop(layer2, H1, neighbor_idx, epochs=epochs, lr=3e-4)\n",
    "\n",
    "    # save final H and model\n",
    "    out_dir = Path(\"out\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\"H2\": H_final.detach().cpu()}, out_dir / \"lpa_layer2_output.pt\")\n",
    "    torch.save({\"model_state\": trained_layer.state_dict()}, out_dir / \"lpa_layer2_model.pth\")\n",
    "    print(\"Saved layer2 outputs and model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ad84d",
   "metadata": {},
   "source": [
    "### 3.4.2.4 The Amortized Proposer Network ($Q_{\\phi}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch_energy.samplers import LangevinSampler\n",
    "\n",
    "# ---------- config ----------\n",
    "RNG_SEED = 20251127\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "OUT_DIR = Path(\"out\")\n",
    "LAYER2_OUT = OUT_DIR / \"lpa_layer2_output.pt\"\n",
    "LAYER1_OUT = OUT_DIR / \"lpa_layer1_output.pt\"\n",
    "SAVE_PROPOSER = OUT_DIR / \"proposer_qphi_checkpoint.pth\"\n",
    "\n",
    "# hyperparams\n",
    "R = 6\n",
    "d_rank = 16\n",
    "d_q = 64\n",
    "d_k = 64\n",
    "d_e = 32\n",
    "film_hidden = 128\n",
    "alpha_energy = 0.1\n",
    "alpha_distill = 1.0\n",
    "alpha_sparse = 1e-2\n",
    "sparse_beta = 1.0\n",
    "proposer_lr = 3e-4\n",
    "ebm_lr = 3e-4\n",
    "epochs = 30\n",
    "mcmc_steps = 10\n",
    "mcmc_step_size = 0.1\n",
    "mcmc_noise = 1e-2\n",
    "ema_tau = 0.995\n",
    "use_rel_loop = False  # set True if memory tight\n",
    "\n",
    "# ---------- loaders ----------\n",
    "def load_HL() -> torch.Tensor:\n",
    "    if LAYER2_OUT.exists():\n",
    "        obj = torch.load(LAYER2_OUT, map_location=\"cpu\")\n",
    "        k = \"H2\" if \"H2\" in obj else next(iter(obj.keys()))\n",
    "        H = obj[k]\n",
    "    elif LAYER1_OUT.exists():\n",
    "        obj = torch.load(LAYER1_OUT, map_location=\"cpu\")\n",
    "        k = \"H1\" if \"H1\" in obj else next(iter(obj.keys()))\n",
    "        H = obj[k]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No Layer outputs found in out/\")\n",
    "    if not isinstance(H, torch.Tensor):\n",
    "        raise RuntimeError(\"Loaded H is not a tensor\")\n",
    "    return H.to(DEVICE)\n",
    "\n",
    "# ---------- proposer modules ----------\n",
    "class FiLMGenerator(nn.Module):\n",
    "    def __init__(self, e_dim:int, hidden_dim:int, out_dim:int):\n",
    "        super().__init__()\n",
    "        self.gam = nn.Sequential(nn.Linear(e_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.bet = nn.Sequential(nn.Linear(e_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "    def forward(self, e):\n",
    "        return self.gam(e), self.bet(e)\n",
    "\n",
    "class SharedG(nn.Module):\n",
    "    def __init__(self, x_dim:int, hidden_dim:int, out_dim:int):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(x_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.W2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.b1 = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.b2 = nn.Parameter(torch.zeros(out_dim))\n",
    "    def forward(self, x, gamma, beta):\n",
    "        h = self.W1(x) + self.b1\n",
    "        h = self.act(h)\n",
    "        h = h * gamma + beta\n",
    "        return self.W2(h) + self.b2\n",
    "\n",
    "class ProposerNet(nn.Module):\n",
    "    def __init__(self, R:int, d_out:int, d_rank:int, d_q:int, d_k:int, d_e:int, film_hidden:int,\n",
    "                 use_rel_loop:bool=False):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.d_out = d_out\n",
    "        self.d_rank = d_rank\n",
    "        self.d_q = d_q\n",
    "        self.d_k = d_k\n",
    "        self.d_e = d_e\n",
    "        self.use_rel_loop = use_rel_loop\n",
    "\n",
    "        self.q_rel = nn.Parameter(torch.randn(R, d_q) * 0.02)\n",
    "        self.WQ = nn.Linear(d_q, d_k, bias=False)\n",
    "        self.WK = nn.Linear(d_out, d_k, bias=False)\n",
    "\n",
    "        self.eU = nn.Parameter(torch.randn(R, d_e) * 0.02)\n",
    "        self.eV = nn.Parameter(torch.randn(R, d_e) * 0.02)\n",
    "\n",
    "        self.film_U = FiLMGenerator(d_e, film_hidden, film_hidden)\n",
    "        self.film_V = FiLMGenerator(d_e, film_hidden, film_hidden)\n",
    "        self.g_shared = SharedG(x_dim=d_out, hidden_dim=film_hidden, out_dim=d_rank)\n",
    "\n",
    "        self.b_rel = nn.Parameter(torch.zeros(R))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ.weight)\n",
    "        nn.init.xavier_uniform_(self.WK.weight)\n",
    "        nn.init.xavier_uniform_(self.g_shared.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.g_shared.W2.weight)\n",
    "\n",
    "    def relation_pooling(self, H: torch.Tensor) -> torch.Tensor:\n",
    "        N, M, d = H.shape\n",
    "        K = self.WK(H.view(N*M, d)).view(N, M, self.d_k)\n",
    "        Q = self.WQ(self.q_rel)  # [R, d_k]\n",
    "        Q_exp = Q.unsqueeze(1).unsqueeze(2)      # [R,1,1,d_k]\n",
    "        K_exp = K.unsqueeze(0)                   # [1,N,M,d_k]\n",
    "        att_logits = (Q_exp * K_exp).sum(dim=-1) / math.sqrt(self.d_k)  # [R,N,M]\n",
    "        att = F.softmax(att_logits, dim=-1)\n",
    "        pooled = (att.unsqueeze(-1) * H.unsqueeze(0)).sum(dim=2)  # [R,N,d_out]\n",
    "        return pooled\n",
    "\n",
    "    def hyper_generate_UV(self, pooled: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        R, N, d = pooled.shape\n",
    "        device = pooled.device\n",
    "        if self.use_rel_loop:\n",
    "            U = torch.zeros(R, N, self.d_rank, device=device)\n",
    "            V = torch.zeros(R, N, self.d_rank, device=device)\n",
    "            for r in range(R):\n",
    "                h_r = pooled[r]\n",
    "                gamma_u, beta_u = self.film_U(self.eU[r:r+1].expand(N, -1))\n",
    "                gamma_v, beta_v = self.film_V(self.eV[r:r+1].expand(N, -1))\n",
    "                U[r] = self.g_shared(h_r, gamma_u, beta_u)\n",
    "                V[r] = self.g_shared(h_r, gamma_v, beta_v)\n",
    "            return U, V\n",
    "        pooled_flat = pooled.reshape(R*N, d)\n",
    "        eU_exp = self.eU.unsqueeze(1).expand(-1, N, -1).reshape(R*N, self.d_e)\n",
    "        eV_exp = self.eV.unsqueeze(1).expand(-1, N, -1).reshape(R*N, self.d_e)\n",
    "        gamma_u, beta_u = self.film_U(eU_exp)\n",
    "        gamma_v, beta_v = self.film_V(eV_exp)\n",
    "        outU = self.g_shared(pooled_flat, gamma_u, beta_u)\n",
    "        outV = self.g_shared(pooled_flat, gamma_v, beta_v)\n",
    "        U = outU.view(R, N, self.d_rank)\n",
    "        V = outV.view(R, N, self.d_rank)\n",
    "        return U, V\n",
    "\n",
    "    def forward(self, H: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        pooled = self.relation_pooling(H)  # [R,N,d_out]\n",
    "        U, V = self.hyper_generate_UV(pooled)  # [R,N,d_rank]\n",
    "        R_, N, dr = U.shape\n",
    "        A_logits = torch.zeros(R_, N, N, device=H.device)\n",
    "        for r in range(R_):\n",
    "            Ur = U[r]\n",
    "            Vr = V[r]\n",
    "            A_logits[r] = Ur @ Vr.t() + self.b_rel[r]\n",
    "        A_probs = torch.sigmoid(A_logits)\n",
    "        return U, V, A_logits, A_probs\n",
    "\n",
    "# ---------- simple EBM ----------\n",
    "class EnergyModel(nn.Module):\n",
    "    def __init__(self, R:int, d_in:int, d_hidden:int=256, d_latent:int=64):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, d_latent),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Rmats = nn.Parameter(torch.randn(R, d_latent, d_latent) * 0.01)\n",
    "        self.head = nn.Sequential(nn.Linear(R, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "    def forward(self, H: torch.Tensor, A_probs: torch.Tensor) -> torch.Tensor:\n",
    "        N, M, d = H.shape\n",
    "        Hpool = H.mean(dim=1)\n",
    "        z = self.encoder(Hpool)\n",
    "        energies = []\n",
    "        for r in range(self.R):\n",
    "            Rmat = self.Rmats[r]\n",
    "            S = z @ Rmat @ z.t()\n",
    "            S_sig = torch.sigmoid(S)\n",
    "            diff = (A_probs[r] - S_sig).abs().mean()\n",
    "            energies.append(diff)\n",
    "        stats = torch.stack(energies)\n",
    "        out = self.head(stats.unsqueeze(0))\n",
    "        return out.squeeze()\n",
    "\n",
    "# ---------- EMA helper ----------\n",
    "def ema_update(model_ema: nn.Module, model: nn.Module, tau: float):\n",
    "    for p_ema, p in zip(model_ema.parameters(), model.parameters()):\n",
    "        p_ema.data.mul_(tau).add_(p.data, alpha=(1.0 - tau))\n",
    "\n",
    "# ---------- training ----------\n",
    "def train():\n",
    "    H = load_HL()\n",
    "    N, M, d_found = H.shape\n",
    "    proposer = ProposerNet(R=R, d_out=d_found, d_rank=d_rank, d_q=d_q, d_k=d_k, d_e=d_e,\n",
    "                           film_hidden=film_hidden, use_rel_loop=use_rel_loop).to(DEVICE)\n",
    "    ebm = EnergyModel(R=R, d_in=d_found).to(DEVICE)\n",
    "    ebm_ema = EnergyModel(R=R, d_in=d_found).to(DEVICE)\n",
    "    ebm_ema.load_state_dict(ebm.state_dict())\n",
    "\n",
    "    opt_prop = torch.optim.AdamW(proposer.parameters(), lr=proposer_lr, weight_decay=1e-6)\n",
    "    opt_ebm = torch.optim.AdamW(ebm.parameters(), lr=ebm_lr, weight_decay=1e-6)\n",
    "\n",
    "    sampler = LangevinSampler(steps=mcmc_steps, step_size=mcmc_step_size, noise=mcmc_noise)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        proposer.train()\n",
    "        ebm.train()\n",
    "\n",
    "        U, V, A_logits, A0 = proposer(H)\n",
    "\n",
    "        ebm_ema.eval()\n",
    "        L_energy = ebm_ema(H, A0)\n",
    "        if L_energy.dim() != 0:\n",
    "            L_energy = L_energy.mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            A_star = sampler.sample(energy_fn=lambda X: ebm(H, X), X_init=A0)\n",
    "\n",
    "        L_distill = F.l1_loss(A0, A_star.detach(), reduction=\"mean\")\n",
    "        L_sparse = A0.abs().mean()\n",
    "\n",
    "        total_prop_loss = alpha_energy * L_energy + alpha_distill * L_distill + alpha_sparse * (sparse_beta * L_sparse)\n",
    "\n",
    "        opt_prop.zero_grad()\n",
    "        total_prop_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(proposer.parameters(), max_norm=5.0)\n",
    "        opt_prop.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            A_neg = torch.bernoulli(torch.rand_like(A0)).to(DEVICE)\n",
    "        E_pos = ebm(H, A_star.detach())\n",
    "        E_neg = ebm(H, A_neg)\n",
    "        margin = 0.1\n",
    "        hinge = F.relu(margin + E_pos - E_neg)\n",
    "        ebm_loss = E_pos + hinge\n",
    "\n",
    "        opt_ebm.zero_grad()\n",
    "        ebm_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ebm.parameters(), max_norm=5.0)\n",
    "        opt_ebm.step()\n",
    "\n",
    "        ema_update(ebm_ema, ebm, ema_tau)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | prop_loss {total_prop_loss.item():.6f} (E {L_energy.item():.6f} D {L_distill.item():.6f} S {L_sparse.item():.6f}) | ebm_loss {ebm_loss.item():.6f}\")\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            ckpt = {\n",
    "                \"epoch\": epoch,\n",
    "                \"proposer\": proposer.state_dict(),\n",
    "                \"ebm\": ebm.state_dict(),\n",
    "                \"ebm_ema\": ebm_ema.state_dict(),\n",
    "                \"U\": U.detach().cpu(),\n",
    "                \"V\": V.detach().cpu(),\n",
    "                \"A0\": A0.detach().cpu()\n",
    "            }\n",
    "            SAVE_PROPOSER.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(ckpt, SAVE_PROPOSER)\n",
    "\n",
    "    final = {\n",
    "        \"proposer\": proposer.state_dict(),\n",
    "        \"ebm\": ebm.state_dict(),\n",
    "        \"ebm_ema\": ebm_ema.state_dict()\n",
    "    }\n",
    "    torch.save(final, SAVE_PROPOSER.with_suffix(\".final.pth\"))\n",
    "    print(\"Done. Saved proposer and ebm checkpoints.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fa19d",
   "metadata": {},
   "source": [
    "### 3.4.2.5 Self-Supervised Training: The Causal Energy Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch_energy.samplers import LangevinSampler\n",
    "\n",
    "# ---------- config  ----------\n",
    "RNG_SEED = 20251127\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "OUT = Path(\"out\")\n",
    "LAYER2_OUT = OUT / \"lpa_layer2_output.pt\"   \n",
    "LAYER1_OUT = OUT / \"lpa_layer1_output.pt\"\n",
    "C_PATH = OUT / \"C_soft.pt\"                  \n",
    "H_AUG_PATH = OUT / \"H_aug.pt\"               \n",
    "\n",
    "SAVE_CKPT = OUT / \"causal_energy_curriculum_ckpt.pth\"\n",
    "\n",
    "# model hyperparams\n",
    "d_rank = 16\n",
    "proposer_lr = 3e-4\n",
    "ebm_lr = 3e-4\n",
    "epochs = 40\n",
    "mcmc_steps_pos = 12\n",
    "mcmc_step_size = 0.08\n",
    "mcmc_noise = 1e-2\n",
    "sld_ascent_steps = 6\n",
    "sld_step = 0.05\n",
    "sld_noise = 1e-2\n",
    "ema_tau = 0.995\n",
    "negatives_easy = 8\n",
    "negatives_hard = 8\n",
    "\n",
    "# loss weights \n",
    "alpha_energy = 0.1\n",
    "alpha_distill = 1.0\n",
    "alpha_sparse = 1e-2\n",
    "lambda_coarse = 1.0\n",
    "lambda_fine = 1.0\n",
    "lambda_consistency = 10.0\n",
    "lambda_dag = 10.0\n",
    "lambda_sparse_coarse = 1.0\n",
    "mu_outdeg = 2.0\n",
    "lambda_deg = 0.1\n",
    "invariance_enable = True  \n",
    "\n",
    "# ---------- utilities ----------\n",
    "def load_H():\n",
    "    if LAYER2_OUT.exists():\n",
    "        o = torch.load(LAYER2_OUT, map_location=\"cpu\")\n",
    "        k = \"H2\" if \"H2\" in o else next(iter(o.keys()))\n",
    "        H = o[k]\n",
    "    elif LAYER1_OUT.exists():\n",
    "        o = torch.load(LAYER1_OUT, map_location=\"cpu\")\n",
    "        k = \"H1\" if \"H1\" in o else next(iter(o.keys()))\n",
    "        H = o[k]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No layer outputs found in out/ (expected lpa_layer2_output.pt or lpa_layer1_output.pt)\")\n",
    "    if not isinstance(H, torch.Tensor):\n",
    "        raise RuntimeError(\"Loaded H is not a tensor\")\n",
    "    return H.to(DEVICE)\n",
    "\n",
    "def load_C():\n",
    "    if not C_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Soft assignment matrix C not found at {C_PATH}\")\n",
    "    C = torch.load(C_PATH, map_location=\"cpu\")\n",
    "    if not isinstance(C, torch.Tensor):\n",
    "        raise RuntimeError(\"Loaded C is not a tensor\")\n",
    "    return C.to(DEVICE)\n",
    "\n",
    "def load_H_aug():\n",
    "    if not H_AUG_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Augmented environment H not found at {H_AUG_PATH} (required when invariance enabled)\")\n",
    "    H = torch.load(H_AUG_PATH, map_location=\"cpu\")\n",
    "    if not isinstance(H, torch.Tensor):\n",
    "        raise RuntimeError(\"Loaded H_aug is not a tensor\")\n",
    "    return H.to(DEVICE)\n",
    "\n",
    "# ---------- proposer (Q_phi) (FiLM hypernetwork) ----------\n",
    "class FiLMGenerator(nn.Module):\n",
    "    def __init__(self, e_dim:int, hidden_dim:int, out_dim:int):\n",
    "        super().__init__()\n",
    "        self.gam = nn.Sequential(nn.Linear(e_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.bet = nn.Sequential(nn.Linear(e_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "    def forward(self, e):\n",
    "        return self.gam(e), self.bet(e)\n",
    "\n",
    "class SharedG(nn.Module):\n",
    "    def __init__(self, x_dim:int, hidden_dim:int, out_dim:int):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(x_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.W2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.b1 = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.b2 = nn.Parameter(torch.zeros(out_dim))\n",
    "    def forward(self, x, gamma, beta):\n",
    "        h = self.W1(x) + self.b1\n",
    "        h = self.act(h)\n",
    "        h = h * gamma + beta\n",
    "        return self.W2(h) + self.b2\n",
    "\n",
    "class ProposerNet(nn.Module):\n",
    "    def __init__(self, R:int, d_out:int, d_rank:int, d_q:int=64, d_k:int=64, d_e:int=32, film_hidden:int=128, use_rel_loop:bool=False):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.d_out = d_out\n",
    "        self.d_rank = d_rank\n",
    "        self.d_q = d_q\n",
    "        self.d_k = d_k\n",
    "        self.d_e = d_e\n",
    "        self.use_rel_loop = use_rel_loop\n",
    "\n",
    "        self.q_rel = nn.Parameter(torch.randn(R, d_q) * 0.02)\n",
    "        self.WQ = nn.Linear(d_q, d_k, bias=False)\n",
    "        self.WK = nn.Linear(d_out, d_k, bias=False)\n",
    "\n",
    "        self.eU = nn.Parameter(torch.randn(R, d_e) * 0.02)\n",
    "        self.eV = nn.Parameter(torch.randn(R, d_e) * 0.02)\n",
    "\n",
    "        self.film_U = FiLMGenerator(d_e, film_hidden, film_hidden)\n",
    "        self.film_V = FiLMGenerator(d_e, film_hidden, film_hidden)\n",
    "        self.g_shared = SharedG(x_dim=d_out, hidden_dim=film_hidden, out_dim=d_rank)\n",
    "\n",
    "        self.b_rel = nn.Parameter(torch.zeros(R))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ.weight)\n",
    "        nn.init.xavier_uniform_(self.WK.weight)\n",
    "        nn.init.xavier_uniform_(self.g_shared.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.g_shared.W2.weight)\n",
    "\n",
    "    def relation_pooling(self, H: torch.Tensor) -> torch.Tensor:\n",
    "        N, M, d = H.shape\n",
    "        K = self.WK(H.view(N*M, d)).view(N, M, self.d_k)\n",
    "        Q = self.WQ(self.q_rel)  # [R, d_k]\n",
    "        Q_exp = Q.unsqueeze(1).unsqueeze(2)\n",
    "        K_exp = K.unsqueeze(0)\n",
    "        att_logits = (Q_exp * K_exp).sum(dim=-1) / math.sqrt(self.d_k)\n",
    "        att = F.softmax(att_logits, dim=-1)\n",
    "        pooled = (att.unsqueeze(-1) * H.unsqueeze(0)).sum(dim=2)\n",
    "        return pooled  # [R, N, d_out]\n",
    "\n",
    "    def hyper_generate_UV(self, pooled: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        R, N, d = pooled.shape\n",
    "        device = pooled.device\n",
    "        if self.use_rel_loop:\n",
    "            U = torch.zeros(R, N, self.d_rank, device=device)\n",
    "            V = torch.zeros(R, N, self.d_rank, device=device)\n",
    "            for r in range(R):\n",
    "                h_r = pooled[r]\n",
    "                gamma_u, beta_u = self.film_U(self.eU[r:r+1].expand(N, -1))\n",
    "                gamma_v, beta_v = self.film_V(self.eV[r:r+1].expand(N, -1))\n",
    "                U[r] = self.g_shared(h_r, gamma_u, beta_u)\n",
    "                V[r] = self.g_shared(h_r, gamma_v, beta_v)\n",
    "            return U, V\n",
    "        pooled_flat = pooled.reshape(R*N, d)\n",
    "        eU_exp = self.eU.unsqueeze(1).expand(-1, N, -1).reshape(R*N, self.d_e)\n",
    "        eV_exp = self.eV.unsqueeze(1).expand(-1, N, -1).reshape(R*N, self.d_e)\n",
    "        gamma_u, beta_u = self.film_U(eU_exp)\n",
    "        gamma_v, beta_v = self.film_V(eV_exp)\n",
    "        outU = self.g_shared(pooled_flat, gamma_u, beta_u)\n",
    "        outV = self.g_shared(pooled_flat, gamma_v, beta_v)\n",
    "        U = outU.view(R, N, self.d_rank)\n",
    "        V = outV.view(R, N, self.d_rank)\n",
    "        return U, V\n",
    "\n",
    "    def forward(self, H: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        pooled = self.relation_pooling(H)\n",
    "        U, V = self.hyper_generate_UV(pooled)\n",
    "        R_, N, dr = U.shape\n",
    "        A_logits = torch.zeros(R_, N, N, device=H.device)\n",
    "        for r in range(R_):\n",
    "            Ur = U[r]\n",
    "            Vr = V[r]\n",
    "            A_logits[r] = Ur @ Vr.t() + self.b_rel[r]\n",
    "        A_probs = torch.sigmoid(A_logits)\n",
    "        return U, V, A_logits, A_probs\n",
    "\n",
    "# ---------- Hierarchical EBM (E_total) ----------\n",
    "class HierarchicalEBM(nn.Module):\n",
    "    def __init__(self, R:int, d_node:int, K_coarse:int, hidden:int=256, latent:int=64):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.K = K_coarse\n",
    "        self.encoder = nn.Sequential(nn.Linear(d_node, hidden), nn.ReLU(), nn.Linear(hidden, latent), nn.ReLU())\n",
    "        # coarse learned A_coarse param (we let the model predict coarse by a small head instead of free param)\n",
    "        self.coarse_head = nn.Sequential(nn.Linear(latent, hidden), nn.ReLU(), nn.Linear(hidden, K_coarse*K_coarse))\n",
    "        # per-relation small matrices for local scoring (used in local fine energy)\n",
    "        self.Rmats = nn.Parameter(torch.randn(R, latent, latent) * 0.01)\n",
    "        self.fine_head = nn.Sequential(nn.Linear(R, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, H: torch.Tensor, A_probs: torch.Tensor, C: torch.Tensor) -> torch.Tensor:\n",
    "        # H: [N, M, d_node], A_probs: [R, N, N], C: [N, K]\n",
    "        N, M, d = H.shape\n",
    "        Hpool = H.mean(dim=1)  # [N, d_node]\n",
    "        z = self.encoder(Hpool)  # [N, latent]\n",
    "        # Coarse prediction from z pooled to communities\n",
    "        # compute community embeddings E = C^T z  -> [K, latent] but we let coarse_head operate on mean\n",
    "        E = (C.t() @ z) / (C.sum(dim=0, keepdim=True).t().clamp(min=1.0))\n",
    "        # produce learned A_coarse per relation via coarse_head applied to each community embedding pair\n",
    "        # simpler: produce A_coarse_pred[r] as outer product normalization of community embeddings projected\n",
    "        Acoarse_pred = []\n",
    "        for r in range(self.R):\n",
    "            # compute relation-specific coarse logits via community embeddings\n",
    "            # project E -> [K, K] via (E @ W_r @ E.T) with W_r derived from Rmats (reuse Rmats)\n",
    "            W = self.Rmats[r]\n",
    "            S = E @ W @ E.t()  # [K, K]\n",
    "            Acoarse_pred.append(torch.sigmoid(S))\n",
    "        Acoarse_pred = torch.stack(Acoarse_pred, dim=0)  # [R, K, K]\n",
    "\n",
    "        # compute aggregated coarse from fine A_probs\n",
    "        # A_hat_coarse[r] = C^T @ A_probs[r] @ C\n",
    "        A_hat_coarse = torch.einsum('ni,rij,nj->rkj', C, A_probs, C)  # shape [R, K, K]\n",
    "        # consistency energy\n",
    "        E_consistency = ((Acoarse_pred - A_hat_coarse).pow(2)).sum()\n",
    "\n",
    "        E_coarse_global = 0.0\n",
    "        for r in range(self.R):\n",
    "            Ar = Acoarse_pred[r]\n",
    "            Ar_sq = Ar * Ar\n",
    "            h = torch.trace(torch.matrix_exp(Ar_sq)) - Ar_sq.size(0)\n",
    "            E_coarse_global = E_coarse_global + lambda_dag * h + lambda_sparse_coarse * Ar.abs().sum()\n",
    "\n",
    "        E_coarse = E_coarse_global\n",
    "\n",
    "        fine_terms = []\n",
    "        for r in range(self.R):\n",
    "            Rmat = self.Rmats[r]\n",
    "            S = z @ Rmat @ z.t()\n",
    "            S_sig = torch.sigmoid(S)\n",
    "            diff = (A_probs[r] - S_sig).abs().mean()\n",
    "            fine_terms.append(diff)\n",
    "        fine_stack = torch.stack(fine_terms)\n",
    "        E_fine = self.fine_head(fine_stack.unsqueeze(0)).squeeze()\n",
    "\n",
    "        # total energy\n",
    "        E_total = lambda_coarse * E_coarse + lambda_fine * E_fine + lambda_consistency * E_consistency\n",
    "        return E_total\n",
    "\n",
    "def sgld_ascent_on_A(A_init: torch.Tensor, H: torch.Tensor, ebm: nn.Module, steps: int, step_size: float, noise: float) -> torch.Tensor:\n",
    "    logits = torch.logit(A_init.clamp(1e-6, 1 - 1e-6))\n",
    "    logits = logits.detach().clone().to(H.device).requires_grad_(True)\n",
    "    for _ in range(steps):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        e = ebm(H, probs)\n",
    "        g = torch.autograd.grad(e, logits, retain_graph=False, create_graph=False)[0]\n",
    "        logits = logits + step_size * g + noise * torch.randn_like(logits)\n",
    "        logits = logits.detach().requires_grad_(True)\n",
    "    return torch.sigmoid(logits.detach())\n",
    "\n",
    "# ---------- contrastive InfoNCE loss ----------\n",
    "def info_nce_loss(E_pos: torch.Tensor, E_negs: torch.Tensor, temp: float = 1.0) -> torch.Tensor:\n",
    "    # E_pos scalar or [B], E_negs [M] or [M,B]; convert to vectors\n",
    "    if E_pos.dim() == 0:\n",
    "        num = torch.exp(-E_pos / temp)\n",
    "        den = num + torch.exp(-E_negs / temp).sum()\n",
    "        return -torch.log(num / den + 1e-12)\n",
    "    else:\n",
    "        num = torch.exp(-E_pos / temp)\n",
    "        den = num + torch.exp(-E_negs / temp).sum(dim=0)\n",
    "        return -torch.log(num / den + 1e-12).mean()\n",
    "\n",
    "# ---------- training loop ----------\n",
    "def train():\n",
    "    H = load_H()  # [N, M, d_out]\n",
    "    N, M, d_out = H.shape\n",
    "    C = load_C()  # [N, K]\n",
    "    K = C.size(1)\n",
    "    H_aug = None\n",
    "    if invariance_enable:\n",
    "        H_aug = load_H_aug()  # [N, M, d_out] for augmented environment\n",
    "\n",
    "    R_local = int(max(1, getattr(torch, \"R\", 0))) \n",
    "    R_used = R_local if R_local > 1 else 6  \n",
    "    R_used = int(R_used)\n",
    "\n",
    "    proposer = ProposerNet(R=R_used, d_out=d_out, d_rank=d_rank).to(DEVICE)\n",
    "    ebm = HierarchicalEBM(R=R_used, d_node=d_out, K_coarse=K).to(DEVICE)\n",
    "    ebm_ema = HierarchicalEBM(R=R_used, d_node=d_out, K_coarse=K).to(DEVICE)\n",
    "    ebm_ema.load_state_dict(ebm.state_dict())\n",
    "\n",
    "    opt_prop = torch.optim.AdamW(proposer.parameters(), lr=proposer_lr, weight_decay=1e-6)\n",
    "    opt_ebm = torch.optim.AdamW(ebm.parameters(), lr=ebm_lr, weight_decay=1e-6)\n",
    "\n",
    "    sampler_pos = LangevinSampler(steps=mcmc_steps_pos, step_size=mcmc_step_size, noise=mcmc_noise)\n",
    "\n",
    "    tasks = [\"contrastive\", \"proposer\", \"invariance\", \"aux\"]\n",
    "    log_vars = nn.ParameterDict({t: nn.Parameter(torch.tensor(0.0)) for t in tasks})\n",
    "    opt_task = torch.optim.AdamW(list(log_vars.parameters()), lr=1e-3)\n",
    "\n",
    "    H = H.to(DEVICE)\n",
    "    if H_aug is not None:\n",
    "        H_aug = H_aug.to(DEVICE)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        proposer.train()\n",
    "        ebm.train()\n",
    "\n",
    "        # forward proposer\n",
    "        U, V, A_logits, A0 = proposer(H)  # A0 [R,N,N]\n",
    "\n",
    "        # proposer Lenergy via EMA EBM\n",
    "        ebm_ema.eval()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            L_energy = ebm_ema(H, A0, C)\n",
    "            if L_energy.dim() != 0:\n",
    "                L_energy = L_energy.mean()\n",
    "\n",
    "        # MCMC refine to get A_star (positive examples) - no grad to proposer through refine (stopgrad)\n",
    "        with torch.no_grad():\n",
    "            A_star = sampler_pos.sample(energy_fn=lambda Ap: ebm(H, Ap, C), X_init=A0)\n",
    "\n",
    "        # Ldistill and Lsparse\n",
    "        L_distill = F.l1_loss(A0, A_star.detach(), reduction=\"mean\")\n",
    "        L_sparse = A0.abs().mean()\n",
    "        L_total_proposer = alpha_energy * L_energy + alpha_distill * L_distill + alpha_sparse * L_sparse\n",
    "\n",
    "        # CONTRASTIVE LOSS for EBM\n",
    "        # Positive energy at A_star\n",
    "        E_pos = ebm(H, A_star.detach(), C)\n",
    "        # Easy negatives: random Bernoulli graphs\n",
    "        A_neg_easy = torch.bernoulli(torch.full_like(A0, 0.01))  # sparse random\n",
    "        E_negs_easy = ebm(H, A_neg_easy.to(DEVICE), C)\n",
    "        # assemble negatives list\n",
    "        # Hard negatives: SGLD ascent from A_star (few steps)\n",
    "        A_neg_hard = sgld_ascent_on_A(A_star, H, ebm, steps=sld_ascent_steps, step_size=sld_step, noise=sld_noise)\n",
    "        # create batched negatives stack\n",
    "        E_negs = torch.stack([E_negs_easy, ebm(H, A_neg_hard, C)])\n",
    "        L_contrastive = info_nce_loss(E_pos, E_negs, temp=1.0)\n",
    "\n",
    "        # INVARIANCE \n",
    "        if invariance_enable and H_aug is not None:\n",
    "            # generate proposer outputs for augmented env (no grad to proposer for invariance teacher? spec updates encoder and ebm)\n",
    "            U2, V2, A_logits2, A02 = proposer(H_aug)\n",
    "            with torch.no_grad():\n",
    "                A_star2 = sampler_pos.sample(energy_fn=lambda Ap: ebm(H_aug, Ap, C), X_init=A02)\n",
    "            E1 = ebm(H, A_star.detach(), C)\n",
    "            E2 = ebm(H_aug, A_star2.detach(), C)\n",
    "            Linv_energy = ((E1 - E2).pow(2)).mean()\n",
    "            # representation-level invariance: extract a causal embedding from proposer/encoder\n",
    "            # heuristic: use U (or the proposer pooled outputs) as proxy Z; compute mean over relations of self rows\n",
    "            Z1 = U.mean(dim=0)  # [N, d_rank]\n",
    "            Z2 = U2.mean(dim=0)\n",
    "            Linv_repr = F.mse_loss(Z1, Z2, reduction=\"mean\")\n",
    "            Linv = 0.1 * Linv_energy + 1.0 * Linv_repr\n",
    "        else:\n",
    "            Linv = torch.tensor(0.0, device=DEVICE)\n",
    "            Linv_energy = torch.tensor(0.0, device=DEVICE)\n",
    "            Linv_repr = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        # Laux: MoE specialization & load balancing - approximate using diversity of U,V & sparsity of relation embeddings\n",
    "        L_moespec = (U.pow(2).mean() + V.pow(2).mean()) * 1e-4\n",
    "        # load-balance proxy: variance of mean usage across relations (minimize variance)\n",
    "        # compute relation mass sum\n",
    "        relation_mass = A0.sum(dim=(1,2))  # [R]\n",
    "        load_bal = relation_mass.mean() / (relation_mass.std() + 1e-8)\n",
    "        L_loadbalance = (1.0 / (load_bal + 1e-8))\n",
    "        L_aux = L_moespec + 1e-2 * L_loadbalance\n",
    "\n",
    "        # Compose weighted total losses with uncertainty weighting\n",
    "        # For EBM (contrastive) and proposer and invariance and aux\n",
    "        def weighted(l, name):\n",
    "            s = log_vars[name]\n",
    "            return 0.5 * torch.exp(-s) * l + 0.5 * s\n",
    "\n",
    "        total_loss_tasks = weighted(L_contrastive, \"contrastive\") + weighted(L_total_proposer, \"proposer\") + weighted(Linv, \"invariance\") + weighted(L_aux, \"aux\")\n",
    "\n",
    "        # Update proposer (only proposer params) using its weighted term\n",
    "        opt_prop.zero_grad()\n",
    "        prop_loss_for_step = weighted(L_total_proposer, \"proposer\")\n",
    "        prop_loss_for_step.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(proposer.parameters(), max_norm=5.0)\n",
    "        opt_prop.step()\n",
    "\n",
    "        # Update EBM using contrastive weighted term\n",
    "        opt_ebm.zero_grad()\n",
    "        ebm_loss_for_step = weighted(L_contrastive, \"contrastive\")\n",
    "        ebm_loss_for_step.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(ebm.parameters(), max_norm=5.0)\n",
    "        opt_ebm.step()\n",
    "\n",
    "        # Update task log_vars\n",
    "        opt_task.zero_grad()\n",
    "        total_loss_tasks.backward()\n",
    "        opt_task.step()\n",
    "\n",
    "        # Optionally update encoder / causal stream if you have access; here we assume H fixed.\n",
    "        # EMA update of EBM critic\n",
    "        for p_ema, p in zip(ebm_ema.parameters(), ebm.parameters()):\n",
    "            p_ema.data.mul_(ema_tau).add_(p.data, alpha=(1.0 - ema_tau))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | prop_loss {L_total_proposer.item():.6f} | contrastive {L_contrastive.item():.6f} | Linv {Linv.item():.6f} | Laux {L_aux.item():.6f}\")\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            ck = {\n",
    "                \"epoch\": epoch,\n",
    "                \"proposer_state\": proposer.state_dict(),\n",
    "                \"ebm_state\": ebm.state_dict(),\n",
    "                \"ebm_ema_state\": ebm_ema.state_dict(),\n",
    "                \"log_vars\": {k: v.detach().cpu() for k, v in log_vars.items()},\n",
    "                \"U\": U.detach().cpu(),\n",
    "                \"V\": V.detach().cpu(),\n",
    "                \"A0\": A0.detach().cpu(),\n",
    "                \"A_star\": A_star.detach().cpu()\n",
    "            }\n",
    "            SAVE_CKPT.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(ck, SAVE_CKPT)\n",
    "\n",
    "    # final save\n",
    "    final = {\n",
    "        \"proposer_state\": proposer.state_dict(),\n",
    "        \"ebm_state\": ebm.state_dict(),\n",
    "        \"ebm_ema_state\": ebm_ema.state_dict(),\n",
    "        \"log_vars\": {k: v.detach().cpu() for k, v in log_vars.items()}\n",
    "    }\n",
    "    torch.save(final, SAVE_CKPT.with_suffix(\".final.pth\"))\n",
    "    print(\"Training finished. Checkpoint saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b339850",
   "metadata": {},
   "source": [
    "### 3.4.2.6 Final Output and Inference (Reasoning with Uncertainity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296284d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from proposer_qphi_final import ProposerNet  # proposer implementation\n",
    "from causal_energy_curriculum import HierarchicalEBM  # hierarchical EBM\n",
    "\n",
    "# ---------- config ----------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUT = Path(\"out\")\n",
    "LAYER2_OUT = OUT / \"lpa_layer2_output.pt\"\n",
    "LAYER1_OUT = OUT / \"lpa_layer1_output.pt\"\n",
    "C_PATH = OUT / \"C_soft.pt\"\n",
    "PROPOSER_CKPT = OUT / \"proposer_qphi_checkpoint.pth.final.pth\"\n",
    "EBM_CKPT = OUT / \"causal_energy_curriculum_ckpt.pth.final.pth\"\n",
    "ENCODER_CKPT = OUT / \"lpa_model\" / \"encoder_finetuned.pth\"\n",
    "RESULTS_OUT = OUT / \"inference_results_reuse.pt\"\n",
    "\n",
    "Nenc = 5\n",
    "Nprop = 10\n",
    "epistemic_threshold = 0.1\n",
    "k_chains = 32\n",
    "T_sgld = 300\n",
    "burn_in = int(T_sgld * 0.2)\n",
    "init_noise_std = 1e-2\n",
    "sgld_step0 = 0.02\n",
    "sgld_noise = 1e-3\n",
    "Tmax = 5.0\n",
    "seed = 20251127\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ---------- loaders ----------\n",
    "def load_H() -> torch.Tensor:\n",
    "    if LAYER2_OUT.exists():\n",
    "        o = torch.load(LAYER2_OUT, map_location=\"cpu\")\n",
    "        k = \"H2\" if \"H2\" in o else next(iter(o.keys()))\n",
    "        H = o[k]\n",
    "    elif LAYER1_OUT.exists():\n",
    "        o = torch.load(LAYER1_OUT, map_location=\"cpu\")\n",
    "        k = \"H1\" if \"H1\" in o else next(iter(o.keys()))\n",
    "        H = o[k]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No layer outputs found in out/\")\n",
    "    return H.to(DEVICE)\n",
    "\n",
    "def load_C() -> torch.Tensor:\n",
    "    obj = torch.load(C_PATH, map_location=\"cpu\")\n",
    "    return obj.to(DEVICE)\n",
    "\n",
    "def load_models() -> Tuple[ProposerNet, HierarchicalEBM]:\n",
    "    ck_p = torch.load(PROPOSER_CKPT, map_location=\"cpu\")\n",
    "    proposer_state = ck_p.get(\"proposer\") or ck_p.get(\"proposer_state\") or ck_p\n",
    "    # infer dims\n",
    "    d_out = None; R = None; d_rank = None\n",
    "    for k,v in proposer_state.items():\n",
    "        if k.endswith(\"WK.weight\"):\n",
    "            d_out = v.size(1)\n",
    "        if k.endswith(\"q_rel\"):\n",
    "            R = v.size(0)\n",
    "        if k.endswith(\"g_shared.W2.weight\"):\n",
    "            d_rank = v.size(0)\n",
    "    if d_out is None or R is None or d_rank is None:\n",
    "        raise RuntimeError(\"Could not infer proposer dims from checkpoint\")\n",
    "    proposer = ProposerNet(R=R, d_out=d_out, d_rank=d_rank).to(DEVICE)\n",
    "    proposer.load_state_dict(proposer_state, strict=True)\n",
    "    ebm_ck = torch.load(EBM_CKPT, map_location=\"cpu\")\n",
    "    ebm_state = ebm_ck.get(\"ebm_state\") or ebm_ck.get(\"ebm\") or ebm_ck\n",
    "    K = load_C().size(1)\n",
    "    ebm = HierarchicalEBM(R=R, d_node=d_out, K_coarse=K).to(DEVICE)\n",
    "    ebm.load_state_dict(ebm_state, strict=True)\n",
    "    proposer.eval(); ebm.eval()\n",
    "    return proposer, ebm\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def reconstruct_A_from_factors(U: torch.Tensor, V: torch.Tensor, b_rel: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "    R, N, d = U.shape\n",
    "    A_logits = torch.zeros(R, N, N, device=U.device)\n",
    "    for r in range(R):\n",
    "        A_logits[r] = U[r] @ V[r].t()\n",
    "        if b_rel is not None:\n",
    "            A_logits[r] = A_logits[r] + b_rel[r]\n",
    "    return torch.sigmoid(A_logits)\n",
    "\n",
    "def temperature_schedule_linear(Tmax: float, steps: int):\n",
    "    def fn(t):\n",
    "        if steps <= 1:\n",
    "            return 1.0\n",
    "        return Tmax - (Tmax - 1.0) * (t / float(steps - 1))\n",
    "    return fn\n",
    "\n",
    "def mc_dropout_proposer_ensemble(proposer: ProposerNet, H: torch.Tensor, Nenc:int, Nprop:int) -> Dict:\n",
    "    samples_U=[]; samples_V=[]; samples_A=[]\n",
    "    proposer.train()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(Nenc):\n",
    "            Hs = H\n",
    "            for _ in range(Nprop):\n",
    "                U,V,logits,A = proposer(Hs)\n",
    "                samples_U.append(U.cpu())\n",
    "                samples_V.append(V.cpu())\n",
    "                samples_A.append(A.cpu())\n",
    "    U_stack = torch.stack(samples_U, dim=0).to(DEVICE)\n",
    "    V_stack = torch.stack(samples_V, dim=0).to(DEVICE)\n",
    "    A_stack = torch.stack(samples_A, dim=0).to(DEVICE)\n",
    "    return {\n",
    "        \"U_stack\": U_stack, \"V_stack\": V_stack, \"A_stack\": A_stack,\n",
    "        \"U_mean\": U_stack.mean(dim=0), \"V_mean\": V_stack.mean(dim=0),\n",
    "        \"A_mean\": A_stack.mean(dim=0), \"A_var\": A_stack.var(dim=0, unbiased=False)\n",
    "    }\n",
    "\n",
    "def select_candidate_edges(A_mean: torch.Tensor, threshold: float=0.1) -> List[Tuple[int,int,int]]:\n",
    "    R,N,_ = A_mean.shape\n",
    "    idx = (A_mean > threshold).nonzero(as_tuple=False)\n",
    "    return [(int(t[0]), int(t[1]), int(t[2])) for t in idx]\n",
    "\n",
    "def compute_edge_stats_from_ensemble(A_stack: torch.Tensor, edges: List[Tuple[int,int,int]]):\n",
    "    stats = {}\n",
    "    if len(edges)==0:\n",
    "        return stats\n",
    "    for (r,i,j) in edges:\n",
    "        vals = A_stack[:, r, i, j].cpu().numpy()\n",
    "        stats[(r,i,j)] = {\"mu\": float(vals.mean()), \"var\": float(vals.var()), \"ci\": (float(np.percentile(vals,2.5)), float(np.percentile(vals,97.5)))}\n",
    "    return stats\n",
    "\n",
    "def sgld_factor_space(U_init: torch.Tensor, V_init: torch.Tensor, H: torch.Tensor, ebm: HierarchicalEBM,\n",
    "                      C: torch.Tensor, steps:int, step_size:float, noise_scale:float, temp_fn) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    k,R,N,d = U_init.shape\n",
    "    U = U_init.clone().to(DEVICE)\n",
    "    V = V_init.clone().to(DEVICE)\n",
    "    for t in range(steps):\n",
    "        Tt = temp_fn(t)\n",
    "        eta = step_size\n",
    "        for c in range(k):\n",
    "            Uc = U[c].detach().clone().requires_grad_(True)\n",
    "            Vc = V[c].detach().clone().requires_grad_(True)\n",
    "            A_logits = torch.zeros(R, N, N, device=DEVICE)\n",
    "            for r in range(R):\n",
    "                A_logits[r] = Uc[r] @ Vc[r].t()\n",
    "            A_probs = torch.sigmoid(A_logits)\n",
    "            E = ebm(H, A_probs, C)\n",
    "            E_scaled = E / Tt\n",
    "            grad_U, grad_V = torch.autograd.grad(E_scaled, (Uc, Vc), retain_graph=False, create_graph=False)\n",
    "            noise_U = torch.randn_like(Uc) * math.sqrt(2.0 * eta) * noise_scale\n",
    "            noise_V = torch.randn_like(Vc) * math.sqrt(2.0 * eta) * noise_scale\n",
    "            Uc_next = Uc - eta * grad_U + noise_U\n",
    "            Vc_next = Vc - eta * grad_V + noise_V\n",
    "            U[c] = Uc_next.detach()\n",
    "            V[c] = Vc_next.detach()\n",
    "    A_final = torch.zeros(k, R, N, N, device=DEVICE)\n",
    "    for c in range(k):\n",
    "        A_final[c] = reconstruct_A_from_factors(U[c], V[c])\n",
    "    return U, V, A_final\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    H = load_H()\n",
    "    C = load_C()\n",
    "    proposer, ebm = load_models()\n",
    "    proposer.to(DEVICE); ebm.to(DEVICE)\n",
    "    proposer.eval(); ebm.eval()\n",
    "\n",
    "    mc = mc_dropout_proposer_ensemble(proposer, H, Nenc=Nenc, Nprop=Nprop)\n",
    "    U_stack = mc[\"U_stack\"]; V_stack = mc[\"V_stack\"]; A_stack = mc[\"A_stack\"]\n",
    "    U_mean = mc[\"U_mean\"]; V_mean = mc[\"V_mean\"]; A_mean = mc[\"A_mean\"]\n",
    "\n",
    "    edges = select_candidate_edges(A_mean, threshold=epistemic_threshold)\n",
    "    epistemic_stats = compute_edge_stats_from_ensemble(A_stack, edges)\n",
    "\n",
    "    R, N, d_rank = U_mean.shape\n",
    "    U0 = U_mean.detach().clone()\n",
    "    V0 = V_mean.detach().clone()\n",
    "    U_chains = U0.unsqueeze(0).expand(k_chains, -1, -1, -1).clone() + torch.randn(k_chains, R, N, d_rank, device=DEVICE) * init_noise_std\n",
    "    V_chains = V0.unsqueeze(0).expand(k_chains, -1, -1, -1).clone() + torch.randn(k_chains, R, N, d_rank, device=DEVICE) * init_noise_std\n",
    "\n",
    "    temp_fn = temperature_schedule_linear(Tmax, T_sgld)\n",
    "    U_final, V_final, A_final = sgld_factor_space(U_chains, V_chains, H, ebm, C, steps=T_sgld, step_size=sgld_step0, noise_scale=sgld_noise, temp_fn=temp_fn)\n",
    "\n",
    "    A_final_np = A_final.cpu().numpy()\n",
    "    struct_stats = {}\n",
    "    for (r,i,j) in edges:\n",
    "        vals = A_final_np[:, r, i, j]\n",
    "        struct_stats[(r,i,j)] = {\"mu\": float(vals.mean()), \"var\": float(vals.var()), \"ci\": (float(np.percentile(vals,2.5)), float(np.percentile(vals,97.5)))}\n",
    "\n",
    "    combined = {}\n",
    "    for e in edges:\n",
    "        ep = epistemic_stats.get(e, {\"mu\":None,\"var\":0.0})\n",
    "        st = struct_stats.get(e, {\"mu\":None,\"var\":0.0})\n",
    "        combined[e] = {\n",
    "            \"epistemic_mu\": ep.get(\"mu\"),\n",
    "            \"epistemic_var\": ep.get(\"var\",0.0),\n",
    "            \"structural_mu\": st.get(\"mu\"),\n",
    "            \"structural_var\": st.get(\"var\",0.0),\n",
    "            \"combined_var\": float(ep.get(\"var\",0.0) + st.get(\"var\",0.0))\n",
    "        }\n",
    "\n",
    "    out = {\n",
    "        \"A_mean\": A_mean.cpu(),\n",
    "        \"A_var_epistemic\": mc[\"A_var\"].cpu(),\n",
    "        \"epistemic_edges\": epistemic_stats,\n",
    "        \"structural_edges\": struct_stats,\n",
    "        \"combined_edges\": combined,\n",
    "        \"A_chains_final\": A_final.cpu(),\n",
    "        \"U_chains_final\": U_final.cpu(),\n",
    "        \"V_chains_final\": V_final.cpu()\n",
    "    }\n",
    "    RESULTS_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(out, RESULTS_OUT)\n",
    "    print(f\"Saved inference results to {RESULTS_OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
