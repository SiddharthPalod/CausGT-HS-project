{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -U dataparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a1d8c",
   "metadata": {},
   "source": [
    "# Initial Loading (From M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79425301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the K1\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "load_path = \"extracted_output/k1.json\"\n",
    "\n",
    "with open(load_path, \"r\") as f:\n",
    "    loaded_list = json.load(f)\n",
    "\n",
    "# Convert back to (int, np.int64) tuple format\n",
    "K1_loaded = [(int(a), np.int64(b)) for (a, b) in loaded_list]\n",
    "\n",
    "print(f\"Loaded K1 from {load_path}\")\n",
    "print(K1_loaded[:10])  # preview\n",
    "\n",
    "## Retrieve the Z (GAE embeddings)\n",
    "\n",
    "import torch\n",
    "\n",
    "load_path = \"extracted_output/Z.pt\"\n",
    "\n",
    "Z_loaded = torch.load(load_path, map_location=\"cpu\")  # safe for CPU use\n",
    "\n",
    "print(f\"Loaded Z from: {load_path}\")\n",
    "print(\"Shape:\", Z_loaded.shape)\n",
    "print(\"Dtype:\", Z_loaded.dtype)\n",
    "\n",
    "## Retrieve the A_co_occur\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "load_path = \"extracted_output/A_co_occur.npz\"\n",
    "\n",
    "A_co_occur_loaded = load_npz(load_path)\n",
    "\n",
    "print(f\"Loaded A_co_occur from: {load_path}\")\n",
    "print(A_co_occur_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bc9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_prior = K1_loaded\n",
    "A_co_occur = A_co_occur_loaded\n",
    "Z = Z_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "## Load our sentences_list\n",
    "sentence_filename = \"segmented_sentences.json\"\n",
    "sentence_file_path = os.path.join(\"extracted_output\", sentence_filename)\n",
    "retrieved_sentences = None\n",
    "\n",
    "print(f\"\\nAttempting to load sentences from: {sentence_file_path}\")\n",
    "\n",
    "with open(sentence_file_path, 'r', encoding='utf-8') as f:\n",
    "    retrieved_sentences = json.load(f)\n",
    "    \n",
    "print(f\"✅ Success: Sentences loaded from {sentence_file_path}\")\n",
    "\n",
    "# Access the list of sentences\n",
    "sentences_list = retrieved_sentences.get(\"sentences\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved relations\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_extracted_relations(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - relations.json\n",
    "      - relations_per_chunk_debug.json\n",
    "\n",
    "    Returns:\n",
    "      relations: list of all deduplicated relation edges\n",
    "      per_chunk_relations: list of per-chunk debug relation records\n",
    "    \"\"\"\n",
    "\n",
    "    relations_path = os.path.join(base_dir, \"relations.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"relations_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(relations_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {relations_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(relations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        relations = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk_relations = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - relations.json\")\n",
    "    print(\" - relations_per_chunk_debug.json\")\n",
    "\n",
    "    return relations, per_chunk_relations\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    relations, relations_per_chunk = load_extracted_relations()\n",
    "\n",
    "    print(f\"Total relations loaded: {len(relations)}\")\n",
    "    print(f\"Total chunks returned: {len(relations_per_chunk)}\")\n",
    "\n",
    "\n",
    "## Load the saved entities\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_extracted_entities(base_dir=\"extracted_output\"):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - entities.json              → list of deduplicated entity/event nodes\n",
    "      - entities_per_chunk_debug.json → raw per-chunk extraction data\n",
    "    \"\"\"\n",
    "\n",
    "    entities_path = os.path.join(base_dir, \"entities.json\")\n",
    "    per_chunk_path = os.path.join(base_dir, \"entities_per_chunk_debug.json\")\n",
    "\n",
    "    if not os.path.exists(entities_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {entities_path}\")\n",
    "\n",
    "    if not os.path.exists(per_chunk_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {per_chunk_path}\")\n",
    "\n",
    "    with open(entities_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        entities = json.load(f)\n",
    "\n",
    "    with open(per_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        per_chunk = json.load(f)\n",
    "\n",
    "    print(\"Loaded:\")\n",
    "    print(\" - entities.json\")\n",
    "    print(\" - entities_per_chunk_debug.json\")\n",
    "\n",
    "    return entities, per_chunk\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    nodes, nodes_per_chunk = load_extracted_entities()\n",
    "    print(f\"Total nodes loaded: {len(nodes)}\")\n",
    "    print(f\"Total chunks loaded: {len(nodes_per_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248ed36",
   "metadata": {},
   "source": [
    "# 3.3.2 Core CoCaD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe3702",
   "metadata": {},
   "source": [
    "## (a) Building $W_{direct}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29932e",
   "metadata": {},
   "source": [
    "### (i).  $f_{structural}$ (Our GNN intervention score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e28740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# =========================================================\n",
    "# HYPERPARAMETERS\n",
    "# =========================================================\n",
    "B_ENSEMBLE   = 5    # number of GAE models in the ensemble\n",
    "GCN_HIDDEN   = 64   # hidden dimension\n",
    "GCN_LATENT   = 64   # latent dimension\n",
    "GCN_EPOCHS   = 200  # epochs per GAE\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY  = 5e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. GCN ENCODER (for GAE)\n",
    "# =========================================================\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer GCN encoder used inside GAE.\n",
    "    cached=False so that changes to edge_index are respected.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=False)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. BUILD BASE GRAPH FROM Z, A_co_occur, E_prior\n",
    "# =========================================================\n",
    "\n",
    "def build_base_graph(\n",
    "    Z_loaded: Union[np.ndarray, torch.Tensor],\n",
    "    A_co_occur: csr_matrix,\n",
    "    E_prior: List[Tuple[int, int]],\n",
    ") -> Tuple[torch.Tensor, csr_matrix, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Uses the learned Z_loaded as node features X.\n",
    "\n",
    "    Inputs:\n",
    "      - Z_loaded: latent embeddings (N x d), either np.ndarray or torch.Tensor.\n",
    "                  Index 0 corresponds to node \"N1\" (1-based in E_prior).\n",
    "      - A_co_occur: csr_matrix adjacency (N x N), typically your symmetric\n",
    "                    co-occurrence scaffolding graph.\n",
    "      - E_prior: list of (i, j) pairs with 1-based node indices (N1 -> 1, N2 -> 2, ...).\n",
    "                 These are your candidate / prior edges from ANN etc.\n",
    "\n",
    "    Steps:\n",
    "      1) Convert Z_loaded → torch.Tensor (X).\n",
    "      2) Convert A_co_occur → LIL and add all E_prior edges (if missing).\n",
    "      3) Symmetrize adjacency.\n",
    "      4) Convert to PyG edge_index.\n",
    "\n",
    "    Returns:\n",
    "      x          : torch.Tensor of shape (N, d)\n",
    "      A_sym      : symmetric csr_matrix adjacency (N x N)\n",
    "      edge_index : torch.LongTensor [2, num_edges]\n",
    "    \"\"\"\n",
    "    assert isinstance(A_co_occur, csr_matrix), \"A_co_occur must be a CSR matrix.\"\n",
    "\n",
    "    # ---- Handle Z_loaded ----\n",
    "    if isinstance(Z_loaded, np.ndarray):\n",
    "        x = torch.from_numpy(Z_loaded.astype(np.float32))\n",
    "    elif isinstance(Z_loaded, torch.Tensor):\n",
    "        x = Z_loaded.float()\n",
    "    else:\n",
    "        raise TypeError(f\"Z_loaded must be np.ndarray or torch.Tensor, got {type(Z_loaded)}\")\n",
    "\n",
    "    N_nodes = x.shape[0]\n",
    "    assert A_co_occur.shape == (N_nodes, N_nodes), \\\n",
    "        \"A_co_occur shape must match number of rows in Z_loaded.\"\n",
    "\n",
    "    # ---- Start from mutable adjacency ----\n",
    "    A_lil: lil_matrix = A_co_occur.tolil()\n",
    "\n",
    "    # ---- Add E_prior edges (1-based → 0-based) ----\n",
    "    for (i_raw, j_raw) in E_prior:\n",
    "        i = int(i_raw) - 1\n",
    "        j = int(j_raw) - 1\n",
    "        if i < 0 or j < 0 or i >= N_nodes or j >= N_nodes:\n",
    "            # defensive: skip out-of-range indices\n",
    "            continue\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        # treat as undirected for the base graph\n",
    "        A_lil[i, j] = 1.0\n",
    "        A_lil[j, i] = 1.0\n",
    "\n",
    "    # Back to CSR and symmetrize\n",
    "    A_aug = A_lil.tocsr().astype(np.float32)\n",
    "    A_sym = ((A_aug + A_aug.T) > 0).astype(np.float32)\n",
    "\n",
    "    # PyG edge_index\n",
    "    edge_index, _ = from_scipy_sparse_matrix(A_sym)\n",
    "\n",
    "    return x, A_sym, edge_index\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAIN A SINGLE GAE\n",
    "# =========================================================\n",
    "\n",
    "def train_single_gae(\n",
    "    x: torch.Tensor,\n",
    "    edge_index: torch.Tensor,\n",
    "    d_in: int,\n",
    "    d_hidden: int,\n",
    "    d_latent: int,\n",
    "    epochs: int,\n",
    "    seed: int,\n",
    ") -> GAE:\n",
    "    \"\"\"\n",
    "    Train a single GAE with a 2-layer GCN encoder on the given graph.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    x = x.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "\n",
    "    encoder = GCNEncoder(d_in, d_hidden, d_latent).to(device)\n",
    "    model = GAE(encoder).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        loss = model.recon_loss(z, data.edge_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 50 == 0:\n",
    "            print(f\"[Seed {seed}] Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD ENSEMBLE OF GNNs\n",
    "# =========================================================\n",
    "\n",
    "def build_gnn_ensemble(\n",
    "    x: torch.Tensor,\n",
    "    edge_index: torch.Tensor,\n",
    "    d_in: int,\n",
    "    d_hidden: int,\n",
    "    d_latent: int,\n",
    "    epochs: int,\n",
    "    B: int,\n",
    ") -> List[GAE]:\n",
    "    \"\"\"\n",
    "    Train B different GAE models with different random seeds.\n",
    "    \"\"\"\n",
    "    ensemble: List[GAE] = []\n",
    "    for b in range(B):\n",
    "        seed = 100 + b\n",
    "        print(f\"\\n=== Training GAE model {b+1}/{B} (seed={seed}) ===\")\n",
    "        model_b = train_single_gae(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            d_in=d_in,\n",
    "            d_hidden=d_hidden,\n",
    "            d_latent=d_latent,\n",
    "            epochs=epochs,\n",
    "            seed=seed,\n",
    "        )\n",
    "        ensemble.append(model_b)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. GNN PREDICTION FOR NODE j\n",
    "# =========================================================\n",
    "\n",
    "def gnn_predict_node_embedding(\n",
    "    model: GAE,\n",
    "    x: torch.Tensor,\n",
    "    edge_index: torch.Tensor,\n",
    "    node_id_1based: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Runs the GAE encoder and returns the latent vector for node j (1-based index).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, edge_index)  # shape: [N_nodes, d_latent]\n",
    "\n",
    "    j_idx = node_id_1based - 1\n",
    "    z_j = z[j_idx].detach().cpu().numpy()\n",
    "\n",
    "    # optional: normalize\n",
    "    norm = np.linalg.norm(z_j) + 1e-12\n",
    "    return z_j / norm\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. COSINE DISTANCE\n",
    "# =========================================================\n",
    "\n",
    "def cosine_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    1 - cosine similarity between two 1D vectors.\n",
    "    \"\"\"\n",
    "    v1 = v1.flatten()\n",
    "    v2 = v2.flatten()\n",
    "    denom = (np.linalg.norm(v1) * np.linalg.norm(v2)) + 1e-12\n",
    "    cos_sim = float(np.dot(v1, v2) / denom)\n",
    "    return 1.0 - cos_sim\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. MAIN: STRUCTURAL EFFECT SCORES (NODE DELETION)\n",
    "# =========================================================\n",
    "\n",
    "def compute_structural_effect_scores(\n",
    "    Z_loaded: Union[np.ndarray, torch.Tensor],\n",
    "    A_co_occur: csr_matrix,\n",
    "    E_prior: List[Tuple[int, int]],\n",
    "    B_ensemble: int = B_ENSEMBLE,\n",
    "    gcn_hidden: int = GCN_HIDDEN,\n",
    "    gcn_latent: int = GCN_LATENT,\n",
    "    gcn_epochs: int = GCN_EPOCHS,\n",
    ") -> Dict[Tuple[int, int], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Structural effect f_structural via a bootstrap ensemble of GNNs\n",
    "    using a NODE-DELETION intervention.\n",
    "\n",
    "    For each (i, j) in E_prior (1-based indices):\n",
    "      For each model b in ensemble:\n",
    "        - z_normal_{j,b}     : embedding of node j on full graph\n",
    "        - z_intervened_{j,b} : embedding of node j when node i is isolated\n",
    "        - score_b(i, j)      : cosine_distance(z_normal, z_intervened)\n",
    "\n",
    "      μ_GNN(i, j)   = mean_b(score_b)\n",
    "      σ^2_GNN(i, j) = var_b(score_b)\n",
    "\n",
    "    Returns:\n",
    "      dict mapping (i, j) → {\n",
    "        \"mean_ensemble_score\": float,\n",
    "        \"variance_ensemble_score\": float\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # Shape sanity\n",
    "    if isinstance(Z_loaded, np.ndarray):\n",
    "        N_nodes, d_embed = Z_loaded.shape\n",
    "    elif isinstance(Z_loaded, torch.Tensor):\n",
    "        N_nodes, d_embed = Z_loaded.shape\n",
    "    else:\n",
    "        raise TypeError(f\"Z_loaded must be np.ndarray or torch.Tensor, got {type(Z_loaded)}\")\n",
    "\n",
    "    print(f\"Z_loaded shape = ({N_nodes}, {d_embed})\")\n",
    "\n",
    "    # 1) Build base graph (features + adjacency)\n",
    "    x, A_sym_base, edge_index_base = build_base_graph(Z_loaded, A_co_occur, E_prior)\n",
    "\n",
    "    # 2) Train ensemble on full graph\n",
    "    print(\"\\n=== Building GNN Ensemble ===\")\n",
    "    ensemble_models = build_gnn_ensemble(\n",
    "        x=x,\n",
    "        edge_index=edge_index_base,\n",
    "        d_in=d_embed,\n",
    "        d_hidden=gcn_hidden,\n",
    "        d_latent=gcn_latent,\n",
    "        epochs=gcn_epochs,\n",
    "        B=B_ensemble,\n",
    "    )\n",
    "\n",
    "    x_device = x.to(device)\n",
    "    edge_index_full_device = edge_index_base.to(device)\n",
    "\n",
    "    results: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    # 3) For each (i, j) pair: delete node i’s edges and measure effect on j\n",
    "    print(\"\\n=== Computing structural effects for E_prior (NODE DELETION) ===\")\n",
    "    for (i_raw, j_raw) in E_prior:\n",
    "        i = int(i_raw)\n",
    "        j = int(j_raw)\n",
    "\n",
    "        if i == j:\n",
    "            # ignore self-edges if any\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n-- Pair (i={i}, j={j}) --\")\n",
    "\n",
    "        # ---- NODE DELETION: isolate node i ----\n",
    "        A_int_lil = A_sym_base.tolil()\n",
    "        idx = i - 1  # 0-based index in adjacency\n",
    "\n",
    "        # zero out row and column\n",
    "        A_int_lil[idx, :] = 0.0\n",
    "        A_int_lil[:, idx] = 0.0\n",
    "\n",
    "        A_int = A_int_lil.tocsr()\n",
    "        edge_index_int, _ = from_scipy_sparse_matrix(A_int)\n",
    "        edge_index_int_device = edge_index_int.to(device)\n",
    "\n",
    "        scores_b: List[float] = []\n",
    "\n",
    "        for b_idx, model_b in enumerate(ensemble_models):\n",
    "            # normal embedding\n",
    "            z_normal_j = gnn_predict_node_embedding(\n",
    "                model=model_b,\n",
    "                x=x_device,\n",
    "                edge_index=edge_index_full_device,\n",
    "                node_id_1based=j,\n",
    "            )\n",
    "\n",
    "            # embedding after deleting node i’s edges\n",
    "            z_intervened_j = gnn_predict_node_embedding(\n",
    "                model=model_b,\n",
    "                x=x_device,\n",
    "                edge_index=edge_index_int_device,\n",
    "                node_id_1based=j,\n",
    "            )\n",
    "\n",
    "            score_b = cosine_distance(z_normal_j, z_intervened_j)\n",
    "            scores_b.append(score_b)\n",
    "            print(f\"  Model {b_idx+1}/{B_ensemble}: score_b = {score_b:.6f}\")\n",
    "\n",
    "        mean_score = float(np.mean(scores_b))\n",
    "        var_score = float(np.var(scores_b))\n",
    "\n",
    "        results[(i, j)] = {\n",
    "            \"mean_ensemble_score\": round(mean_score, 6),\n",
    "            \"variance_ensemble_score\": round(var_score, 8),\n",
    "        }\n",
    "\n",
    "        print(f\"  -> mean = {mean_score:.6f}, var = {var_score:.8f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. EXAMPLE USAGE (plug in your Z, A_co_occur, E_prior)\n",
    "# =========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example wiring. You already have:\n",
    "\n",
    "      - entities (list of dicts; not directly used here)\n",
    "      - relations (list of dicts; used earlier to build A_co_occur and E_prior)\n",
    "      - Z.pt (latent embeddings) saved earlier\n",
    "      - A_co_occur.npz (symmetric co-occurrence adjacency) saved earlier\n",
    "      - E_prior.json (candidate edges from ANN / prior stage), 1-based indices\n",
    "\n",
    "    Replace paths as needed.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from scipy.sparse import load_npz\n",
    "    \n",
    "    # 4) Compute structural effects\n",
    "    results = compute_structural_effect_scores(\n",
    "        Z_loaded=Z,\n",
    "        A_co_occur=A_co_occur,\n",
    "        E_prior=E_prior,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Structural Effect Scores (μ, σ²) ===\")\n",
    "    pprint.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44429165",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_direct_structural_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f549cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(w_direct_structural_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_w_direct_structural_results(w_direct_structural_results: dict,\n",
    "                                     base_path=\"extracted_output/cocad/w_direct\"):\n",
    "    \"\"\"\n",
    "    Saves w_direct_structural_results as a pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(base_path, \"w_direct_structural_results.pkl\")\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(w_direct_structural_results, f)\n",
    "\n",
    "    print(f\"Saved w_direct_structural_results → {save_path}\")\n",
    "\n",
    "\n",
    "save_w_direct_structural_results(w_direct_structural_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve from the above\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_w_direct_structural_results(base_path=\"extracted_output/cocad/w_direct\"):\n",
    "    \"\"\"\n",
    "    Loads w_direct_structural_results from pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    load_path = os.path.join(base_path, \"w_direct_structural_results.pkl\")\n",
    "\n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"No file found at {load_path}\")\n",
    "\n",
    "    with open(load_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded w_direct_structural_results ← {load_path}\")\n",
    "    return data\n",
    "\n",
    "w_direct_structural_results = load_w_direct_structural_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107cd7c",
   "metadata": {},
   "source": [
    "### (ii).  $f_{llm}$ (The LLM Counterfactual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6561c3",
   "metadata": {},
   "source": [
    "#### A. Context Retrieval (RAG-HyDE-RAV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_output/entities.json\", \"r\") as f:\n",
    "    nodes = json.load(f)\n",
    "\n",
    "# Convert list → dict\n",
    "entities_by_id = { node[\"id\"]: node for node in nodes }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(entities_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from typing import List, Tuple, Set, Dict, Any, Optional\n",
    "\n",
    "# External Libraries required: google-genai, sentence-transformers, faiss-cpu, numpy, scikit-learn\n",
    "from google.genai import Client, types\n",
    "from google.genai import errors as genai_errors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "K_HYPOTHETICAL = 2       # Number of causal hypotheses to generate per pair\n",
    "R_STOCHASTIC_SAMPLES = 3 # Number of samples for Semantic Entropy estimation\n",
    "K_RAG = 3                # Top-k items retrieved for verification (per view)\n",
    "K_BASE = 5               # Base pool size for RAG-MMR\n",
    "K_EXPANSION = 5          # Pool expansion factor for uncertainty\n",
    "LAMBDA_MMR = 0.5         # MMR trade-off: 0.5 favors both relevance and diversity\n",
    "KAPPA_RRF = 60           # RRF constant\n",
    "TAU_SUPPORT = 0.80       # Min confidence (p_support) threshold for verification\n",
    "TAU_ENTROPY = 0.15       # Max Semantic Entropy threshold (low entropy = stable)\n",
    "DELAY_SECONDS = 10       # Delay between API calls for stability (free tier friendly)\n",
    "\n",
    "PRIMARY_MODEL = \"gemini-2.5-flash-lite\"\n",
    "FALLBACK_MODEL = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "# --- ASSUMED INPUTS (Must be defined in the execution environment) ---\n",
    "# N_map (Dict[str, Any]): Node ID -> node info (string or rich dict)\n",
    "#   e.g. {\n",
    "#     \"N1\": \"Global Magnitsky Sanctions Bill passage\"\n",
    "#   }\n",
    "#   OR\n",
    "#   {\n",
    "#     \"N1\": {\n",
    "#       \"name\": \"...\",\n",
    "#       \"type\": \"...\",\n",
    "#       \"time\": \"...\",\n",
    "#       \"location\": \"...\",\n",
    "#       \"description\": \"...\"\n",
    "#     },\n",
    "#     ...\n",
    "#   }\n",
    "#\n",
    "# candidate_set (Set[Tuple[int, int]]): Candidate edges in *0-indexed* integer space,\n",
    "#   aligned with sorted(N_map.keys()) order.\n",
    "#\n",
    "# sentences (List[str]): The full document split into sentences (sentence chunking).\n",
    "#\n",
    "# relations (List[Dict]): \n",
    "#   [\n",
    "#     {\n",
    "#       \"source_id\": \"N1\",\n",
    "#       \"target_id\": \"N2\",\n",
    "#       \"relation\": \"targets\",\n",
    "#       \"description\": \"...\",\n",
    "#       \"evidence\": \"...\",\n",
    "#       \"confidence\": 0.95,\n",
    "#       \"source_chunks\": [0]\n",
    "#     },\n",
    "#     ...\n",
    "#   ]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Initialize Client and Embedder\n",
    "client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# GENERAL LLM CALL WRAPPER WITH FALLBACK\n",
    "# ====================================================================\n",
    "\n",
    "def safe_generate_content(\n",
    "    contents: str,\n",
    "    config: types.GenerateContentConfig,\n",
    "    max_retries_per_model: int = 2,\n",
    ") -> types.GenerateContentResponse:\n",
    "    \"\"\"\n",
    "    Try PRIMARY_MODEL first; on 429/RESOURCE_EXHAUSTED, fallback to FALLBACK_MODEL.\n",
    "    Retries each model up to max_retries_per_model times.\n",
    "    \"\"\"\n",
    "\n",
    "    models_to_try = [PRIMARY_MODEL, FALLBACK_MODEL]\n",
    "    last_exc: Optional[Exception] = None\n",
    "\n",
    "    for model_name in models_to_try:\n",
    "        for attempt in range(1, max_retries_per_model + 1):\n",
    "            try:\n",
    "                print(f\"[safe_generate_content] Calling model={model_name}, attempt={attempt}\")\n",
    "                resp = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=contents,\n",
    "                    config=config,\n",
    "                )\n",
    "                return resp\n",
    "            except genai_errors.ClientError as e:\n",
    "                msg = str(e)\n",
    "                status_code = getattr(e, \"status_code\", None)\n",
    "                print(f\"[safe_generate_content] ClientError on {model_name}: {msg}\")\n",
    "\n",
    "                # 429 / RESOURCE_EXHAUSTED -> try next model or retry\n",
    "                if status_code == 429 or \"RESOURCE_EXHAUSTED\" in msg:\n",
    "                    last_exc = e\n",
    "                    # backoff a bit before next attempt / model\n",
    "                    time.sleep(DELAY_SECONDS)\n",
    "                    # break out of retry loop and move to next model\n",
    "                    break\n",
    "                else:\n",
    "                    # Other client errors are not transient\n",
    "                    raise\n",
    "\n",
    "    # If we reach here, both models failed with quota/rate errors\n",
    "    if last_exc is not None:\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"safe_generate_content failed with unknown error.\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 1: PREPARATION & INDEXING\n",
    "# ====================================================================\n",
    "\n",
    "def setup_document_index(sentences: List[str]) -> Tuple[faiss.IndexFlatL2, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sentence-level vector store (VD) + FAISS index over sentences.\n",
    "    \"\"\"\n",
    "    print(\"1. Embedding document sentences (VD)...\")\n",
    "    VD = embedder.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "    d_embed = VD.shape[1]\n",
    "    idx_doc = faiss.IndexFlatL2(d_embed)\n",
    "    idx_doc.add(VD)\n",
    "    print(f\"2. FAISS Index built on M={len(sentences)} sentences.\")\n",
    "\n",
    "    return idx_doc, VD\n",
    "\n",
    "\n",
    "def build_relation_chunks(\n",
    "    relations: List[Dict[str, Any]],\n",
    "    N_map: Dict[str, Any],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Build textual 'relation chunks' for retrieval, one per relation.\n",
    "    Example chunk:\n",
    "\n",
    "    \"N1 -> N2 (targets): The Global Magnitsky Sanctions Bill primarily targeted Titan Industries.\n",
    "     Evidence: The primary target of this legislation was the sprawling Titan Industries conglomerate.\"\n",
    "    \"\"\"\n",
    "    chunks: List[str] = []\n",
    "\n",
    "    def fmt_node_short(node_id: str) -> str:\n",
    "        v = N_map.get(node_id, node_id)\n",
    "        if isinstance(v, dict):\n",
    "            name = v.get(\"name\") or node_id\n",
    "        else:\n",
    "            name = str(v)\n",
    "        return f\"{node_id} ({name})\"\n",
    "\n",
    "    for r in relations:\n",
    "        src = r.get(\"source_id\", \"\")\n",
    "        tgt = r.get(\"target_id\", \"\")\n",
    "        rel = r.get(\"relation\", \"\")\n",
    "        desc = r.get(\"description\") or \"\"\n",
    "        ev = r.get(\"evidence\") or \"\"\n",
    "\n",
    "        src_str = fmt_node_short(src)\n",
    "        tgt_str = fmt_node_short(tgt)\n",
    "\n",
    "        chunk = f\"{src_str} -> {tgt_str} [{rel}]. {desc}\"\n",
    "        if ev:\n",
    "            chunk += f\" Evidence: {ev}\"\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def setup_relation_index(\n",
    "    relations: Optional[List[Dict[str, Any]]],\n",
    "    N_map: Dict[str, Any],\n",
    ") -> Tuple[Optional[faiss.IndexFlatL2], Optional[np.ndarray], Optional[List[str]]]:\n",
    "    \"\"\"\n",
    "    Build a FAISS index over 'relation chunks' if relations are provided.\n",
    "    Returns (idx_rel, VR, rel_chunks) or (None, None, None) if no relations.\n",
    "    \"\"\"\n",
    "    if not relations:\n",
    "        print(\"No relations provided – skipping relation index.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"Embedding relation chunks (VR)...\")\n",
    "    rel_chunks = build_relation_chunks(relations, N_map)\n",
    "    if not rel_chunks:\n",
    "        print(\"No relation chunks built – skipping relation index.\")\n",
    "        return None, None, None\n",
    "\n",
    "    VR = embedder.encode(rel_chunks, convert_to_numpy=True)\n",
    "    d_embed = VR.shape[1]\n",
    "    idx_rel = faiss.IndexFlatL2(d_embed)\n",
    "    idx_rel.add(VR)\n",
    "    print(f\"Relation FAISS Index built on R={len(rel_chunks)} relation chunks.\")\n",
    "    return idx_rel, VR, rel_chunks\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 2: LLM CALLS AND UNCERTAINTY ESTIMATION\n",
    "# ====================================================================\n",
    "\n",
    "def format_node_for_prompt(node_id: str, N_map: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Turn a node id into a rich, human-readable description for the LLM.\n",
    "    Handles:\n",
    "      - simple string entries, or\n",
    "      - dict entries with name/type/time/location/description.\n",
    "    \"\"\"\n",
    "    v = N_map.get(node_id, node_id)\n",
    "    if isinstance(v, dict):\n",
    "        name = v.get(\"name\") or node_id\n",
    "        ntype = v.get(\"type\")\n",
    "        time_val = v.get(\"time\")\n",
    "        loc = v.get(\"location\")\n",
    "        desc = v.get(\"description\")\n",
    "\n",
    "        parts = [name]\n",
    "        meta_bits = []\n",
    "        if ntype:\n",
    "            meta_bits.append(ntype)\n",
    "        if time_val:\n",
    "            meta_bits.append(f\"time={time_val}\")\n",
    "        if loc:\n",
    "            meta_bits.append(f\"location={loc}\")\n",
    "        if meta_bits:\n",
    "            parts.append(f\"({', '.join(meta_bits)})\")\n",
    "        if desc:\n",
    "            parts.append(f\"- {desc}\")\n",
    "        return \" \".join(parts)\n",
    "    else:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def llm_generate_hypotheses(node_i_str: str, node_j_str: str) -> List[str]:\n",
    "    \"\"\"f_hypothetical: Generates K_HYPOTHETICAL causal claims.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a causal reasoning assistant.\n",
    "\n",
    "Generate {K_HYPOTHETICAL} short, hypothetical sentences describing a plausible causal connection\n",
    "from the *cause* to the *effect*.\n",
    "\n",
    "Cause node: {node_i_str}\n",
    "Effect node: {node_j_str}\n",
    "\n",
    "Rules:\n",
    "- Start each sentence with 'It is plausible that'.\n",
    "- Be concise (max 25 words).\n",
    "- Only output the sentences, one per line, no numbering, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "    resp = safe_generate_content(\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.8,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    raw_text = resp.text or \"\"\n",
    "    if not raw_text.strip():\n",
    "        print(\"[llm_generate_hypotheses] Empty LLM response; returning no hypotheses.\")\n",
    "        return []\n",
    "\n",
    "    hypotheses = [\n",
    "        line.strip()\n",
    "        for line in raw_text.split(\"\\n\")\n",
    "        if line.strip() and \"It is plausible that\" in line\n",
    "    ]\n",
    "\n",
    "    return hypotheses[:K_HYPOTHETICAL]\n",
    "\n",
    "\n",
    "class VerifySupport(BaseModel):\n",
    "    \"\"\"Schema for claim verification result.\"\"\"\n",
    "    support: Literal[\"YES\", \"NO\"]\n",
    "\n",
    "\n",
    "def llm_verify_claim(claim: str, evidence: str, temperature: float) -> Tuple[float, str]:\n",
    "    \"\"\"f_verify: Verifies factual support and returns confidence + the model's primary choice.\"\"\"\n",
    "\n",
    "    prompt_verify = f\"\"\"\n",
    "Claim: '{claim}'.\n",
    "Evidence:\n",
    "\\\"\\\"\\\"\n",
    "{evidence}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Based ONLY on this Evidence, does the Evidence strongly support the Claim?\n",
    "\n",
    "Answer strictly as a JSON object of the form:\n",
    "{{\"support\": \"YES\"}} or {{\"support\": \"NO\"}}.\n",
    "\"\"\"\n",
    "\n",
    "    resp = safe_generate_content(\n",
    "        contents=prompt_verify,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=VerifySupport,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    choice = \"NO\"\n",
    "\n",
    "    try:\n",
    "        parsed = getattr(resp, \"parsed\", None)\n",
    "        support_val = None\n",
    "\n",
    "        if isinstance(parsed, VerifySupport):\n",
    "            support_val = parsed.support\n",
    "\n",
    "        if support_val not in (\"YES\", \"NO\"):\n",
    "            raise ValueError(\"Could not find valid 'support' field in structured response\")\n",
    "\n",
    "        choice = support_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[llm_verify_claim] Failed to parse structured response, defaulting to NO: {e}\")\n",
    "        choice = \"NO\"\n",
    "\n",
    "    p_support = 1.0 if choice == \"YES\" else 0.0\n",
    "    return p_support, choice\n",
    "\n",
    "\n",
    "def estimate_semantic_entropy(claim: str, evidence: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates p_support (mean confidence) and H_semantic via R stochastic passes.\n",
    "    \"\"\"\n",
    "    TEMPERATURE = 0.7\n",
    "    yes_count = 0\n",
    "\n",
    "    for _ in range(R_STOCHASTIC_SAMPLES):\n",
    "        p_support_r, choice_r = llm_verify_claim(claim, evidence, temperature=TEMPERATURE)\n",
    "        if choice_r == \"YES\":\n",
    "            yes_count += 1\n",
    "        time.sleep(DELAY_SECONDS)  # Respect rate limits\n",
    "\n",
    "    p_support_mean = yes_count / R_STOCHASTIC_SAMPLES\n",
    "\n",
    "    # Consistency = majority frequency; H_semantic proxy = 1 - consistency\n",
    "    consistency = max(p_support_mean, 1.0 - p_support_mean)\n",
    "    h_semantic_proxy = 1.0 - consistency\n",
    "\n",
    "    return p_support_mean, h_semantic_proxy\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 3: RAG-MMR AND RANK FUSION (RRF) LOGIC\n",
    "# ====================================================================\n",
    "\n",
    "def mmr_rerank(query_vec, candidates_vectors, k_rag: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Reranks candidate indices using Maximal Marginal Relevance (MMR).\n",
    "    query_vec: shape (d,) or (1, d)\n",
    "    candidates_vectors: np.ndarray of shape (K, d)\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    q = np.array(query_vec)\n",
    "    C = np.array(candidates_vectors)\n",
    "\n",
    "    # Make sure both are 2D: (1, d) and (K, d)\n",
    "    if q.ndim == 1:\n",
    "        q = q.reshape(1, -1)\n",
    "    elif q.ndim > 2:\n",
    "        q = q.reshape(q.shape[0], -1)\n",
    "\n",
    "    if C.ndim == 1:\n",
    "        C = C.reshape(1, -1)\n",
    "    elif C.ndim > 2:\n",
    "        C = C.reshape(C.shape[0], -1)\n",
    "\n",
    "    num_candidates = C.shape[0]\n",
    "    if num_candidates == 0:\n",
    "        return []\n",
    "\n",
    "    selected_indices: List[int] = []\n",
    "\n",
    "    for _ in range(min(k_rag, num_candidates)):\n",
    "        best_mmr_score = -np.inf\n",
    "        best_candidate_index = -1\n",
    "\n",
    "        for idx in range(num_candidates):\n",
    "            if idx in selected_indices:\n",
    "                continue\n",
    "\n",
    "            d_i_vec = C[idx:idx+1, :]  # shape (1, d)\n",
    "\n",
    "            # Relevance: sim(d_i, q)\n",
    "            rel_score = cosine_similarity(d_i_vec, q)[0, 0]\n",
    "\n",
    "            # Diversity: max sim(d_i, d_j) over already selected\n",
    "            div_score = 0.0\n",
    "            if selected_indices:\n",
    "                selected_vecs = C[selected_indices, :]  # shape (m, d)\n",
    "                sim_to_selected = cosine_similarity(d_i_vec, selected_vecs)[0]  # shape (m,)\n",
    "                div_score = float(sim_to_selected.max())\n",
    "\n",
    "            mmr_score = (LAMBDA_MMR * rel_score) - ((1.0 - LAMBDA_MMR) * div_score)\n",
    "\n",
    "            if mmr_score > best_mmr_score:\n",
    "                best_mmr_score = mmr_score\n",
    "                best_candidate_index = idx\n",
    "\n",
    "        if best_candidate_index != -1:\n",
    "            selected_indices.append(best_candidate_index)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(all_ranked_lists: List[List[int]], k_final: int) -> List[int]:\n",
    "    \"\"\"Applies RRF to combine multiple ranked lists into a single consensus list.\"\"\"\n",
    "    RRF_scores: Dict[int, float] = {}\n",
    "    for ranked_list in all_ranked_lists:\n",
    "        for rank, doc_index in enumerate(ranked_list, start=1):\n",
    "            score = 1.0 / (KAPPA_RRF + rank)\n",
    "            RRF_scores[doc_index] = RRF_scores.get(doc_index, 0.0) + score\n",
    "\n",
    "    sorted_scores = sorted(RRF_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [doc_index for doc_index, score in sorted_scores][:k_final]\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 4: MAIN PIPELINE EXECUTION\n",
    "# ====================================================================\n",
    "\n",
    "def run_causal_verification_pipeline(\n",
    "    candidate_set: Set[Tuple[int, int]],\n",
    "    N_map: Dict[str, Any],\n",
    "    sentences: List[str],\n",
    "    relations: Optional[List[Dict[str, Any]]] = None,\n",
    ") -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Executes the full hypothesis verification and RAG-MMR pipeline for all candidate pairs.\n",
    "\n",
    "    Improvements vs older version:\n",
    "    - Uses rich node metadata from N_map (if available).\n",
    "    - Uses both sentence chunks (sentences) AND relation chunks (relations)\n",
    "      as evidence for verifying hypotheses.\n",
    "    - Keeps progress bar + RAG-MMR + RRF for final evidence selection.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Setup ---\n",
    "    # Sort node IDs numerically (handles 'N0' if present)\n",
    "    sorted_node_ids = sorted(\n",
    "        N_map.keys(),\n",
    "        key=lambda x: int(x[1:]) if x != \"N0\" else 0\n",
    "    )\n",
    "\n",
    "    # idx -> node_id str map\n",
    "    idx_to_node_id = {idx: node_id for idx, node_id in enumerate(sorted_node_ids)}\n",
    "\n",
    "    # 1. Prepare sentence-level document index\n",
    "    idx_doc, VD = setup_document_index(sentences)\n",
    "\n",
    "    # 2. Prepare relation-level index \n",
    "    idx_rel, VR, rel_chunks = setup_relation_index(relations, N_map)\n",
    "\n",
    "    final_output_map: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "\n",
    "    # Precompute valid pairs (exclude ones with 0) and fix an order\n",
    "    # NOTE: candidate_set is assumed to be 0-based indices aligned with sorted_node_ids\n",
    "    candidate_pairs: List[Tuple[int, int]] = [\n",
    "        (i, j) for (i, j) in sorted(candidate_set) if i != 0 and j != 0\n",
    "    ]\n",
    "    total_pairs = len(candidate_pairs)\n",
    "\n",
    "    print(f\"\\nTotal valid pairs to process: {total_pairs}\")\n",
    "\n",
    "    def print_progress(current_idx: int):\n",
    "        \"\"\"Print simple progress bar on a single line.\"\"\"\n",
    "        if total_pairs == 0:\n",
    "            return\n",
    "        progress = current_idx / total_pairs\n",
    "        bar_len = 30\n",
    "        filled = int(bar_len * progress)\n",
    "        bar = \"#\" * filled + \"-\" * (bar_len - filled)\n",
    "        print(\n",
    "            f\"\\rProgress {current_idx}/{total_pairs} [{bar}] {progress * 100:5.1f}%\",\n",
    "            end=\"\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    # --- Main Loop: Iterate through each plausible link (i, j) ---\n",
    "    for idx_pair, (i, j) in enumerate(candidate_pairs, start=1):\n",
    "        # Update progress bar\n",
    "        print_progress(idx_pair)\n",
    "\n",
    "        node_i_id = idx_to_node_id.get(i, f\"N{i}\")\n",
    "        node_j_id = idx_to_node_id.get(j, f\"N{j}\")\n",
    "\n",
    "        node_i_str = format_node_for_prompt(node_i_id, N_map)\n",
    "        node_j_str = format_node_for_prompt(node_j_id, N_map)\n",
    "\n",
    "        print(f\"\\n--- Processing Pair: {node_i_id} ({i}) -> {node_j_id} ({j}) ---\")\n",
    "        print(f\"Cause: {node_i_str}\")\n",
    "        print(f\"Effect: {node_j_str}\")\n",
    "\n",
    "        H_verified_list: List[str] = []\n",
    "        all_ranked_lists_for_pair: List[List[int]] = []\n",
    "\n",
    "        # --- 1. Hypothesis Generation ---\n",
    "        hypotheses = llm_generate_hypotheses(node_i_str, node_j_str)\n",
    "        if not hypotheses:\n",
    "            print(\"Skipped: No hypotheses generated.\")\n",
    "            final_output_map[(i, j)] = {\n",
    "                \"verified_causal_hypothesis\": [],\n",
    "                \"evidence_text\": \"No strong, stable evidence found.\",\n",
    "            }\n",
    "            time.sleep(DELAY_SECONDS)\n",
    "            continue\n",
    "\n",
    "        # --- 2. Verification and Dual Filtering ---\n",
    "        for hl in hypotheses:\n",
    "            # 2a. Embed hypothesis once\n",
    "            v_hl = embedder.encode([hl])[0].reshape(1, -1)\n",
    "\n",
    "            # --- Sentence-based retrieval ---\n",
    "            D_rag, I_rag = idx_doc.search(v_hl, K_RAG)\n",
    "            sentence_snippets = \" \".join([sentences[idx] for idx in I_rag[0]])\n",
    "\n",
    "            # --- Relation-based retrieval (if available) ---\n",
    "            relation_snippets = \"\"\n",
    "            if idx_rel is not None and rel_chunks is not None and VR is not None:\n",
    "                D_rel, I_rel = idx_rel.search(v_hl, K_RAG)\n",
    "                relation_snippets = \" \".join(\n",
    "                    [rel_chunks[k] for k in I_rel[0] if 0 <= k < len(rel_chunks)]\n",
    "                )\n",
    "\n",
    "            # Combine both views into a single evidence text for verification\n",
    "            if relation_snippets:\n",
    "                combined_evidence = (\n",
    "                    \"SENTENCE EVIDENCE:\\n\" + sentence_snippets +\n",
    "                    \"\\n\\nRELATION EVIDENCE:\\n\" + relation_snippets\n",
    "                )\n",
    "            else:\n",
    "                combined_evidence = sentence_snippets\n",
    "\n",
    "            # 2b. Estimate support + semantic entropy\n",
    "            p_support, h_semantic = estimate_semantic_entropy(hl, combined_evidence)\n",
    "\n",
    "            # 2c. Dual Filtering: Check thresholds\n",
    "            if p_support > TAU_SUPPORT and h_semantic < TAU_ENTROPY:\n",
    "                H_verified_list.append(hl)\n",
    "\n",
    "                # --- 3. Adaptive Pooling and MMR (For verified hypotheses only) ---\n",
    "                score_h = p_support  # Use p_support as the score for adaptive pooling\n",
    "\n",
    "                # Adaptive Pool Size: kpool = kbase + (1 - score) * kexpansion\n",
    "                k_pool = int(K_BASE + (1.0 - score_h) * K_EXPANSION)\n",
    "                k_pool = max(K_RAG, k_pool)  # Ensure pool is at least k_RAG\n",
    "\n",
    "                # Retrieve larger pool from sentence index (for final evidence selection)\n",
    "                D_pool, I_pool = idx_doc.search(v_hl, k_pool)\n",
    "                pool_vectors = VD[I_pool[0]]\n",
    "\n",
    "                # MMR Reranking\n",
    "                final_indices_h = mmr_rerank(v_hl, pool_vectors, k_rag=K_RAG)\n",
    "\n",
    "                # Store indices (mapped back to global sentence IDs)\n",
    "                global_indices_h = [I_pool[0][idx] for idx in final_indices_h]\n",
    "                all_ranked_lists_for_pair.append(global_indices_h)\n",
    "\n",
    "            print(\n",
    "                f\"  Claim Verified: {p_support:.2f}/{TAU_SUPPORT:.2f} (Support) | \"\n",
    "                f\"{h_semantic:.2f}/{TAU_ENTROPY:.2f} (Entropy) -> \"\n",
    "                f\"{'KEPT' if hl in H_verified_list else 'REJECTED'}\"\n",
    "            )\n",
    "\n",
    "        # --- 4. Final Rank Fusion (RRF) over sentence indices ---\n",
    "        if all_ranked_lists_for_pair:\n",
    "            final_indices_total = reciprocal_rank_fusion(all_ranked_lists_for_pair, k_final=3)\n",
    "            evidence_text = \"\\n\".join([sentences[idx] for idx in final_indices_total])\n",
    "        else:\n",
    "            final_indices_total = []\n",
    "            evidence_text = \"No strong, stable evidence found.\"\n",
    "\n",
    "        # --- 5. Final Output Format ---\n",
    "        final_output_map[(i, j)] = {\n",
    "            \"verified_causal_hypothesis\": H_verified_list,\n",
    "            \"evidence_text\": evidence_text,\n",
    "        }\n",
    "\n",
    "        # One delay per pair to avoid hammering the API\n",
    "        time.sleep(DELAY_SECONDS)\n",
    "\n",
    "    # Finish progress bar line cleanly\n",
    "    if total_pairs > 0:\n",
    "        print_progress(total_pairs)\n",
    "        print()  # newline\n",
    "\n",
    "    return final_output_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_causal_map = run_causal_verification_pipeline(\n",
    "    candidate_set=E_prior,          # still 0-indexed ints\n",
    "    N_map=entities_by_id,      # dict: \"N1\" -> rich node dict or name string\n",
    "    sentences=sentences_list,  # sentence chunks\n",
    "    relations=relations,       # full relations list (optional but recommended)\n",
    ")\n",
    "\n",
    "pairwise_structural_context_retrieval = final_causal_map\n",
    "\n",
    "## Save the above\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_final_causal_map(final_causal_map,\n",
    "                          base_path=\"extracted_output/cocad/w_direct\"):\n",
    "    \"\"\"\n",
    "    Saves final_causal_map as a pickle file.\n",
    "    Location:\n",
    "      extracted_output/cocad/w_direct/final_causal_map.pkl\n",
    "    \"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(base_path, \"final_causal_map.pkl\")\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(final_causal_map, f)\n",
    "\n",
    "    print(f\"[SAVE] final_causal_map saved → {save_path}\")\n",
    "\n",
    "# Save map\n",
    "save_final_causal_map(pairwise_structural_context_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the above \n",
    "\n",
    "def load_final_causal_map(base_path=\"extracted_output/cocad/w_direct\"):\n",
    "    \"\"\"\n",
    "    Loads final_causal_map from pickle file.\n",
    "    \"\"\"\n",
    "    load_path = os.path.join(base_path, \"final_causal_map.pkl\")\n",
    "\n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"No file found at: {load_path}\")\n",
    "\n",
    "    with open(load_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"[LOAD] final_causal_map loaded ← {load_path}\")\n",
    "    return data\n",
    "\n",
    "pairwise_structural_context_retrieval = load_final_causal_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe92ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pairwise_structural_context_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (10, 2)\n",
    "\n",
    "for k, v in pairwise_structural_context_retrieval.items():\n",
    "    # Convert both elements of the key to int for safe comparison\n",
    "    if int(k[0]) == target[0] and int(k[1]) == target[1]:\n",
    "        print(\"Key:\", k)\n",
    "        print(\"Value:\", v)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f0b5",
   "metadata": {},
   "source": [
    "##### evidence_text vs verified_causal_hypothesis\n",
    "\n",
    "- evidence_text = The actual text snippets retrieved from the document that support that causal hypothesis.\n",
    "\n",
    "- verified_causal_hypothesis = A sentence generated by the LLM saying what causal relationship might hold between node i and node j — if the model believes it is plausible AND well-supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4403a",
   "metadata": {},
   "source": [
    "#### B. Counterfactual Reasoning (CoT Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81121651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, Tuple, Any, List, Optional\n",
    "\n",
    "from google.genai import types\n",
    "from google.genai import errors as genai_errors\n",
    "\n",
    "# Uses your global:\n",
    "#   client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "#   PRIMARY_MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "# ------------------------------\n",
    "# HYPERPARAMETERS\n",
    "# ------------------------------\n",
    "NS_SAMPLES_CF = 5              # Ns counterfactual samples\n",
    "CF_TEMPERATURE = 0.7\n",
    "CF_DELAY_SECONDS = 5         # Avoid rate-limit hammering\n",
    "\n",
    "# ------------------------------\n",
    "# MODEL ROTATION (same pool as earlier)\n",
    "# ------------------------------\n",
    "MODEL_CANDIDATES = [\n",
    "    PRIMARY_MODEL,             # \"gemini-2.5-flash-lite\"\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "]\n",
    "\n",
    "MODEL: str = MODEL_CANDIDATES[0]\n",
    "CURRENT_MODEL_INDEX: int = 0\n",
    "EXHAUSTED_MODELS = set()\n",
    "\n",
    "\n",
    "def _switch_to_next_model():\n",
    "    \"\"\"\n",
    "    Mark current MODEL as exhausted and switch to the next available one.\n",
    "    Raises if all candidates are exhausted.\n",
    "    \"\"\"\n",
    "    global MODEL, CURRENT_MODEL_INDEX\n",
    "\n",
    "    EXHAUSTED_MODELS.add(MODEL)\n",
    "\n",
    "    for i in range(len(MODEL_CANDIDATES)):\n",
    "        idx = (CURRENT_MODEL_INDEX + 1 + i) % len(MODEL_CANDIDATES)\n",
    "        candidate = MODEL_CANDIDATES[idx]\n",
    "        if candidate not in EXHAUSTED_MODELS:\n",
    "            MODEL = candidate\n",
    "            CURRENT_MODEL_INDEX = idx\n",
    "            print(f\"[LLM] (CF) Switching to backup model: {MODEL}\")\n",
    "            return\n",
    "\n",
    "    raise RuntimeError(\"All configured models appear exhausted/quota-limited for today (counterfactual).\")\n",
    "\n",
    "\n",
    "def safe_llm_text(\n",
    "    prompt: str,\n",
    "    temperature: float = CF_TEMPERATURE,\n",
    "    max_retries: int = 3,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Simple text-generation wrapper with:\n",
    "      - structured retries\n",
    "      - model rotation over MODEL_CANDIDATES on quota / rate-limit errors.\n",
    "    \"\"\"\n",
    "    last_exc: Optional[Exception] = None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        from_model = MODEL\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            # no response_schema here; we get plain text\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            print(f\"[LLM] (CF) Call attempt {attempt} with {from_model}\")\n",
    "            resp = client.models.generate_content(\n",
    "                model=from_model,\n",
    "                contents=prompt,\n",
    "                config=config,\n",
    "            )\n",
    "            return getattr(resp, \"text\", \"\") or \"\"\n",
    "        except genai_errors.ClientError as e:\n",
    "            msg = str(e)\n",
    "            if (\n",
    "                \"RESOURCE_EXHAUSTED\" in msg\n",
    "                or \"429\" in msg\n",
    "                or \"exceeded your current quota\" in msg.lower()\n",
    "            ):\n",
    "                print(f\"[LLM] (CF) Model {from_model} hit quota / rate limit: {msg}\")\n",
    "                last_exc = e\n",
    "                try:\n",
    "                    _switch_to_next_model()\n",
    "                except RuntimeError as switch_err:\n",
    "                    print(\"[LLM] (CF) No backup models left.\")\n",
    "                    raise switch_err\n",
    "                time.sleep(CF_DELAY_SECONDS)\n",
    "                continue\n",
    "\n",
    "            raise\n",
    "\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"safe_llm_text (CF) failed unexpectedly.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. PARSER FOR LLM OUTPUT\n",
    "# =========================================================\n",
    "def _parse_reasoning_and_score(raw_text: str) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Parses the LLM output into (reasoning_str, score_float).\n",
    "\n",
    "    Expected soft pattern:\n",
    "        <reasoning text...>\n",
    "        SCORE: <float>\n",
    "\n",
    "    If parsing fails, returns score = 0.0.\n",
    "    \"\"\"\n",
    "    if raw_text is None:\n",
    "        return \"\", 0.0\n",
    "\n",
    "    text = raw_text.strip()\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "\n",
    "    reasoning = text\n",
    "    score = 0.0\n",
    "\n",
    "    for ln in reversed(lines):\n",
    "        if ln.upper().startswith(\"SCORE\"):\n",
    "            parts = ln.split(\":\")\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    score = float(parts[1].strip())\n",
    "                except ValueError:\n",
    "                    score = 0.0\n",
    "\n",
    "            # Reasoning = text before SCORE line\n",
    "            idx = text.rfind(ln)\n",
    "            if idx != -1:\n",
    "                reasoning = text[:idx].strip()\n",
    "            break\n",
    "\n",
    "    return reasoning, score\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. COUNTERFACTUAL REASONING\n",
    "# =========================================================\n",
    "def counterfactual_reasoning_for_pairs(\n",
    "    pairs: List[Tuple[int, int]],\n",
    "    context_map: Dict[Tuple[int, int], str],   # evidence_text from earlier stage\n",
    "    ns_samples: int = NS_SAMPLES_CF,\n",
    "    temperature: float = CF_TEMPERATURE,\n",
    "    delay_seconds: int = CF_DELAY_SECONDS,\n",
    ") -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each pair (i, j), retrieve context_map[(i, j)] = evidence_text\n",
    "    from earlier RAG verification stage.\n",
    "\n",
    "    Then run Ns LLM samples asking:\n",
    "        \"If i were NOT present, what is the effect on j?\"\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        (i, j): {\n",
    "           \"context\": <evidence_text>,\n",
    "           \"reasoning_strings\": [...],\n",
    "           \"causal_strength_scores\": [...]\n",
    "        }\n",
    "      }\n",
    "    \"\"\"\n",
    "    results: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "\n",
    "    total_pairs = len(pairs)\n",
    "    if total_pairs == 0:\n",
    "        print(\"No pairs provided for counterfactual reasoning.\")\n",
    "        return results\n",
    "\n",
    "    def print_progress(current_idx: int):\n",
    "        \"\"\"Simple pair-level progress bar.\"\"\"\n",
    "        progress = current_idx / total_pairs\n",
    "        bar_len = 30\n",
    "        filled = int(bar_len * progress)\n",
    "        bar = \"#\" * filled + \"-\" * (bar_len - filled)\n",
    "        print(\n",
    "            f\"\\r[CF] Progress {current_idx}/{total_pairs} [{bar}] {progress * 100:5.1f}%\",\n",
    "            end=\"\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    print(f\"Starting counterfactual reasoning for {total_pairs} pairs...\")\n",
    "\n",
    "    for pair_idx, (i, j) in enumerate(pairs, start=1):\n",
    "        key = (int(i), int(j))\n",
    "\n",
    "        context_txt = context_map.get(key, \"\")\n",
    "        print(f\"\\n\\n=== Counterfactual for (i={i}, j={j}) ===\")\n",
    "        print(f\"Context length: {len(context_txt)} chars\")\n",
    "\n",
    "        reasoning_list: List[str] = []\n",
    "        scores_list: List[float] = []\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Build prompt with evidence_text as context\n",
    "        # --------------------------------------------------\n",
    "        base_prompt = f\"\"\"\n",
    "You are a precise causal reasoning assistant.\n",
    "\n",
    "Context (evidence supporting the causal relation i -> j):\n",
    "\\\"\\\"\\\"{context_txt}\\\"\\\"\\\"\n",
    "\n",
    "We are analyzing the counterfactual influence of i on j.\n",
    "\n",
    "1. Reasoning:\n",
    "   If i were NOT present, what would happen to j?\n",
    "   - Use only the information in the context.\n",
    "   - Think step-by-step.\n",
    "   - Explicitly state whether the causal impact is strong, weak, or negligible.\n",
    "\n",
    "2. After your explanation, on a new line write:\n",
    "   SCORE: <a single number between 0.0 and 1.0>\n",
    "\n",
    "Where:\n",
    "- SCORE ≈ 0.0 → almost no causal effect of i on j.\n",
    "- SCORE ≈ 1.0 → strong causal effect of i on j.\n",
    "\n",
    "Return only the explanation + the SCORE line.\n",
    "\"\"\".strip()\n",
    "\n",
    "        # ======================================================\n",
    "        #  Run Ns LLM samples\n",
    "        # ======================================================\n",
    "        for s_idx in range(ns_samples):\n",
    "            print(f\"  -> Sample {s_idx+1}/{ns_samples} ...\")\n",
    "\n",
    "            try:\n",
    "                raw_text = safe_llm_text(\n",
    "                    prompt=base_prompt,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "\n",
    "                reasoning_s, score_s = _parse_reasoning_and_score(raw_text)\n",
    "\n",
    "                reasoning_list.append(reasoning_s)\n",
    "                scores_list.append(score_s)\n",
    "\n",
    "                print(f\"     SCORE parsed = {score_s:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     [Warning] LLM call failed: {e}\")\n",
    "                reasoning_list.append(\"\")\n",
    "                scores_list.append(0.0)\n",
    "\n",
    "            # Respect rate-limits\n",
    "            time.sleep(delay_seconds)\n",
    "\n",
    "        # ======================================================\n",
    "        # Store results\n",
    "        # ======================================================\n",
    "        results[key] = {\n",
    "            \"context\": context_txt,\n",
    "            \"reasoning_strings\": reasoning_list,\n",
    "            \"causal_strength_scores\": scores_list,\n",
    "        }\n",
    "\n",
    "        # Update pair-level progress bar\n",
    "        print_progress(pair_idx)\n",
    "\n",
    "    # Final newline for clean console\n",
    "    print()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81024b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(pairwise_structural_context_retrieval.keys())\n",
    "\n",
    "context_map = {\n",
    "    k: pairwise_structural_context_retrieval[k][\"evidence_text\"]\n",
    "    for k in pairwise_structural_context_retrieval\n",
    "}\n",
    "\n",
    "cot_results_counterfactual = counterfactual_reasoning_for_pairs(pairs, context_map)\n",
    "\n",
    "# Save the above:\n",
    "import os, pickle\n",
    "\n",
    "save_path = \"extracted_output/cocad/w_direct/cot_results_counterfactual.pkl\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(cot_results_counterfactual, f)\n",
    "\n",
    "print(\"Saved to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the above\n",
    "import pickle\n",
    "\n",
    "load_path = \"extracted_output/cocad/w_direct/cot_results_counterfactual.pkl\"\n",
    "\n",
    "with open(load_path, \"rb\") as f:\n",
    "    cot_results_counterfactual = pickle.load(f)\n",
    "\n",
    "print(\"Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bc741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cot_results_counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedder for semantic coherence\n",
    "coh_embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def compute_llm_metrics(cot_results_counterfactual):\n",
    "    \"\"\"\n",
    "    Takes your counterfactual reasoning output and computes:\n",
    "      - llm_mean_ensemble_score\n",
    "      - llm_variance_ensemble_score\n",
    "      - llm_semantic_coherence_score (Rcoh)\n",
    "    \"\"\"\n",
    "    w_direct_llm_results = {}\n",
    "\n",
    "    for pair, data in cot_results_counterfactual.items():\n",
    "        scores = data[\"causal_strength_scores\"]        # list of floats\n",
    "        reasons = data[\"reasoning_strings\"]            # list of strings\n",
    "        Ns = len(scores)\n",
    "\n",
    "        # ------------ Mean (µLLM) ------------\n",
    "        mean_score = float(np.mean(scores))\n",
    "\n",
    "        # ------------ Variance (σ²LLM) ------------\n",
    "        variance_score = float(np.var(scores))\n",
    "\n",
    "        # ------------ Semantic Coherence (Rcoh) ------------\n",
    "        # Embed each reasoning string\n",
    "        embeddings = coh_embedder.encode(\n",
    "            reasons, convert_to_numpy=True, normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Compute pairwise cosine similarity matrix\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Extract upper triangle (p < q)\n",
    "        total_pairs = Ns * (Ns - 1) / 2\n",
    "        if total_pairs > 0:\n",
    "            upper_tri_vals = []\n",
    "            for p in range(Ns):\n",
    "                for q in range(p + 1, Ns):\n",
    "                    upper_tri_vals.append(sim_matrix[p, q])\n",
    "\n",
    "            semantic_coherence = float(np.mean(upper_tri_vals))\n",
    "        else:\n",
    "            semantic_coherence = 0.0\n",
    "\n",
    "        # ------------ Store ------------\n",
    "        w_direct_llm_results[pair] = {\n",
    "            \"llm_mean_ensemble_score\": round(mean_score, 6),\n",
    "            \"llm_variance_ensemble_score\": round(variance_score, 6),\n",
    "            \"llm_semantic_coherence_score\": round(semantic_coherence, 6),\n",
    "        }\n",
    "\n",
    "    return w_direct_llm_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_direct_llm_results = compute_llm_metrics(cot_results_counterfactual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the above\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ensure output directory exists\n",
    "save_path = \"extracted_output/cocad/w_direct\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(save_path, \"w_direct_llm_results.pkl\")\n",
    "\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(w_direct_llm_results, f)\n",
    "\n",
    "print(f\"Saved w_direct_llm_results → {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd61568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the above\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "file_path = \"extracted_output/cocad/w_direct/w_direct_llm_results.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    w_direct_llm_results_loaded = pickle.load(f)\n",
    "\n",
    "print(\"Loaded w_direct_llm_results:\")\n",
    "print(type(w_direct_llm_results_loaded))\n",
    "print(len(w_direct_llm_results_loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ab7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(w_direct_llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# Extract each metric across all pairs\n",
    "mean_scores = [v[\"llm_mean_ensemble_score\"] for v in w_direct_llm_results.values()]\n",
    "coherence_scores = [v[\"llm_semantic_coherence_score\"] for v in w_direct_llm_results.values()]\n",
    "variance_scores = [v[\"llm_variance_ensemble_score\"] for v in w_direct_llm_results.values()]\n",
    "\n",
    "results = {\n",
    "    # Averages and variances\n",
    "    \"mean_score_avg\": float(np.mean(mean_scores)),\n",
    "    \"mean_score_var\": float(np.var(mean_scores, ddof=0)),\n",
    "\n",
    "    \"coherence_score_avg\": float(np.mean(coherence_scores)),\n",
    "    \"coherence_score_var\": float(np.var(coherence_scores, ddof=0)),\n",
    "\n",
    "    \"variance_score_avg\": float(np.mean(variance_scores)),\n",
    "    \"variance_score_var\": float(np.var(variance_scores, ddof=0)),\n",
    "\n",
    "    # Medians\n",
    "    \"mean_score_median\": float(np.median(mean_scores)),\n",
    "    \"coherence_score_median\": float(np.median(coherence_scores)),\n",
    "    \"variance_score_median\": float(np.median(variance_scores)),\n",
    "\n",
    "    # 75th percentile cutoffs (score required to be in top 25%)\n",
    "    \"mean_score_top40_cutoff\": float(np.percentile(mean_scores, 60)),\n",
    "    \"coherence_score_top40_cutoff\": float(np.percentile(coherence_scores, 60)),\n",
    "    \"variance_score_top40_cutoff\": float(np.percentile(variance_scores, 60)),\n",
    "}\n",
    "\n",
    "pprint.pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86305935",
   "metadata": {},
   "source": [
    "### (iii).  $f_{CI}$ and $f_{llmconfounder}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408cf16",
   "metadata": {},
   "source": [
    "#### A. Finding our confounders ($A_{set-confounded}$)\n",
    "\n",
    "Now to find a \"Potential\" set of confounders you need a okish reliable asymmetric signal (As of now our only such think is the previous step f_llm's counterfactual reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple, List, Set, Any\n",
    "\n",
    "import numpy as np\n",
    "import dateparser\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Time parsing utilities using \"dateparser\"\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def normalize_time_to_ordinal(time_str: str) -> int | None:\n",
    "    \n",
    "    if not time_str or not time_str.strip():\n",
    "        return None\n",
    "\n",
    "    text = time_str.strip()\n",
    "\n",
    "    text = re.sub(\n",
    "        r\"\\b(around|about|approximately|circa|around the|early|late|mid|beginning of|end of)\\b\",\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    text = text.strip(\" ,.;\")\n",
    "\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    dt = dateparser.parse(\n",
    "        text,\n",
    "        settings={\n",
    "            \"PREFER_DATES_FROM\": \"past\",\n",
    "            \"RETURN_AS_TIMEZONE_AWARE\": False,\n",
    "        },\n",
    "    )\n",
    "    if dt is None:\n",
    "        return None\n",
    "\n",
    "    return dt.date().toordinal()\n",
    "\n",
    "\n",
    "\n",
    "def build_node_time_ordinals(entities: List[Dict[str, Any]]) -> Dict[int, int | None]:\n",
    "    \"\"\"\n",
    "    From the rich `entities` list, build:\n",
    "\n",
    "        node_time_ordinals[idx] = ordinal or None\n",
    "\n",
    "    where idx is the numeric node index (1-based, from 'N1', 'N2', ...).\n",
    "    \"\"\"\n",
    "    node_time_ordinals: Dict[int, int | None] = {}\n",
    "\n",
    "    for ent in entities:\n",
    "        node_id = ent.get(\"id\", \"\")\n",
    "        if not node_id or not node_id.startswith(\"N\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            idx = int(node_id[1:])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        time_str = ent.get(\"time\")\n",
    "        ordinal = normalize_time_to_ordinal(time_str) if time_str else None\n",
    "        node_time_ordinals[idx] = ordinal\n",
    "\n",
    "    return node_time_ordinals\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Build A_quick_causal from LLM scores + temporal gating\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_quick_causal_adjacency(\n",
    "    entities: List[Dict[str, Any]],\n",
    "    w_direct_llm_results: Dict[Tuple[int, int], Dict[str, float]],\n",
    "    tau_score: float = 0.7,\n",
    "    tau_coh: float = 0.9,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build the directed, asymmetric adjacency matrix A_quick_causal (N+1 x N+1)\n",
    "    using:\n",
    "\n",
    "      - μ_LLM(i, j) = w_direct_llm_results[(i,j)][\"llm_mean_ensemble_score\"]\n",
    "      - R_coh(i, j) = w_direct_llm_results[(i,j)][\"llm_semantic_coherence_score\"]\n",
    "\n",
    "    AND enforce a temporal constraint:\n",
    "      - if both i and j have parseable times and time(i) > time(j),\n",
    "        then we forbid edge i -> j (cannot cause the past).\n",
    "\n",
    "    Returns:\n",
    "        A_quick_causal: np.ndarray of shape (N+1, N+1), dtype=int8\n",
    "        (index 0 is unused; nodes are 1..N)\n",
    "    \"\"\"\n",
    "    # Determine how many nodes we have from the entities list\n",
    "    num_nodes = max(int(ent[\"id\"][1:]) for ent in entities)\n",
    "    A_quick_causal = np.zeros((num_nodes + 1, num_nodes + 1), dtype=np.int8)\n",
    "\n",
    "    # Precompute node time ordinals\n",
    "    node_time_ordinals = build_node_time_ordinals(entities)\n",
    "\n",
    "    for (i, j), scores in w_direct_llm_results.items():\n",
    "        mu = float(scores.get(\"llm_mean_ensemble_score\", 0.0))\n",
    "        r_coh = float(scores.get(\"llm_semantic_coherence_score\", 0.0))\n",
    "\n",
    "        # Threshold on score & coherence\n",
    "        if mu <= tau_score or r_coh <= tau_coh:\n",
    "            continue  # do not include this edge at all\n",
    "\n",
    "        # Temporal gating (if we have both times)\n",
    "        t_i = node_time_ordinals.get(i)\n",
    "        t_j = node_time_ordinals.get(j)\n",
    "\n",
    "        if t_i is not None and t_j is not None:\n",
    "            # If i's time is strictly AFTER j's time, i -> j is forbidden.\n",
    "            if t_i > t_j:\n",
    "                # e.g., event on July 15 should not cause event on July 2\n",
    "                continue\n",
    "\n",
    "        # Passed all filters → keep directed edge i -> j\n",
    "        A_quick_causal[i, j] = 1\n",
    "\n",
    "    return A_quick_causal\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Confounder search via BFS on A_quick_causal^T\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_transpose_adj_list(A_quick_causal: np.ndarray) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Build adjacency lists for the transpose graph A^T:\n",
    "\n",
    "      If A_quick_causal[u, v] == 1 (u -> v in original),\n",
    "      then in the transpose we store an edge v -> u.\n",
    "\n",
    "    Returns:\n",
    "        adj_T: dict[node] -> list of parents in the original graph\n",
    "    \"\"\"\n",
    "    num_nodes = A_quick_causal.shape[0] - 1  # ignore index 0\n",
    "    adj_T: Dict[int, List[int]] = {i: [] for i in range(1, num_nodes + 1)}\n",
    "\n",
    "    # Only iterate over 1..N (skip row/col 0)\n",
    "    for u in range(1, num_nodes + 1):\n",
    "        row = A_quick_causal[u]\n",
    "        # find all v where u -> v\n",
    "        targets = np.where(row == 1)[0]\n",
    "        for v in targets:\n",
    "            if v == 0:\n",
    "                continue\n",
    "            # In transpose: v -> u\n",
    "            adj_T[v].append(u)\n",
    "\n",
    "    return adj_T\n",
    "\n",
    "\n",
    "def bfs_ancestors(start: int, adj_T: Dict[int, List[int]], max_hops: int) -> Set[int]:\n",
    "    \"\"\"\n",
    "    BFS on the transpose adjacency from 'start', up to 'max_hops' steps.\n",
    "\n",
    "    This finds all nodes k such that k -> ... -> start in the original graph.\n",
    "\n",
    "    Returns:\n",
    "        ancestors: set of node indices k (excluding 'start').\n",
    "    \"\"\"\n",
    "    visited: Set[int] = {start}\n",
    "    frontier: List[int] = [start]\n",
    "    ancestors: Set[int] = set()\n",
    "\n",
    "    hops = 0\n",
    "    while frontier and hops < max_hops:\n",
    "        next_frontier: List[int] = []\n",
    "        for node in frontier:\n",
    "            for parent in adj_T.get(node, []):\n",
    "                if parent not in visited:\n",
    "                    visited.add(parent)\n",
    "                    ancestors.add(parent)\n",
    "                    next_frontier.append(parent)\n",
    "        frontier = next_frontier\n",
    "        hops += 1\n",
    "\n",
    "    return ancestors\n",
    "\n",
    "\n",
    "def compute_potential_confounders(\n",
    "    A_quick_causal: np.ndarray,\n",
    "    max_hops: int = 3,\n",
    ") -> Dict[Tuple[int, int], List[int]]:\n",
    "    \"\"\"\n",
    "    For every directed edge (i, j) in A_quick_causal (i.e., A[i,j] == 1),\n",
    "    compute the set of potential confounders:\n",
    "\n",
    "        M_set_confound(i, j) = Ancestors_i ∩ Ancestors_j\n",
    "\n",
    "    where ancestors are computed on the transpose graph up to 'max_hops'.\n",
    "\n",
    "    Returns:\n",
    "        M_set_potential_confounder: dict mapping\n",
    "            (i, j) -> sorted list of node indices k.\n",
    "    \"\"\"\n",
    "    num_nodes = A_quick_causal.shape[0] - 1\n",
    "    adj_T = build_transpose_adj_list(A_quick_causal)\n",
    "\n",
    "    M_set_potential_confounder: Dict[Tuple[int, int], List[int]] = {}\n",
    "\n",
    "    # Iterate over all directed edges in A_quick_causal\n",
    "    for i in range(1, num_nodes + 1):\n",
    "        row = A_quick_causal[i]\n",
    "        targets = np.where(row == 1)[0]  # all j where i -> j\n",
    "        for j in targets:\n",
    "            if j == 0:\n",
    "                continue\n",
    "\n",
    "            # Get ancestor sets for i and j\n",
    "            ancestors_i = bfs_ancestors(i, adj_T, max_hops=max_hops)\n",
    "            ancestors_j = bfs_ancestors(j, adj_T, max_hops=max_hops)\n",
    "\n",
    "            confounders = (ancestors_i & ancestors_j) - {i, j}\n",
    "            M_set_potential_confounder[(i, j)] = sorted(confounders)\n",
    "\n",
    "    return M_set_potential_confounder\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. High-level helper tying everything together\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_quick_causal_and_confounders(\n",
    "    entities: List[Dict[str, Any]],\n",
    "    w_direct_llm_results: Dict[Tuple[int, int], Dict[str, float]],\n",
    "    tau_score: float = 0.7,\n",
    "    tau_coh: float = 0.9,\n",
    "    max_hops: int = 3,\n",
    ") -> Tuple[np.ndarray, Dict[Tuple[int, int], List[int]]]:\n",
    "    \"\"\"\n",
    "    Main entrypoint:\n",
    "\n",
    "    1. Build A_quick_causal with:\n",
    "         - LLM score threshold (tau_score)\n",
    "         - coherence threshold (tau_coh)\n",
    "         - temporal constraint (cause time <= effect time when both known)\n",
    "\n",
    "    2. On this directed graph, compute potential confounders for each edge\n",
    "       via BFS on the transpose up to 'max_hops' hops.\n",
    "\n",
    "    Returns:\n",
    "        A_quick_causal, M_set_potential_confounder\n",
    "    \"\"\"\n",
    "    A_quick_causal = build_quick_causal_adjacency(\n",
    "        entities=entities,\n",
    "        w_direct_llm_results=w_direct_llm_results,\n",
    "        tau_score=tau_score,\n",
    "        tau_coh=tau_coh,\n",
    "    )\n",
    "\n",
    "    M_set_potential_confounder = compute_potential_confounders(\n",
    "        A_quick_causal,\n",
    "        max_hops=max_hops,\n",
    "    )\n",
    "\n",
    "    return A_quick_causal, M_set_potential_confounder\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Example usage (you would plug in your real data here)\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: plug your `entities` list and `w_direct_llm_results` here.\n",
    "    # entities = [...]  # from entities.json\n",
    "    # w_direct_llm_results = {...}  # built from your CoT counterfactual stage\n",
    "\n",
    "    # A_quick_causal, M_conf = build_quick_causal_and_confounders(\n",
    "    #     entities,\n",
    "    #     w_direct_llm_results,\n",
    "    #     tau_score=0.7,\n",
    "    #     tau_coh=0.9,\n",
    "    #     max_hops=3,\n",
    "    # )\n",
    "    #\n",
    "    # print(\"A_quick_causal shape:\", A_quick_causal.shape)\n",
    "    # print(\"Potential confounders per edge:\")\n",
    "    # for (i, j), ks in M_conf.items():\n",
    "    #     print(f\"({i}, {j}): {ks}\")\n",
    "    pass\n",
    "\n",
    "#     entities: List[Dict[str, Any]],\n",
    "#     w_direct_llm_results: Dict[Tuple[int, int], Dict[str, float]],\n",
    "#     tau_score: float = 0.7,\n",
    "#     tau_coh: float = 0.9,\n",
    "#     max_hops: int = 3,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28aaaf7",
   "metadata": {},
   "source": [
    "- A_quick_causal = A_quick_causal is a final directed causal adjacency matrix.\n",
    "\n",
    "  It is built only from LLM counterfactual results (μ_LLM and R_coh).\n",
    "\n",
    "  Its only used for ancestor traversal for confounder detection.\n",
    "\n",
    "- M_set_potential_confounder = For every directed candidate edge i → j, you find the set of nodes k such that.\n",
    "\n",
    "  Different for each edge — completely pair-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes: e.g. from your entities list\n",
    "num_nodes = max(int(ent[\"id\"][1:]) for ent in nodes)\n",
    "\n",
    "A_quick_causal, M_set_potential_confounder = build_quick_causal_and_confounders(\n",
    "    nodes,\n",
    "    w_direct_llm_results,\n",
    "    tau_score=0.57, # LLM score\n",
    "    tau_coh=0.89, # Coh score\n",
    "    max_hops=3,\n",
    ")\n",
    "\n",
    "# Example:\n",
    "print(\"Directed edges in A_quick_causal:\")\n",
    "num_nodes = A_quick_causal.shape[0] - 1  # ignore index 0\n",
    "for i in range(1, num_nodes + 1):\n",
    "    for j in range(1, num_nodes + 1):\n",
    "        if A_quick_causal[i, j] == 1:\n",
    "            print(f\"{i} -> {j}\")\n",
    "\n",
    "print(\"\\nPotential confounders per edge:\")\n",
    "for (i, j), confs in M_set_potential_confounder.items():\n",
    "    print(f\"({i}, {j}): {confs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87369cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the above\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "save_dir = \"extracted_output/cocad/w_direct\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save adjacency matrix\n",
    "with open(os.path.join(save_dir, \"A_quick_causal.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(A_quick_causal, f)\n",
    "\n",
    "# Save confounder map\n",
    "with open(os.path.join(save_dir, \"M_set_potential_confounder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(M_set_potential_confounder, f)\n",
    "\n",
    "print(\"Saved A_quick_causal and M_set_potential_confounder into:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the above\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "load_dir = \"extracted_output/cocad/w_direct\"\n",
    "\n",
    "with open(os.path.join(load_dir, \"A_quick_causal.pkl\"), \"rb\") as f:\n",
    "    A_quick_causal = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(load_dir, \"M_set_potential_confounder.pkl\"), \"rb\") as f:\n",
    "    M_set_potential_confounder = pickle.load(f)\n",
    "\n",
    "print(\"Loaded A_quick_causal:\", type(A_quick_causal), A_quick_causal.shape)\n",
    "print(\"Example edges:\")\n",
    "num_nodes = A_quick_causal.shape[0] - 1\n",
    "for i in range(1, num_nodes + 1):\n",
    "    for j in range(1, num_nodes + 1):\n",
    "        if A_quick_causal[i, j] == 1:\n",
    "            print(f\"{i} -> {j}\")\n",
    "\n",
    "print(\"\\nLoaded confounder map size:\", len(M_set_potential_confounder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(M_set_potential_confounder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = max(int(ent[\"id\"][1:]) for ent in nodes)\n",
    "\n",
    "pprint.pprint(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce06618",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(entities_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fbc39",
   "metadata": {},
   "source": [
    "#### B. Dual Check on confounders\n",
    "\n",
    "Now we have (i,j) pairs which possibly have a potential confounder, so we just need to evaluate for (i,j) pairs in the A_quick_causal.\n",
    "\n",
    "We want a mathematically principled way to evaluate whether the link $i \\rightarrow j$ is still meaningful after removing the influence of all potential confounders $M_{set-confound}$(i,j).\n",
    "\n",
    "Above for a given pair (i,j) remove all its corresponding confounders at once then calculate the score of the direct link $i \\rightarrow j$.\n",
    "\n",
    "Above for a given link (i,j) we can get 3 types of scores:\n",
    "- Positive $f_{CI}(i,j)$: i and j still move together even after removing all confounders → suggests a true direct causal link likely exists.\n",
    "- Zero $f_{CI}(i,j)$: i and j become independent once confounders are removed → the original link was likely spurious / confounded, not causal.\n",
    "- Negative $f_{CI}(i,j)$: i and j move in opposite directions after conditioning on confounders → indicates a negative / inhibitory causal influence or a suppressed relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72725a0f",
   "metadata": {},
   "source": [
    "##### The statistical check $f_{CI}$\n",
    "\n",
    "(Just added the Ledoit-Wolf and GraphicalLasso (there is an option to toggle between these) for regularization which arent there in the report currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List, Union, Any\n",
    "from sklearn.covariance import LedoitWolf, GraphicalLasso\n",
    "\n",
    "ArrayLike = Union[np.ndarray, \"torch.Tensor\"]  # torch optional\n",
    "\n",
    "\n",
    "def compute_f_ci_scores(\n",
    "    Z: ArrayLike,\n",
    "    M_set_potential_confounder: Dict[Tuple[int, int], List[int]],\n",
    "    lambda_shrink: float = 0.25,           # 0 = no LW, 1 = full LW, 0.2–0.4 is \"slight\"\n",
    "    use_graphical_lasso: bool = False,\n",
    "    graphical_lasso_alpha: float = 0.01,\n",
    ") -> Dict[Tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Compute fCI(i, j) for each directed pair (i, j), using a REGULARIZED precision\n",
    "    matrix and supporting multiple confounders.\n",
    "\n",
    "    V' = {i, j} ∪ M_set_confound(i, j)\n",
    "    Z_V' : dz x n'   (dz = latent dim, n' = |V'|)\n",
    "    Σ_reg : n' x n'  regularized covariance over nodes\n",
    "    Θ     : n' x n'  precision matrix (inverse of Σ_reg or learned via GraphicalLasso)\n",
    "\n",
    "    fCI(i, j) = - Θ_ij / sqrt(Θ_ii * Θ_jj)\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    Z : np.ndarray or torch.Tensor, shape (N_nodes+1, dz) or (N_nodes, dz)\n",
    "        GAE latent embeddings. Row index k is node k (1-based); row 0 may be N0.\n",
    "\n",
    "    M_set_potential_confounder : dict\n",
    "        {\n",
    "          (i, j): [k1, k2, ...]   # all indices are 1-based node IDs\n",
    "        }\n",
    "\n",
    "    lambda_shrink : float in [0, 1]\n",
    "        How much to blend Ledoit–Wolf covariance into the raw covariance:\n",
    "            Σ_reg = (1 - λ) * Σ_raw + λ * Σ_LW\n",
    "\n",
    "    use_graphical_lasso : bool\n",
    "        If True, ignore lambda_shrink and use GraphicalLasso directly to learn Θ.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f_ci_scores : dict\n",
    "        {\n",
    "          (i, j): f_ci_value (float in [-1, 1], or 0.0 if degenerate)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 0. Normalize Z to numpy array ---\n",
    "    if hasattr(Z, \"detach\"):  # torch.Tensor\n",
    "        Z_np = Z.detach().cpu().numpy().astype(np.float64)\n",
    "    else:\n",
    "        Z_np = np.asarray(Z, dtype=np.float64)\n",
    "\n",
    "    num_nodes, dz = Z_np.shape\n",
    "    f_ci_scores: Dict[Tuple[int, int], float] = {}\n",
    "\n",
    "    for (i, j), confounders in M_set_potential_confounder.items():\n",
    "        # Ensure 1-based indices\n",
    "        i_idx = int(i)\n",
    "        j_idx = int(j)\n",
    "\n",
    "        # Skip invalid indices (N0 or out-of-range)\n",
    "        if i_idx <= 0 or j_idx <= 0:\n",
    "            f_ci_scores[(i, j)] = 0.0\n",
    "            continue\n",
    "        if i_idx >= num_nodes or j_idx >= num_nodes:\n",
    "            f_ci_scores[(i, j)] = 0.0\n",
    "            continue\n",
    "\n",
    "        # --- 1. Build V' = {i, j} ∪ confounders ---\n",
    "        V_prime: List[int] = [i_idx, j_idx]\n",
    "        for k in confounders:\n",
    "            k_idx = int(k)\n",
    "            if 0 < k_idx < num_nodes and k_idx not in V_prime:\n",
    "                V_prime.append(k_idx)\n",
    "\n",
    "        n_prime = len(V_prime)\n",
    "\n",
    "        # Need at least i and j, and at least 2 embedding dims\n",
    "        if n_prime < 2 or dz < 2:\n",
    "            f_ci_scores[(i, j)] = 0.0\n",
    "            continue\n",
    "\n",
    "        # --- 2. Extract embeddings for nodes in V' ---\n",
    "        # Z_np: (N_nodes+1, dz)\n",
    "        rows = [idx for idx in V_prime]   # 1-based rows directly\n",
    "        Z_sub = Z_np[rows, :]            # (n_prime, dz)\n",
    "        Z_Vprime = Z_sub.T               # (dz, n_prime) -> rows = embedding dims, cols = nodes\n",
    "\n",
    "        # --- 3. Raw covariance Σ_raw over nodes ---\n",
    "        # np.cov with rowvar=False means: each column is a variable (node).\n",
    "        try:\n",
    "            Sigma_raw = np.cov(Z_Vprime, rowvar=False)  # (n_prime, n_prime)\n",
    "        except Exception:\n",
    "            f_ci_scores[(i, j)] = 0.0\n",
    "            continue\n",
    "\n",
    "        if Sigma_raw.shape != (n_prime, n_prime):\n",
    "            f_ci_scores[(i, j)] = 0.0\n",
    "            continue\n",
    "\n",
    "        # --- 3B. Regularization ---\n",
    "        if use_graphical_lasso:\n",
    "            # GraphicalLasso learns precision directly from the same orientation:\n",
    "            # X: samples x features = dz x n_prime\n",
    "            try:\n",
    "                gl = GraphicalLasso(alpha=graphical_lasso_alpha).fit(Z_Vprime)\n",
    "                Theta = gl.precision_  # (n_prime, n_prime)\n",
    "            except Exception:\n",
    "                # Fallback: plain pseudo-inverse of raw covariance\n",
    "                try:\n",
    "                    Theta = np.linalg.pinv(Sigma_raw)\n",
    "                except Exception:\n",
    "                    f_ci_scores[(i, j)] = 0.0\n",
    "                    continue\n",
    "        else:\n",
    "            # Ledoit–Wolf shrinkage on covariance across nodes (same shape as Sigma_raw)\n",
    "            # X: samples x features = dz x n_prime\n",
    "            try:\n",
    "                lw = LedoitWolf().fit(Z_Vprime)\n",
    "                Sigma_LW = lw.covariance_  # (n_prime, n_prime)\n",
    "            except Exception:\n",
    "                Sigma_LW = Sigma_raw\n",
    "\n",
    "            lam = float(np.clip(lambda_shrink, 0.0, 1.0))\n",
    "            Sigma_reg = (1.0 - lam) * Sigma_raw + lam * Sigma_LW\n",
    "\n",
    "            # Precision matrix via pseudo-inverse\n",
    "            try:\n",
    "                Theta = np.linalg.pinv(Sigma_reg)\n",
    "            except Exception:\n",
    "                f_ci_scores[(i, j)] = 0.0\n",
    "                continue\n",
    "\n",
    "        # --- 4. Compute partial correlation fCI(i, j) from Θ ---\n",
    "        # Positions of i, j in V_prime: [0] and [1] by construction\n",
    "        pos_i = 0\n",
    "        pos_j = 1\n",
    "\n",
    "        theta_ij = Theta[pos_i, pos_j]\n",
    "        theta_ii = Theta[pos_i, pos_i]\n",
    "        theta_jj = Theta[pos_j, pos_j]\n",
    "\n",
    "        denom = theta_ii * theta_jj\n",
    "        if denom <= 0:\n",
    "            f_ci = 0.0\n",
    "        else:\n",
    "            f_ci = -theta_ij / np.sqrt(denom)\n",
    "            f_ci = float(np.clip(f_ci, -1.0, 1.0))\n",
    "\n",
    "        f_ci_scores[(i, j)] = f_ci\n",
    "\n",
    "    return f_ci_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72cdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is your GAE embeddings (torch.Tensor or np.ndarray), shape (N_nodes+1, dz)\n",
    "# M_set_potential_confounder is what we built earlier:\n",
    "#   {(i, j): [k1, k2, ...], ...}   with 1-based node IDs\n",
    "\n",
    "f_ci_dict = compute_f_ci_scores(\n",
    "    Z,\n",
    "    M_set_potential_confounder,\n",
    "    lambda_shrink=0.25   # <-- relaxed and works best for your size\n",
    ")\n",
    "\n",
    "\n",
    "# Example: see fCI for edge (1, 3)\n",
    "print(\"fCI(1,3) =\", f_ci_dict.get((1, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23789c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(f_ci_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631300d8",
   "metadata": {},
   "source": [
    "##### The LLM check $f_{llm-confounder}$\n",
    "\n",
    "w_direct_llm_confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7845ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, Tuple, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from google.genai import Client, types\n",
    "from google.genai import errors as genai_errors\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "# -------------------------\n",
    "# Configuration / Hyperparams\n",
    "# -------------------------\n",
    "MODEL_CANDIDATES = [\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "]\n",
    "MODEL = MODEL_CANDIDATES[0]\n",
    "CURRENT_MODEL_INDEX = 0\n",
    "EXHAUSTED_MODELS = set()\n",
    "\n",
    "NS_SAMPLES = 5               # number of CoT samples per pair\n",
    "CF_TEMPERATURE = 0.7\n",
    "CF_DELAY_SECONDS = 15         # seconds between requests\n",
    "TAU_COARSE_PARSE_SLEEP = 0.5\n",
    "\n",
    "# Persistence paths\n",
    "OUT_DIR = \"extracted_output\"\n",
    "COCAD_DIR = os.path.join(OUT_DIR, \"cocad\", \"w_direct\")\n",
    "RAW_REASONINGS_DIR = os.path.join(COCAD_DIR, \"llm_confounder_raw_reasonings\")\n",
    "RESULTS_JSON = os.path.join(COCAD_DIR, \"w_direct_llm_confounders.json\")\n",
    "EMBEDDINGS_NPZ = os.path.join(COCAD_DIR, \"llm_confounder_reasoning_embeddings.npz\")\n",
    "\n",
    "os.makedirs(RAW_REASONINGS_DIR, exist_ok=True)\n",
    "os.makedirs(COCAD_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LLM client + embedder\n",
    "client = Client(api_key=os.environ.get(\"GEMINI_API_KEY\", \"\"))\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pydantic model for structured LLM response\n",
    "# -------------------------\n",
    "class ConfounderResponse(BaseModel):\n",
    "    reasoning: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers: model rotation + safe structured call\n",
    "# -------------------------\n",
    "def _switch_to_next_model():\n",
    "    global MODEL, CURRENT_MODEL_INDEX\n",
    "    EXHAUSTED_MODELS.add(MODEL)\n",
    "    for i in range(len(MODEL_CANDIDATES)):\n",
    "        idx = (CURRENT_MODEL_INDEX + 1 + i) % len(MODEL_CANDIDATES)\n",
    "        candidate = MODEL_CANDIDATES[idx]\n",
    "        if candidate not in EXHAUSTED_MODELS:\n",
    "            MODEL = candidate\n",
    "            CURRENT_MODEL_INDEX = idx\n",
    "            print(f\"[LLM] (confounder) switching to backup model: {MODEL}\")\n",
    "            return\n",
    "    raise RuntimeError(\"All configured models exhausted (confounder).\")\n",
    "\n",
    "\n",
    "def safe_generate_structured(\n",
    "    prompt: str,\n",
    "    temperature: float = CF_TEMPERATURE,\n",
    "    max_retries: int = 3,\n",
    ") -> Optional[ConfounderResponse]:\n",
    "    \"\"\"\n",
    "    Try to get a structured JSON response parsed into ConfounderResponse.\n",
    "    Uses model rotation and retries on quota errors.\n",
    "    Returns parsed ConfounderResponse or None if parsing failed.\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        from_model = MODEL\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=ConfounderResponse,\n",
    "        )\n",
    "        try:\n",
    "            print(f\"[LLM] (confounder) structured call attempt {attempt} model={from_model}\")\n",
    "            resp = client.models.generate_content(model=from_model, contents=prompt, config=config)\n",
    "            parsed = getattr(resp, \"parsed\", None)\n",
    "            if isinstance(parsed, ConfounderResponse):\n",
    "                return parsed\n",
    "            # if parsed is a dict-like that pydantic can parse, try that\n",
    "            try:\n",
    "                if parsed is not None:\n",
    "                    return ConfounderResponse.parse_obj(parsed)\n",
    "            except ValidationError:\n",
    "                pass\n",
    "            # parsing failed even though model returned something; fallthrough to raw-text fallback\n",
    "            raw_text = getattr(resp, \"text\", \"\") or \"\"\n",
    "            # attempt to parse raw_text heuristically below in caller if None returned\n",
    "            return None\n",
    "        except genai_errors.ClientError as e:\n",
    "            msg = str(e)\n",
    "            last_exc = e\n",
    "            print(f\"[LLM] (confounder) client error on {from_model}: {msg}\")\n",
    "            # detect quota/rate-limit\n",
    "            if \"RESOURCE_EXHAUSTED\" in msg or \"429\" in msg or \"exceeded your current quota\" in msg.lower():\n",
    "                try:\n",
    "                    _switch_to_next_model()\n",
    "                except RuntimeError:\n",
    "                    raise\n",
    "                time.sleep(CF_DELAY_SECONDS)\n",
    "                continue\n",
    "            # otherwise escalate\n",
    "            raise\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fallback parser for unstructured/raw responses\n",
    "# -------------------------\n",
    "def parse_json_like_response(raw_text: str) -> Tuple[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Attempts to parse the LLM reply expected to be JSON like:\n",
    "      {\"reasoning\": \"...\", \"score\": 0.72}\n",
    "    Returns (reasoning_str, score_float_or_None).\n",
    "    If parsing fails, tries to heuristically extract a trailing SCORE: <num>.\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return \"\", None\n",
    "\n",
    "    # 1) Try JSON parse\n",
    "    try:\n",
    "        j = json.loads(raw_text)\n",
    "        reasoning = j.get(\"reasoning\") if isinstance(j.get(\"reasoning\"), str) else \"\"\n",
    "        score = j.get(\"score\")\n",
    "        try:\n",
    "            score = float(score) if score is not None else None\n",
    "        except Exception:\n",
    "            score = None\n",
    "        return (reasoning.strip(), score)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try to find a last line \"SCORE: <num>\"\n",
    "    lines = [ln.strip() for ln in raw_text.splitlines() if ln.strip()]\n",
    "    score = None\n",
    "    reasoning = raw_text.strip()\n",
    "    for ln in reversed(lines):\n",
    "        up = ln.upper()\n",
    "        if up.startswith(\"SCORE\"):\n",
    "            parts = ln.split(\":\")\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    score = float(parts[1].strip())\n",
    "                except Exception:\n",
    "                    score = None\n",
    "            # reasoning = everything before this line\n",
    "            idx = raw_text.rfind(ln)\n",
    "            if idx != -1:\n",
    "                reasoning = raw_text[:idx].strip()\n",
    "            break\n",
    "\n",
    "    # 3) If still no score, try to find a number between 0 and 1 anywhere\n",
    "    if score is None:\n",
    "        import re\n",
    "        m = re.search(r\"([0](?:\\.\\d+)?|1(?:\\.0+)?)\", raw_text)\n",
    "        if m:\n",
    "            try:\n",
    "                cand = float(m.group(1))\n",
    "                if 0.0 <= cand <= 1.0:\n",
    "                    score = cand\n",
    "            except Exception:\n",
    "                score = None\n",
    "\n",
    "    if reasoning is None:\n",
    "        reasoning = \"\"\n",
    "    return (reasoning.strip(), score)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Semantic coherence helper\n",
    "# -------------------------\n",
    "def mean_pairwise_cosine(embeddings: List[np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    embeddings: list of 1D numpy arrays (vectors)\n",
    "    returns average pairwise cosine similarity across p<q pairs.\n",
    "    If len < 2 returns 1.0 (fully coherent) by convention.\n",
    "    \"\"\"\n",
    "    n = len(embeddings)\n",
    "    if n < 2:\n",
    "        return 1.0\n",
    "    mats = np.vstack(embeddings)  # shape (n, d)\n",
    "    norms = np.linalg.norm(mats, axis=1, keepdims=True) + 1e-12\n",
    "    mats = mats / norms\n",
    "    sim = mats @ mats.T  # (n,n) cosine matrix\n",
    "    iu = np.triu_indices(n, k=1)\n",
    "    vals = sim[iu]\n",
    "    return float(vals.mean())\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main Option B Implementation (uses structured responses with fallback)\n",
    "# -------------------------\n",
    "def llm_confounder_cot_check_and_persist(\n",
    "    M_set_potential_confounder: Dict[Tuple[int, int], List[int]],\n",
    "    entities_by_id: Dict[int, Dict[str, Any]],\n",
    "    pairwise_context_map: Dict[Tuple[int, int], str],\n",
    "    ns_samples: int = NS_SAMPLES,\n",
    "    temperature: float = CF_TEMPERATURE,\n",
    "    delay_seconds: int = CF_DELAY_SECONDS,\n",
    ") -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs the LLM CoT confounder checks for each (i,j) in M_set_potential_confounder.\n",
    "\n",
    "    Inputs:\n",
    "      - M_set_potential_confounder: {(i,j): [k1,k2,...], ...} (1-based ints)\n",
    "      - entities_by_id: {id_int: entity_dict} where entity_dict contains 'name' etc.\n",
    "      - pairwise_context_map: {(i,j): evidence_text} from earlier RAG stage.\n",
    "\n",
    "    Returns:\n",
    "      - results: dict keyed by (i,j) containing:\n",
    "          {\n",
    "            \"mu_score\": float,\n",
    "            \"var_score\": float,\n",
    "            \"R_coh\": float,\n",
    "            \"Cconfidence\": float,\n",
    "            \"S_final\": float,\n",
    "            \"raw_reasonings\": [...],\n",
    "            \"raw_scores\": [...],\n",
    "            \"reasoning_filenames\": [...],\n",
    "            \"reasoning_embedding_keys\": [...],\n",
    "            \"structured_responses\": [...],  # raw parsed objects if available\n",
    "          }\n",
    "    Also persists:\n",
    "      - RESULTS_JSON (summary under extracted_output/cocad/w_direct)\n",
    "      - EMBEDDINGS_NPZ (all embeddings per pair)\n",
    "      - RAW_REASONINGS_DIR/* (raw .txt files per sample)\n",
    "    \"\"\"\n",
    "\n",
    "    all_results: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "    embeddings_store: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    pairs = list(M_set_potential_confounder.keys())\n",
    "    total_pairs = len(pairs)\n",
    "    print(f\"[LLM-Confounder-CoT] Running on {total_pairs} directed pairs...\")\n",
    "\n",
    "    for idx_pair, (i, j) in enumerate(pairs, start=1):\n",
    "        print(f\"\\n[{idx_pair}/{total_pairs}] Pair (i={i}, j={j})\")\n",
    "\n",
    "        confounders = M_set_potential_confounder.get((i, j), [])\n",
    "        i_name = entities_by_id.get(int(i), {}).get(\"name\", f\"N{int(i)}\")\n",
    "        j_name = entities_by_id.get(int(j), {}).get(\"name\", f\"N{int(j)}\")\n",
    "        confounder_names = [\n",
    "            entities_by_id.get(int(k), {}).get(\"name\", f\"N{int(k)}\") for k in confounders\n",
    "        ]\n",
    "\n",
    "        context_text = pairwise_context_map.get((i, j), \"\")\n",
    "        if not context_text:\n",
    "            print(\"  Warning: no context / evidence_text for this pair; skipping.\")\n",
    "            all_results[(i, j)] = {\n",
    "                \"mu_score\": 0.0,\n",
    "                \"var_score\": 0.0,\n",
    "                \"R_coh\": 1.0,\n",
    "                \"Cconfidence\": 0.0,\n",
    "                \"S_final\": 0.0,\n",
    "                \"raw_reasonings\": [],\n",
    "                \"raw_scores\": [],\n",
    "                \"reasoning_filenames\": [],\n",
    "                \"reasoning_embedding_keys\": [],\n",
    "                \"structured_responses\": [],\n",
    "                \"note\": \"no context\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Build structured / constrained prompt\n",
    "        confounder_list_str = (\n",
    "            \"[\" + \", \".join([f'\"{n}\"' for n in confounder_names]) + \"]\"\n",
    "            if confounder_names\n",
    "            else \"[]\"\n",
    "        )\n",
    "        prompt = f\"\"\"\n",
    "You are a careful causal-reasoning assistant.\n",
    "\n",
    "Context (evidence):\n",
    "\\\"\\\"\\\"{context_text}\\\"\\\"\\\" \n",
    "\n",
    "Claim to test: Does '{i_name}' -> '{j_name}' represent a direct causal influence?\n",
    "\n",
    "Potential Confounders (held constant): {confounder_list_str}\n",
    "\n",
    "Task (2 steps):\n",
    "1) Reasoning: If '{i_name}' were NOT present, while all Potential Confounders above are HELD CONSTANT,\n",
    "   explain step-by-step only using the Context why there would still / would not be a direct effect on '{j_name}'.\n",
    "2) Return a single JSON object EXACTLY in the form:\n",
    "   {{ \"reasoning\": \"<your step-by-step reasoning>\", \"score\": <a decimal between 0.0 and 1.0> }}\n",
    "\n",
    "Constraints:\n",
    "- Use only information from the provided Context (do NOT hallucinate new facts).\n",
    "- Keep the \"reasoning\" concise but explicit (2-6 sentences is fine).\n",
    "- Return strictly one JSON object (no extra text).\n",
    "\"\"\"\n",
    "\n",
    "        raw_reasonings: List[str] = []\n",
    "        raw_scores: List[float] = []\n",
    "        structured_responses: List[Dict[str, Any]] = []\n",
    "        per_pair_embedding_keys: List[str] = []\n",
    "        per_pair_reasoning_filenames: List[str] = []\n",
    "        per_pair_embeddings: List[np.ndarray] = []\n",
    "\n",
    "        # stochastic samples\n",
    "        for s_idx in range(ns_samples):\n",
    "            print(f\"   sample {s_idx+1}/{ns_samples} ...\", end=\" \")\n",
    "            parsed_obj: Optional[ConfounderResponse] = None\n",
    "            raw_text_for_file = \"\"\n",
    "            try:\n",
    "                # first try to get structured parsed response via the typed schema call\n",
    "                parsed_obj = safe_generate_structured(prompt=prompt, temperature=temperature)\n",
    "                if parsed_obj is not None:\n",
    "                    reasoning_str = parsed_obj.reasoning.strip()\n",
    "                    score_val = float(parsed_obj.score)\n",
    "                    raw_text_for_file = json.dumps(parsed_obj.dict(), ensure_ascii=False)\n",
    "                else:\n",
    "                    # fallback: ask raw text using safe llm and parse heuristically\n",
    "                    # reuse safe_generate_structured's model rotation logic by calling the unstructured safe path\n",
    "                    # (we call the current MODEL directly)\n",
    "                    print(\"(structured parse failed; falling back to raw text) \", end=\"\")\n",
    "                    from_model = MODEL\n",
    "                    config = types.GenerateContentConfig(temperature=temperature)\n",
    "                    resp = client.models.generate_content(model=from_model, contents=prompt, config=config)\n",
    "                    raw_text = getattr(resp, \"text\", \"\") or \"\"\n",
    "                    raw_text_for_file = raw_text\n",
    "                    # attempt to parse heuristically\n",
    "                    reasoning_str, score_val = parse_json_like_response(raw_text)\n",
    "                    if score_val is None:\n",
    "                        score_val = 0.0\n",
    "\n",
    "                # persist raw reasoning text to file\n",
    "                fn = os.path.join(RAW_REASONINGS_DIR, f\"pair_{i}_{j}_sample_{s_idx+1}.txt\")\n",
    "                with open(fn, \"w\", encoding=\"utf-8\") as fw:\n",
    "                    fw.write(raw_text_for_file)\n",
    "                per_pair_reasoning_filenames.append(fn)\n",
    "\n",
    "                raw_reasonings.append(reasoning_str)\n",
    "                raw_scores.append(float(score_val))\n",
    "\n",
    "                # save structured dict if available (for auditing)\n",
    "                if parsed_obj is not None:\n",
    "                    structured_responses.append(parsed_obj.dict())\n",
    "                else:\n",
    "                    # store a minimal structured fallback\n",
    "                    structured_responses.append({\"reasoning\": reasoning_str, \"score\": float(score_val)})\n",
    "\n",
    "                # embed the reasoning (or empty vector if empty)\n",
    "                if isinstance(reasoning_str, str) and reasoning_str.strip():\n",
    "                    emb = embedder.encode([reasoning_str], convert_to_numpy=True)[0]\n",
    "                else:\n",
    "                    emb = np.zeros(embedder.get_sentence_embedding_dimension(), dtype=np.float32)\n",
    "\n",
    "                emb_key = f\"pair_{i}_{j}_s{s_idx+1}\"\n",
    "                embeddings_store[emb_key] = emb\n",
    "                per_pair_embedding_keys.append(emb_key)\n",
    "                per_pair_embeddings.append(emb)\n",
    "\n",
    "                print(f\"SCORE={score_val:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] LLM/sample failed: {e}\")\n",
    "                # store fallback placeholders, persist error file\n",
    "                raw_reasonings.append(\"\")\n",
    "                raw_scores.append(0.0)\n",
    "                structured_responses.append({\"reasoning\": \"\", \"score\": 0.0})\n",
    "                fn = os.path.join(RAW_REASONINGS_DIR, f\"pair_{i}_{j}_sample_{s_idx+1}_error.txt\")\n",
    "                with open(fn, \"w\", encoding=\"utf-8\") as fw:\n",
    "                    fw.write(f\"[ERROR] {e}\\n\")\n",
    "                per_pair_reasoning_filenames.append(fn)\n",
    "                emb_key = f\"pair_{i}_{j}_s{s_idx+1}\"\n",
    "                emb = np.zeros(embedder.get_sentence_embedding_dimension(), dtype=np.float32)\n",
    "                embeddings_store[emb_key] = emb\n",
    "                per_pair_embedding_keys.append(emb_key)\n",
    "                per_pair_embeddings.append(emb)\n",
    "\n",
    "            # rate-limit pause\n",
    "            time.sleep(delay_seconds)\n",
    "\n",
    "        # Aggregate stats\n",
    "        scores_arr = np.asarray(raw_scores, dtype=float)\n",
    "        mu_score = float(np.mean(scores_arr)) if len(scores_arr) > 0 else 0.0\n",
    "        var_score = float(np.var(scores_arr, ddof=0)) if len(scores_arr) > 0 else 0.0\n",
    "\n",
    "        # compute normalized variance in [0,1] for Cconfidence\n",
    "        eps = 1e-8\n",
    "        normalized_variance = var_score / (var_score + (mu_score ** 2) + eps)\n",
    "        normalized_variance = float(np.clip(normalized_variance, 0.0, 1.0))\n",
    "        Cconfidence = float(1.0 - normalized_variance)\n",
    "\n",
    "        # semantic coherence Rcoh\n",
    "        R_coh = float(mean_pairwise_cosine(per_pair_embeddings))\n",
    "\n",
    "        # final combined score (as proposed): mu * Cconfidence\n",
    "        S_final = float(mu_score * Cconfidence)\n",
    "\n",
    "        # save per-pair summary\n",
    "        all_results[(i, j)] = {\n",
    "            \"mu_score\": mu_score,\n",
    "            \"var_score\": var_score,\n",
    "            \"R_coh\": R_coh,\n",
    "            \"Cconfidence\": Cconfidence,\n",
    "            \"S_final\": S_final,\n",
    "            \"raw_reasonings\": raw_reasonings,\n",
    "            \"raw_scores\": raw_scores,\n",
    "            \"reasoning_filenames\": per_pair_reasoning_filenames,\n",
    "            \"reasoning_embedding_keys\": per_pair_embedding_keys,\n",
    "            \"structured_responses\": structured_responses,\n",
    "            \"confounder_names\": confounder_names,\n",
    "            \"i_name\": i_name,\n",
    "            \"j_name\": j_name,\n",
    "            \"context_snippet_len\": len(context_text),\n",
    "        }\n",
    "\n",
    "        # progress info\n",
    "        print(f\"  -> mu={mu_score:.3f}, var={var_score:.4f}, R_coh={R_coh:.3f}, Cconf={Cconfidence:.3f}, Sfinal={S_final:.3f}\")\n",
    "\n",
    "    # Persist summary JSON (convert tuple keys to \"i_j\")\n",
    "    serializable_results = {}\n",
    "    for (i, j), rec in all_results.items():\n",
    "        key = f\"{i}_{j}\"\n",
    "        serializable_results[key] = rec\n",
    "    with open(RESULTS_JSON, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(serializable_results, fout, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved summary results to: {RESULTS_JSON}\")\n",
    "\n",
    "    # Persist embeddings_store to npz\n",
    "    np.savez_compressed(EMBEDDINGS_NPZ, **embeddings_store)\n",
    "    print(f\"Saved all reasoning embeddings to: {EMBEDDINGS_NPZ}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage:\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Run the pipeline (this will call the LLM and persist files)\n",
    "    results = llm_confounder_cot_check_and_persist(\n",
    "        M_set_potential_confounder=M_set_potential_confounder,\n",
    "        entities_by_id=entities_by_id,\n",
    "        pairwise_context_map=pairwise_structural_context_retrieval,\n",
    "        ns_samples=NS_SAMPLES,\n",
    "        temperature=CF_TEMPERATURE,\n",
    "        delay_seconds=CF_DELAY_SECONDS,\n",
    "    )\n",
    "\n",
    "    print(\"\\nDone. Example item (first) :\")\n",
    "    if results:\n",
    "        k0 = next(iter(results.keys()))\n",
    "        print(k0, results[k0])\n",
    "    else:\n",
    "        print(\"no results (empty input).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To save the above results to the extracted_output/cocad/w_direct/w_direct_llm_confounders.json\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "def _to_json_compatible(o: Any):\n",
    "    \"\"\"Recursively convert numpy types/arrays and other non-JSON-native types to JSON-friendly types.\"\"\"\n",
    "    # Numpy scalar\n",
    "    if isinstance(o, (np.floating, np.float32, np.float64)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.integer, np.int32, np.int64)):\n",
    "        return int(o)\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    # basic types + lists/dicts/tuples\n",
    "    if isinstance(o, dict):\n",
    "        return {str(k): _to_json_compatible(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple)):\n",
    "        return [_to_json_compatible(v) for v in o]\n",
    "    # fallback to str for unknown objects\n",
    "    if isinstance(o, (str, int, float, bool)) or o is None:\n",
    "        return o\n",
    "    return str(o)\n",
    "\n",
    "def save_w_direct_llm_confounders(results: dict, save_path: str):\n",
    "    \"\"\"\n",
    "    Save `results` where keys may be tuples (i,j).\n",
    "    Converts tuple keys to \"i_j\" string keys and makes values JSON-safe.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    serializable = {}\n",
    "    for k, v in results.items():\n",
    "        # Turn tuple key (i,j) into \"i_j\"\n",
    "        if isinstance(k, tuple):\n",
    "            key_str = f\"{int(k[0])}_{int(k[1])}\"\n",
    "        else:\n",
    "            key_str = str(k)\n",
    "        serializable[key_str] = _to_json_compatible(v)\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "        json.dump(serializable, fw, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[OK] Saved w_direct LLM confounders to: {save_path}\")\n",
    "\n",
    "# example usage:\n",
    "save_dir = \"extracted_output/cocad/w_direct\"\n",
    "save_path = os.path.join(save_dir, \"w_direct_llm_confounders.json\")\n",
    "# results is your dict from earlier\n",
    "save_w_direct_llm_confounders(results, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the above \n",
    "\n",
    "\n",
    "import json\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "def load_w_direct_llm_confounders(load_path: str) -> Dict[Tuple[int,int], Any]:\n",
    "    \"\"\"\n",
    "    Load JSON saved with save_w_direct_llm_confounders and restore tuple keys (i,j).\n",
    "    Returns dict with keys (i,j) as ints.\n",
    "    \"\"\"\n",
    "    with open(load_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        raw = json.load(fr)\n",
    "\n",
    "    results_restored = {}\n",
    "    for key_str, value in raw.items():\n",
    "        # Expect keys like \"i_j\"\n",
    "        if isinstance(key_str, str) and \"_\" in key_str:\n",
    "            try:\n",
    "                a, b = key_str.split(\"_\", 1)\n",
    "                k = (int(a), int(b))\n",
    "            except Exception:\n",
    "                # fallback: use the key string as-is (not a tuple)\n",
    "                k = key_str\n",
    "        else:\n",
    "            # fallback: use the original key string\n",
    "            try:\n",
    "                k = int(key_str)\n",
    "            except Exception:\n",
    "                k = key_str\n",
    "\n",
    "        results_restored[k] = value\n",
    "\n",
    "    return results_restored\n",
    "\n",
    "# example usage:\n",
    "load_path = \"extracted_output/cocad/w_direct/w_direct_llm_confounders.json\"\n",
    "w_direct_llm_confounders = load_w_direct_llm_confounders(load_path)\n",
    "print(f\"[OK] Loaded {len(w_direct_llm_confounders)} items from {load_path}\")\n",
    "# inspect one:\n",
    "if w_direct_llm_confounders:\n",
    "    first_k = next(iter(w_direct_llm_confounders))\n",
    "    print(\"Example key:\", first_k)\n",
    "    print(\"Example value keys:\", list(w_direct_llm_confounders[first_k].keys())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703001c",
   "metadata": {},
   "source": [
    "What all is saved above and where:\n",
    "\n",
    "- Summary JSON:\n",
    "\n",
    "Saved in extracted_output/llm_confounder_results.json\n",
    "\n",
    "- Raw LLM outputs for audit/provenances\n",
    "\n",
    "Saved to extracted_output/llm_confounder_raw_reasonings/.\n",
    "There we have One .txt file per sample:\n",
    "\n",
    "```\n",
    "pair_<i>_<j>_sample_<s>.txt\n",
    "pair_<i>_<j>_sample_<s>_error.txt   (if LLM call failed)\n",
    "\n",
    "```\n",
    "\n",
    "- Reasoning Embeddings (for semantic coherence R_coh):\n",
    "\n",
    "Saved to extracted_output/llm_confounder_reasoning_embeddings.npz\n",
    "\n",
    "```\n",
    "{\n",
    "  \"pair_i_j_s1\": <768-d vector>,\n",
    "  \"pair_i_j_s2\": <768-d vector>,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "Each key corresponds to a single CoT reasoning sample for a pair.\n",
    "\n",
    "Used to compute semantic coherence:\n",
    "\n",
    "$R_{coh}​(i,j)$=mean pairwise cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets find our initial W_direct results from earlier stages\n",
    "## These would later anyays be refined in EM-refinement stage\n",
    "\n",
    "## As of now lets take W_direct as product of structural and LLM stages f*** the f_fusion as of now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(w_direct_structural_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d154074",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(w_direct_llm_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
